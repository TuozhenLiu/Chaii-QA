{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a07956c",
   "metadata": {
    "papermill": {
     "duration": 0.024951,
     "end_time": "2021-10-03T03:16:10.230915",
     "exception": false,
     "start_time": "2021-10-03T03:16:10.205964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h2>chaii QA - 5 Fold XLMRoberta Finetuning in Torch w/o Trainer API</h2>\n",
    "    \n",
    "<h3><span \"style: color=#444\">Introduction</span></h3>\n",
    "\n",
    "The kernel implements 5-Fold XLMRoberta QA Model without using the Trainer API from HuggingFace.\n",
    "\n",
    "This is a three part kernel,\n",
    "\n",
    "- [External Data - MLQA, XQUAD Preprocessing](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing) which preprocesses the Hindi Corpus of MLQA and XQUAD. I have used these data for training.\n",
    "\n",
    "- [chaii QA - 5 Fold XLMRoberta Torch | FIT](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-fit/edit) This kernel is the current kernel and used for Finetuning (FIT) on competition + external data.\n",
    "\n",
    "- [chaii QA - 5 Fold XLMRoberta Torch | Infer](https://www.kaggle.com/rhtsingh/chaii-qa-5-fold-xlmroberta-torch-infer) The Inference kernel where we ensemble our 5 Fold XLMRoberta Models and do the submission.\n",
    "\n",
    "<h3><span \"style: color=#444\">Techniques</span></h3>\n",
    "\n",
    "The kernel has implementation for below techniques, click on the links to learn more -\n",
    "\n",
    " - [External Data Preprocessing + Training](https://www.kaggle.com/rhtsingh/external-data-mlqa-xquad-preprocessing)\n",
    " \n",
    " - [Mixed Precision Training using APEX](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
    " \n",
    " - [Gradient Accumulation](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies)\n",
    " \n",
    " - [Gradient Clipping](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch)\n",
    " \n",
    " - [Grouped Layerwise Learning Rate Decay](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs)\n",
    " \n",
    " - [Utilizing Intermediate Transformer Representations](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently)\n",
    " \n",
    " - [Layer Initialization](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning)\n",
    " \n",
    " - [5-Fold Training](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit)\n",
    " \n",
    " - etc.\n",
    " \n",
    "<h3><span \"style: color=#444\">References</span></h3>\n",
    "I would like to acknowledge below kagglers work and resources without whom this kernel wouldn't have been possible,  \n",
    "\n",
    "- [roberta inference 5 folds by Abhishek](https://www.kaggle.com/abhishek/roberta-inference-5-folds)\n",
    "\n",
    "- [ChAII - EDA & Baseline by Darek](https://www.kaggle.com/thedrcat/chaii-eda-baseline) due to whom i found the great notebook below from HuggingFace,\n",
    "\n",
    "- [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb)\n",
    "\n",
    "- [HuggingFace QA Examples](https://github.com/huggingface/transformers/tree/master/examples/pytorch/question-answering)\n",
    "\n",
    "- [TensorFlow 2.0 Question Answering - Collections of gold solutions](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127409) these solutions are gold and highly recommend everyone to give a detailed read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbefbb",
   "metadata": {
    "papermill": {
     "duration": 0.022844,
     "end_time": "2021-10-03T03:16:10.277127",
     "exception": false,
     "start_time": "2021-10-03T03:16:10.254283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3><span style=\"color=#444\">Note</span></h3>\n",
    "\n",
    "The below points are worth noting,\n",
    "\n",
    " - I haven't used FP16 because due to some reason this fails and model never starts training.\n",
    " - These are the original hyperparamters and setting that I have used for training my models.\n",
    " - I tried few pooling layers but none of them performed better than simple one.\n",
    " - Gradient clipping reduces model performance.\n",
    " - <span style=\"color:#2E86C1\">Training 5 Folds at once will give OOM issue on Kaggle and Colab. I have trained one fold at a time i.e. After 1st fold is completed I save the model as a dataset then train second fold. Repeat this process until all 5 folds are completed. Training each fold takes around 50 minuts on Kaggle.</span>\n",
    " - <span style=\"color:#2E86C1\">I have modified the preprocessing code a lot from original used in HuggingFace notebook since I am not using HuggingFace Datasets API as well. So I had to handle the sequence-ids specially since they contain None values which torch doesn't support.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6b85d",
   "metadata": {
    "papermill": {
     "duration": 0.022506,
     "end_time": "2021-10-03T03:16:10.323293",
     "exception": false,
     "start_time": "2021-10-03T03:16:10.300787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install APEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872617b2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:10.381586Z",
     "iopub.status.busy": "2021-10-03T03:16:10.380950Z",
     "iopub.status.idle": "2021-10-03T03:16:10.386383Z",
     "shell.execute_reply": "2021-10-03T03:16:10.386914Z",
     "shell.execute_reply.started": "2021-08-21T23:10:00.180045Z"
    },
    "id": "aCY6yvR6ET3s",
    "outputId": "99831be8-d7e0-47d4-c561-f266d3ed1fd3",
    "papermill": {
     "duration": 0.041088,
     "end_time": "2021-10-03T03:16:10.387192",
     "exception": false,
     "start_time": "2021-10-03T03:16:10.346104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.sh\n",
    "export CUDA_HOME=/usr/local/cuda-10.1\n",
    "git clone https://github.com/NVIDIA/apex\n",
    "cd apex\n",
    "pip install -v --cuda_ext --cpp_ext --disable-pip-version-check --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b3c2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:10.439434Z",
     "iopub.status.busy": "2021-10-03T03:16:10.438896Z",
     "iopub.status.idle": "2021-10-03T03:16:13.962197Z",
     "shell.execute_reply": "2021-10-03T03:16:13.962827Z",
     "shell.execute_reply.started": "2021-08-21T23:10:00.19196Z"
    },
    "id": "-l2Jsav9ET3v",
    "papermill": {
     "duration": 3.551421,
     "end_time": "2021-10-03T03:16:13.963008",
     "exception": false,
     "start_time": "2021-10-03T03:16:10.411587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sh setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd241065",
   "metadata": {
    "papermill": {
     "duration": 0.023813,
     "end_time": "2021-10-03T03:16:14.011625",
     "exception": false,
     "start_time": "2021-10-03T03:16:13.987812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01d8d9e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:14.069356Z",
     "iopub.status.busy": "2021-10-03T03:16:14.068557Z",
     "iopub.status.idle": "2021-10-03T03:16:20.662768Z",
     "shell.execute_reply": "2021-10-03T03:16:20.663327Z",
     "shell.execute_reply.started": "2021-08-21T23:10:01.579531Z"
    },
    "id": "E4l6PirHET3x",
    "outputId": "eeaea823-bdbc-4bef-a518-e51736877d6e",
    "papermill": {
     "duration": 6.628679,
     "end_time": "2021-10-03T03:16:20.663529",
     "exception": false,
     "start_time": "2021-10-03T03:16:14.034850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex AMP Installed :: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab7d3d",
   "metadata": {
    "papermill": {
     "duration": 0.022968,
     "end_time": "2021-10-03T03:16:20.710481",
     "exception": false,
     "start_time": "2021-10-03T03:16:20.687513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfdc691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:20.763258Z",
     "iopub.status.busy": "2021-10-03T03:16:20.762729Z",
     "iopub.status.idle": "2021-10-03T03:16:20.766629Z",
     "shell.execute_reply": "2021-10-03T03:16:20.766195Z",
     "shell.execute_reply.started": "2021-08-21T23:10:08.645633Z"
    },
    "id": "LUx5XplNET3y",
    "papermill": {
     "duration": 0.033055,
     "end_time": "2021-10-03T03:16:20.766735",
     "exception": false,
     "start_time": "2021-10-03T03:16:20.733680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    model_type = 'xlm_roberta'\n",
    "    model_name_or_path = \"deepset/xlm-roberta-large-squad2\"\n",
    "    config_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "    gradient_accumulation_steps = 2\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = \"deepset/xlm-roberta-large-squad2\"\n",
    "    max_seq_length = 384\n",
    "    doc_stride = 128\n",
    "\n",
    "    # train\n",
    "    epochs = 2\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 8\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'AdamW'\n",
    "    learning_rate = 1.5e-5\n",
    "    weight_decay = 1e-2\n",
    "    epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'linear-warmup'\n",
    "    warmup_ratio = 0.1\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eedb8",
   "metadata": {
    "papermill": {
     "duration": 0.031522,
     "end_time": "2021-10-03T03:16:20.821644",
     "exception": false,
     "start_time": "2021-10-03T03:16:20.790122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d79c99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:20.876413Z",
     "iopub.status.busy": "2021-10-03T03:16:20.875813Z",
     "iopub.status.idle": "2021-10-03T03:16:21.973704Z",
     "shell.execute_reply": "2021-10-03T03:16:21.974128Z",
     "shell.execute_reply.started": "2021-08-21T23:10:08.654795Z"
    },
    "id": "X_eRZQrzET3z",
    "papermill": {
     "duration": 1.129528,
     "end_time": "2021-10-03T03:16:21.974322",
     "exception": false,
     "start_time": "2021-10-03T03:16:20.844794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n",
    "test = pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n",
    "external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\n",
    "external_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\n",
    "external_train = pd.concat([external_mlqa, external_xquad])\n",
    "\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=69)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "\n",
    "train = create_folds(train, num_splits=5)\n",
    "external_train[\"kfold\"] = -1\n",
    "external_train['id'] = list(np.arange(1, len(external_train)+1))\n",
    "train = pd.concat([train, external_train]).reset_index(drop=True)\n",
    "\n",
    "def convert_answers(row):\n",
    "    return {'answer_start': [row[0]], 'text': [row[1]]}\n",
    "\n",
    "train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e7330",
   "metadata": {
    "papermill": {
     "duration": 0.023591,
     "end_time": "2021-10-03T03:16:22.023066",
     "exception": false,
     "start_time": "2021-10-03T03:16:21.999475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Covert Examples to Features (Preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4edff59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.083608Z",
     "iopub.status.busy": "2021-10-03T03:16:22.081900Z",
     "iopub.status.idle": "2021-10-03T03:16:22.084294Z",
     "shell.execute_reply": "2021-10-03T03:16:22.084718Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.845103Z"
    },
    "id": "dxbZdct1ET3z",
    "papermill": {
     "duration": 0.038229,
     "end_time": "2021-10-03T03:16:22.084860",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.046631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_train_features(args, example, tokenizer):\n",
    "    example[\"question\"] = example[\"question\"].lstrip()\n",
    "    tokenized_example = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=args.max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n",
    "\n",
    "    features = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        feature = {}\n",
    "\n",
    "        input_ids = tokenized_example[\"input_ids\"][i]\n",
    "        attention_mask = tokenized_example[\"attention_mask\"][i]\n",
    "\n",
    "        feature['input_ids'] = input_ids\n",
    "        feature['attention_mask'] = attention_mask\n",
    "        feature['offset_mapping'] = offsets\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_example.sequence_ids(i)\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"]\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            feature[\"start_position\"] = cls_index\n",
    "            feature[\"end_position\"] = cls_index\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                feature[\"start_position\"] = cls_index\n",
    "                feature[\"end_position\"] = cls_index\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                feature[\"start_position\"] = token_start_index - 1\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                feature[\"end_position\"] = token_end_index + 1\n",
    "\n",
    "        features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a39c5",
   "metadata": {
    "papermill": {
     "duration": 0.023041,
     "end_time": "2021-10-03T03:16:22.130954",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.107913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224468dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.188115Z",
     "iopub.status.busy": "2021-10-03T03:16:22.186582Z",
     "iopub.status.idle": "2021-10-03T03:16:22.188822Z",
     "shell.execute_reply": "2021-10-03T03:16:22.189222Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.860781Z"
    },
    "id": "6TuzHdjmET30",
    "papermill": {
     "duration": 0.035393,
     "end_time": "2021-10-03T03:16:22.189363",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.153970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, features, mode='train'):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.features = features\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item):   \n",
    "        feature = self.features[item]\n",
    "        if self.mode == 'train':\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n",
    "                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n",
    "                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n",
    "                'offset_mapping':feature['offset_mapping'],\n",
    "                'sequence_ids':feature['sequence_ids'],\n",
    "                'id':feature['example_id'],\n",
    "                'context': feature['context'],\n",
    "                'question': feature['question']\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e06a3a",
   "metadata": {
    "papermill": {
     "duration": 0.022638,
     "end_time": "2021-10-03T03:16:22.235209",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.212571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47637c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.291598Z",
     "iopub.status.busy": "2021-10-03T03:16:22.289922Z",
     "iopub.status.idle": "2021-10-03T03:16:22.292352Z",
     "shell.execute_reply": "2021-10-03T03:16:22.292857Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.876331Z"
    },
    "id": "9OxhKqxcET31",
    "papermill": {
     "duration": 0.034814,
     "end_time": "2021-10-03T03:16:22.292997",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.258183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self._init_weights(self.qa_outputs)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids, \n",
    "        attention_mask=None, \n",
    "        # token_type_ids=None\n",
    "    ):\n",
    "        outputs = self.xlm_roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        # sequence_output = self.dropout(sequence_output)\n",
    "        qa_logits = self.qa_outputs(sequence_output)\n",
    "        \n",
    "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "    \n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12608b8c",
   "metadata": {
    "papermill": {
     "duration": 0.022835,
     "end_time": "2021-10-03T03:16:22.339367",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.316532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0306e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.390749Z",
     "iopub.status.busy": "2021-10-03T03:16:22.390072Z",
     "iopub.status.idle": "2021-10-03T03:16:22.393575Z",
     "shell.execute_reply": "2021-10-03T03:16:22.393136Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.888214Z"
    },
    "id": "SxuNrJqqET32",
    "papermill": {
     "duration": 0.03132,
     "end_time": "2021-10-03T03:16:22.393683",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.362363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(preds, labels):\n",
    "    start_preds, end_preds = preds\n",
    "    start_labels, end_labels = labels\n",
    "    \n",
    "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
    "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
    "    total_loss = (start_loss + end_loss) / 2\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13847e9a",
   "metadata": {
    "papermill": {
     "duration": 0.022542,
     "end_time": "2021-10-03T03:16:22.438973",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.416431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Grouped Layerwise Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866c5112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.498416Z",
     "iopub.status.busy": "2021-10-03T03:16:22.497859Z",
     "iopub.status.idle": "2021-10-03T03:16:22.501499Z",
     "shell.execute_reply": "2021-10-03T03:16:22.501092Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.902935Z"
    },
    "id": "vf6HVcu2ET34",
    "papermill": {
     "duration": 0.039564,
     "end_time": "2021-10-03T03:16:22.501605",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.462041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(args, model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.xlm_roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02994b",
   "metadata": {
    "papermill": {
     "duration": 0.022659,
     "end_time": "2021-10-03T03:16:22.547008",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.524349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Metric Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d7febb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.600024Z",
     "iopub.status.busy": "2021-10-03T03:16:22.598758Z",
     "iopub.status.idle": "2021-10-03T03:16:22.601529Z",
     "shell.execute_reply": "2021-10-03T03:16:22.601092Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.921015Z"
    },
    "id": "bkFB-iMcET34",
    "papermill": {
     "duration": 0.031611,
     "end_time": "2021-10-03T03:16:22.601631",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.570020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54a914",
   "metadata": {
    "papermill": {
     "duration": 0.022499,
     "end_time": "2021-10-03T03:16:22.646778",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.624279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46feb4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.706932Z",
     "iopub.status.busy": "2021-10-03T03:16:22.705582Z",
     "iopub.status.idle": "2021-10-03T03:16:22.707866Z",
     "shell.execute_reply": "2021-10-03T03:16:22.708297Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.93744Z"
    },
    "id": "spFRutV0ET34",
    "papermill": {
     "duration": 0.038847,
     "end_time": "2021-10-03T03:16:22.708437",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.669590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return config, tokenizer, model\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    # optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    if args.optimizer_type == \"AdamW\":\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            correct_bias=True\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    \n",
    "    train_features, valid_features = [[] for _ in range(2)]\n",
    "    for i, row in train_set.iterrows():\n",
    "        train_features += prepare_train_features(args, row, tokenizer)\n",
    "    for i, row in valid_set.iterrows():\n",
    "        valid_features += prepare_train_features(args, row, tokenizer)\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_features)\n",
    "    valid_dataset = DatasetRetriever(valid_features)\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57b300",
   "metadata": {
    "papermill": {
     "duration": 0.022532,
     "end_time": "2021-10-03T03:16:22.753706",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.731174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a383ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.815300Z",
     "iopub.status.busy": "2021-10-03T03:16:22.814745Z",
     "iopub.status.idle": "2021-10-03T03:16:22.818692Z",
     "shell.execute_reply": "2021-10-03T03:16:22.818270Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.957635Z"
    },
    "id": "iFLvh1VQET35",
    "papermill": {
     "duration": 0.042361,
     "end_time": "2021-10-03T03:16:22.818811",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.776450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "\n",
    "            outputs_start, outputs_end = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            \n",
    "            loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += input_ids.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            # if args.fp16:\n",
    "            #     torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
    "            # else:\n",
    "            #     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            if batch_idx % args.gradient_accumulation_steps == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf050dd",
   "metadata": {
    "papermill": {
     "duration": 0.023217,
     "end_time": "2021-10-03T03:16:22.865755",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.842538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a07e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:22.924908Z",
     "iopub.status.busy": "2021-10-03T03:16:22.923151Z",
     "iopub.status.idle": "2021-10-03T03:16:22.925672Z",
     "shell.execute_reply": "2021-10-03T03:16:22.926095Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.977667Z"
    },
    "id": "1a8kG2UYET36",
    "papermill": {
     "duration": 0.037075,
     "end_time": "2021-10-03T03:16:22.926224",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.889149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], \\\n",
    "                    batch_data['start_position'], batch_data['end_position']\n",
    "            \n",
    "            input_ids, attention_mask, targets_start, targets_end = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), targets_start.cuda(), targets_end.cuda()\n",
    "            \n",
    "            with torch.no_grad():            \n",
    "                outputs_start, outputs_end = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "                \n",
    "                loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "                \n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b1d09",
   "metadata": {
    "papermill": {
     "duration": 0.023157,
     "end_time": "2021-10-03T03:16:22.972834",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.949677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Initialize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8fd406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:23.031454Z",
     "iopub.status.busy": "2021-10-03T03:16:23.030110Z",
     "iopub.status.idle": "2021-10-03T03:16:23.032979Z",
     "shell.execute_reply": "2021-10-03T03:16:23.032583Z",
     "shell.execute_reply.started": "2021-08-21T23:10:09.989625Z"
    },
    "id": "v-gUDyq2ET37",
    "papermill": {
     "duration": 0.03594,
     "end_time": "2021-10-03T03:16:23.033086",
     "exception": false,
     "start_time": "2021-10-03T03:16:22.997146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model_config, tokenizer, model = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    \n",
    "    # data loaders\n",
    "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f0cac",
   "metadata": {
    "papermill": {
     "duration": 0.023536,
     "end_time": "2021-10-03T03:16:23.080376",
     "exception": false,
     "start_time": "2021-10-03T03:16:23.056840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0c9976e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:23.139260Z",
     "iopub.status.busy": "2021-10-03T03:16:23.138673Z",
     "iopub.status.idle": "2021-10-03T03:16:23.141297Z",
     "shell.execute_reply": "2021-10-03T03:16:23.141744Z",
     "shell.execute_reply.started": "2021-08-21T23:10:10.003378Z"
    },
    "id": "39ei5Bm5ET37",
    "papermill": {
     "duration": 0.037963,
     "end_time": "2021-10-03T03:16:23.141885",
     "exception": false,
     "start_time": "2021-10-03T03:16:23.103922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler)\n",
    "    evaluator = Evaluator(model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        \n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "            \n",
    "        print()\n",
    "\n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    \n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del trainer, evaluator\n",
    "    del model, model_config, tokenizer\n",
    "    del optimizer, scheduler\n",
    "    del train_dataloader, valid_dataloader, result_dict\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ff47a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T03:16:23.193240Z",
     "iopub.status.busy": "2021-10-03T03:16:23.192653Z",
     "iopub.status.idle": "2021-10-03T04:57:37.749119Z",
     "shell.execute_reply": "2021-10-03T04:57:37.749756Z",
     "shell.execute_reply.started": "2021-08-21T23:10:10.019991Z"
    },
    "id": "mPaGnnCnbhWl",
    "outputId": "1b25d103-7d4d-4779-ea74-34312b42ad4c",
    "papermill": {
     "duration": 6074.584711,
     "end_time": "2021-10-03T04:57:37.749955",
     "exception": false,
     "start_time": "2021-10-03T03:16:23.165244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f94b70de2a4504bf35270e1c54416b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa803b8f26c4fc8805f84ec35309e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4912f3c823674254babaca8e7b92d26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eabe97d850d448db71bf4172246dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8046d6a5812b485d8e86c07a8f329caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20026, Num examples Valid=3043\n",
      "Total Training Steps: 5008, Total Warmup Steps: 500\n",
      "Epoch: 00 [    4/20026 (  0%)], Train Loss: 3.04148\n",
      "Epoch: 00 [   44/20026 (  0%)], Train Loss: 3.08151\n",
      "Epoch: 00 [   84/20026 (  0%)], Train Loss: 3.07408\n",
      "Epoch: 00 [  124/20026 (  1%)], Train Loss: 3.04619\n",
      "Epoch: 00 [  164/20026 (  1%)], Train Loss: 3.02885\n",
      "Epoch: 00 [  204/20026 (  1%)], Train Loss: 3.01060\n",
      "Epoch: 00 [  244/20026 (  1%)], Train Loss: 2.97001\n",
      "Epoch: 00 [  284/20026 (  1%)], Train Loss: 2.92684\n",
      "Epoch: 00 [  324/20026 (  2%)], Train Loss: 2.87287\n",
      "Epoch: 00 [  364/20026 (  2%)], Train Loss: 2.81157\n",
      "Epoch: 00 [  404/20026 (  2%)], Train Loss: 2.74965\n",
      "Epoch: 00 [  444/20026 (  2%)], Train Loss: 2.68178\n",
      "Epoch: 00 [  484/20026 (  2%)], Train Loss: 2.60042\n",
      "Epoch: 00 [  524/20026 (  3%)], Train Loss: 2.50732\n",
      "Epoch: 00 [  564/20026 (  3%)], Train Loss: 2.40780\n",
      "Epoch: 00 [  604/20026 (  3%)], Train Loss: 2.31725\n",
      "Epoch: 00 [  644/20026 (  3%)], Train Loss: 2.25056\n",
      "Epoch: 00 [  684/20026 (  3%)], Train Loss: 2.16460\n",
      "Epoch: 00 [  724/20026 (  4%)], Train Loss: 2.08764\n",
      "Epoch: 00 [  764/20026 (  4%)], Train Loss: 2.02080\n",
      "Epoch: 00 [  804/20026 (  4%)], Train Loss: 1.96078\n",
      "Epoch: 00 [  844/20026 (  4%)], Train Loss: 1.90254\n",
      "Epoch: 00 [  884/20026 (  4%)], Train Loss: 1.84010\n",
      "Epoch: 00 [  924/20026 (  5%)], Train Loss: 1.78272\n",
      "Epoch: 00 [  964/20026 (  5%)], Train Loss: 1.73146\n",
      "Epoch: 00 [ 1004/20026 (  5%)], Train Loss: 1.68377\n",
      "Epoch: 00 [ 1044/20026 (  5%)], Train Loss: 1.64231\n",
      "Epoch: 00 [ 1084/20026 (  5%)], Train Loss: 1.59649\n",
      "Epoch: 00 [ 1124/20026 (  6%)], Train Loss: 1.55303\n",
      "Epoch: 00 [ 1164/20026 (  6%)], Train Loss: 1.51598\n",
      "Epoch: 00 [ 1204/20026 (  6%)], Train Loss: 1.47731\n",
      "Epoch: 00 [ 1244/20026 (  6%)], Train Loss: 1.44137\n",
      "Epoch: 00 [ 1284/20026 (  6%)], Train Loss: 1.40801\n",
      "Epoch: 00 [ 1324/20026 (  7%)], Train Loss: 1.38879\n",
      "Epoch: 00 [ 1364/20026 (  7%)], Train Loss: 1.36295\n",
      "Epoch: 00 [ 1404/20026 (  7%)], Train Loss: 1.33790\n",
      "Epoch: 00 [ 1444/20026 (  7%)], Train Loss: 1.31362\n",
      "Epoch: 00 [ 1484/20026 (  7%)], Train Loss: 1.29277\n",
      "Epoch: 00 [ 1524/20026 (  8%)], Train Loss: 1.28063\n",
      "Epoch: 00 [ 1564/20026 (  8%)], Train Loss: 1.26917\n",
      "Epoch: 00 [ 1604/20026 (  8%)], Train Loss: 1.25336\n",
      "Epoch: 00 [ 1644/20026 (  8%)], Train Loss: 1.23630\n",
      "Epoch: 00 [ 1684/20026 (  8%)], Train Loss: 1.21916\n",
      "Epoch: 00 [ 1724/20026 (  9%)], Train Loss: 1.20934\n",
      "Epoch: 00 [ 1764/20026 (  9%)], Train Loss: 1.19306\n",
      "Epoch: 00 [ 1804/20026 (  9%)], Train Loss: 1.17451\n",
      "Epoch: 00 [ 1844/20026 (  9%)], Train Loss: 1.15717\n",
      "Epoch: 00 [ 1884/20026 (  9%)], Train Loss: 1.14563\n",
      "Epoch: 00 [ 1924/20026 ( 10%)], Train Loss: 1.12867\n",
      "Epoch: 00 [ 1964/20026 ( 10%)], Train Loss: 1.11876\n",
      "Epoch: 00 [ 2004/20026 ( 10%)], Train Loss: 1.10586\n",
      "Epoch: 00 [ 2044/20026 ( 10%)], Train Loss: 1.09355\n",
      "Epoch: 00 [ 2084/20026 ( 10%)], Train Loss: 1.07951\n",
      "Epoch: 00 [ 2124/20026 ( 11%)], Train Loss: 1.06799\n",
      "Epoch: 00 [ 2164/20026 ( 11%)], Train Loss: 1.05982\n",
      "Epoch: 00 [ 2204/20026 ( 11%)], Train Loss: 1.04587\n",
      "Epoch: 00 [ 2244/20026 ( 11%)], Train Loss: 1.03423\n",
      "Epoch: 00 [ 2284/20026 ( 11%)], Train Loss: 1.02153\n",
      "Epoch: 00 [ 2324/20026 ( 12%)], Train Loss: 1.01391\n",
      "Epoch: 00 [ 2364/20026 ( 12%)], Train Loss: 1.00131\n",
      "Epoch: 00 [ 2404/20026 ( 12%)], Train Loss: 0.99050\n",
      "Epoch: 00 [ 2444/20026 ( 12%)], Train Loss: 0.98357\n",
      "Epoch: 00 [ 2484/20026 ( 12%)], Train Loss: 0.97602\n",
      "Epoch: 00 [ 2524/20026 ( 13%)], Train Loss: 0.96481\n",
      "Epoch: 00 [ 2564/20026 ( 13%)], Train Loss: 0.95594\n",
      "Epoch: 00 [ 2604/20026 ( 13%)], Train Loss: 0.95452\n",
      "Epoch: 00 [ 2644/20026 ( 13%)], Train Loss: 0.94830\n",
      "Epoch: 00 [ 2684/20026 ( 13%)], Train Loss: 0.93975\n",
      "Epoch: 00 [ 2724/20026 ( 14%)], Train Loss: 0.93257\n",
      "Epoch: 00 [ 2764/20026 ( 14%)], Train Loss: 0.92478\n",
      "Epoch: 00 [ 2804/20026 ( 14%)], Train Loss: 0.91960\n",
      "Epoch: 00 [ 2844/20026 ( 14%)], Train Loss: 0.91445\n",
      "Epoch: 00 [ 2884/20026 ( 14%)], Train Loss: 0.90650\n",
      "Epoch: 00 [ 2924/20026 ( 15%)], Train Loss: 0.89839\n",
      "Epoch: 00 [ 2964/20026 ( 15%)], Train Loss: 0.89271\n",
      "Epoch: 00 [ 3004/20026 ( 15%)], Train Loss: 0.88533\n",
      "Epoch: 00 [ 3044/20026 ( 15%)], Train Loss: 0.87713\n",
      "Epoch: 00 [ 3084/20026 ( 15%)], Train Loss: 0.87130\n",
      "Epoch: 00 [ 3124/20026 ( 16%)], Train Loss: 0.86406\n",
      "Epoch: 00 [ 3164/20026 ( 16%)], Train Loss: 0.85969\n",
      "Epoch: 00 [ 3204/20026 ( 16%)], Train Loss: 0.85364\n",
      "Epoch: 00 [ 3244/20026 ( 16%)], Train Loss: 0.84868\n",
      "Epoch: 00 [ 3284/20026 ( 16%)], Train Loss: 0.84256\n",
      "Epoch: 00 [ 3324/20026 ( 17%)], Train Loss: 0.83509\n",
      "Epoch: 00 [ 3364/20026 ( 17%)], Train Loss: 0.83145\n",
      "Epoch: 00 [ 3404/20026 ( 17%)], Train Loss: 0.82802\n",
      "Epoch: 00 [ 3444/20026 ( 17%)], Train Loss: 0.82871\n",
      "Epoch: 00 [ 3484/20026 ( 17%)], Train Loss: 0.82506\n",
      "Epoch: 00 [ 3524/20026 ( 18%)], Train Loss: 0.81966\n",
      "Epoch: 00 [ 3564/20026 ( 18%)], Train Loss: 0.81425\n",
      "Epoch: 00 [ 3604/20026 ( 18%)], Train Loss: 0.81099\n",
      "Epoch: 00 [ 3644/20026 ( 18%)], Train Loss: 0.80747\n",
      "Epoch: 00 [ 3684/20026 ( 18%)], Train Loss: 0.80492\n",
      "Epoch: 00 [ 3724/20026 ( 19%)], Train Loss: 0.80005\n",
      "Epoch: 00 [ 3764/20026 ( 19%)], Train Loss: 0.79540\n",
      "Epoch: 00 [ 3804/20026 ( 19%)], Train Loss: 0.78977\n",
      "Epoch: 00 [ 3844/20026 ( 19%)], Train Loss: 0.78478\n",
      "Epoch: 00 [ 3884/20026 ( 19%)], Train Loss: 0.77957\n",
      "Epoch: 00 [ 3924/20026 ( 20%)], Train Loss: 0.77642\n",
      "Epoch: 00 [ 3964/20026 ( 20%)], Train Loss: 0.76992\n",
      "Epoch: 00 [ 4004/20026 ( 20%)], Train Loss: 0.76760\n",
      "Epoch: 00 [ 4044/20026 ( 20%)], Train Loss: 0.76296\n",
      "Epoch: 00 [ 4084/20026 ( 20%)], Train Loss: 0.76194\n",
      "Epoch: 00 [ 4124/20026 ( 21%)], Train Loss: 0.75950\n",
      "Epoch: 00 [ 4164/20026 ( 21%)], Train Loss: 0.75536\n",
      "Epoch: 00 [ 4204/20026 ( 21%)], Train Loss: 0.75194\n",
      "Epoch: 00 [ 4244/20026 ( 21%)], Train Loss: 0.74850\n",
      "Epoch: 00 [ 4284/20026 ( 21%)], Train Loss: 0.74449\n",
      "Epoch: 00 [ 4324/20026 ( 22%)], Train Loss: 0.73995\n",
      "Epoch: 00 [ 4364/20026 ( 22%)], Train Loss: 0.73804\n",
      "Epoch: 00 [ 4404/20026 ( 22%)], Train Loss: 0.73452\n",
      "Epoch: 00 [ 4444/20026 ( 22%)], Train Loss: 0.73174\n",
      "Epoch: 00 [ 4484/20026 ( 22%)], Train Loss: 0.72890\n",
      "Epoch: 00 [ 4524/20026 ( 23%)], Train Loss: 0.72608\n",
      "Epoch: 00 [ 4564/20026 ( 23%)], Train Loss: 0.72185\n",
      "Epoch: 00 [ 4604/20026 ( 23%)], Train Loss: 0.71765\n",
      "Epoch: 00 [ 4644/20026 ( 23%)], Train Loss: 0.71461\n",
      "Epoch: 00 [ 4684/20026 ( 23%)], Train Loss: 0.71069\n",
      "Epoch: 00 [ 4724/20026 ( 24%)], Train Loss: 0.70702\n",
      "Epoch: 00 [ 4764/20026 ( 24%)], Train Loss: 0.70563\n",
      "Epoch: 00 [ 4804/20026 ( 24%)], Train Loss: 0.70551\n",
      "Epoch: 00 [ 4844/20026 ( 24%)], Train Loss: 0.70209\n",
      "Epoch: 00 [ 4884/20026 ( 24%)], Train Loss: 0.69924\n",
      "Epoch: 00 [ 4924/20026 ( 25%)], Train Loss: 0.70036\n",
      "Epoch: 00 [ 4964/20026 ( 25%)], Train Loss: 0.69900\n",
      "Epoch: 00 [ 5004/20026 ( 25%)], Train Loss: 0.69842\n",
      "Epoch: 00 [ 5044/20026 ( 25%)], Train Loss: 0.69516\n",
      "Epoch: 00 [ 5084/20026 ( 25%)], Train Loss: 0.69245\n",
      "Epoch: 00 [ 5124/20026 ( 26%)], Train Loss: 0.69057\n",
      "Epoch: 00 [ 5164/20026 ( 26%)], Train Loss: 0.68869\n",
      "Epoch: 00 [ 5204/20026 ( 26%)], Train Loss: 0.68761\n",
      "Epoch: 00 [ 5244/20026 ( 26%)], Train Loss: 0.68613\n",
      "Epoch: 00 [ 5284/20026 ( 26%)], Train Loss: 0.68390\n",
      "Epoch: 00 [ 5324/20026 ( 27%)], Train Loss: 0.68308\n",
      "Epoch: 00 [ 5364/20026 ( 27%)], Train Loss: 0.68224\n",
      "Epoch: 00 [ 5404/20026 ( 27%)], Train Loss: 0.67937\n",
      "Epoch: 00 [ 5444/20026 ( 27%)], Train Loss: 0.67843\n",
      "Epoch: 00 [ 5484/20026 ( 27%)], Train Loss: 0.67528\n",
      "Epoch: 00 [ 5524/20026 ( 28%)], Train Loss: 0.67146\n",
      "Epoch: 00 [ 5564/20026 ( 28%)], Train Loss: 0.66798\n",
      "Epoch: 00 [ 5604/20026 ( 28%)], Train Loss: 0.66894\n",
      "Epoch: 00 [ 5644/20026 ( 28%)], Train Loss: 0.66631\n",
      "Epoch: 00 [ 5684/20026 ( 28%)], Train Loss: 0.66483\n",
      "Epoch: 00 [ 5724/20026 ( 29%)], Train Loss: 0.66241\n",
      "Epoch: 00 [ 5764/20026 ( 29%)], Train Loss: 0.66016\n",
      "Epoch: 00 [ 5804/20026 ( 29%)], Train Loss: 0.65912\n",
      "Epoch: 00 [ 5844/20026 ( 29%)], Train Loss: 0.65764\n",
      "Epoch: 00 [ 5884/20026 ( 29%)], Train Loss: 0.65578\n",
      "Epoch: 00 [ 5924/20026 ( 30%)], Train Loss: 0.65494\n",
      "Epoch: 00 [ 5964/20026 ( 30%)], Train Loss: 0.65349\n",
      "Epoch: 00 [ 6004/20026 ( 30%)], Train Loss: 0.65349\n",
      "Epoch: 00 [ 6044/20026 ( 30%)], Train Loss: 0.65157\n",
      "Epoch: 00 [ 6084/20026 ( 30%)], Train Loss: 0.64956\n",
      "Epoch: 00 [ 6124/20026 ( 31%)], Train Loss: 0.64966\n",
      "Epoch: 00 [ 6164/20026 ( 31%)], Train Loss: 0.64848\n",
      "Epoch: 00 [ 6204/20026 ( 31%)], Train Loss: 0.64683\n",
      "Epoch: 00 [ 6244/20026 ( 31%)], Train Loss: 0.64552\n",
      "Epoch: 00 [ 6284/20026 ( 31%)], Train Loss: 0.64313\n",
      "Epoch: 00 [ 6324/20026 ( 32%)], Train Loss: 0.64170\n",
      "Epoch: 00 [ 6364/20026 ( 32%)], Train Loss: 0.64006\n",
      "Epoch: 00 [ 6404/20026 ( 32%)], Train Loss: 0.63768\n",
      "Epoch: 00 [ 6444/20026 ( 32%)], Train Loss: 0.63518\n",
      "Epoch: 00 [ 6484/20026 ( 32%)], Train Loss: 0.63426\n",
      "Epoch: 00 [ 6524/20026 ( 33%)], Train Loss: 0.63176\n",
      "Epoch: 00 [ 6564/20026 ( 33%)], Train Loss: 0.62960\n",
      "Epoch: 00 [ 6604/20026 ( 33%)], Train Loss: 0.62749\n",
      "Epoch: 00 [ 6644/20026 ( 33%)], Train Loss: 0.62616\n",
      "Epoch: 00 [ 6684/20026 ( 33%)], Train Loss: 0.62461\n",
      "Epoch: 00 [ 6724/20026 ( 34%)], Train Loss: 0.62306\n",
      "Epoch: 00 [ 6764/20026 ( 34%)], Train Loss: 0.62144\n",
      "Epoch: 00 [ 6804/20026 ( 34%)], Train Loss: 0.61969\n",
      "Epoch: 00 [ 6844/20026 ( 34%)], Train Loss: 0.61852\n",
      "Epoch: 00 [ 6884/20026 ( 34%)], Train Loss: 0.61666\n",
      "Epoch: 00 [ 6924/20026 ( 35%)], Train Loss: 0.61601\n",
      "Epoch: 00 [ 6964/20026 ( 35%)], Train Loss: 0.61571\n",
      "Epoch: 00 [ 7004/20026 ( 35%)], Train Loss: 0.61484\n",
      "Epoch: 00 [ 7044/20026 ( 35%)], Train Loss: 0.61373\n",
      "Epoch: 00 [ 7084/20026 ( 35%)], Train Loss: 0.61272\n",
      "Epoch: 00 [ 7124/20026 ( 36%)], Train Loss: 0.61176\n",
      "Epoch: 00 [ 7164/20026 ( 36%)], Train Loss: 0.61227\n",
      "Epoch: 00 [ 7204/20026 ( 36%)], Train Loss: 0.61039\n",
      "Epoch: 00 [ 7244/20026 ( 36%)], Train Loss: 0.60947\n",
      "Epoch: 00 [ 7284/20026 ( 36%)], Train Loss: 0.60857\n",
      "Epoch: 00 [ 7324/20026 ( 37%)], Train Loss: 0.60791\n",
      "Epoch: 00 [ 7364/20026 ( 37%)], Train Loss: 0.60706\n",
      "Epoch: 00 [ 7404/20026 ( 37%)], Train Loss: 0.60618\n",
      "Epoch: 00 [ 7444/20026 ( 37%)], Train Loss: 0.60421\n",
      "Epoch: 00 [ 7484/20026 ( 37%)], Train Loss: 0.60330\n",
      "Epoch: 00 [ 7524/20026 ( 38%)], Train Loss: 0.60236\n",
      "Epoch: 00 [ 7564/20026 ( 38%)], Train Loss: 0.60076\n",
      "Epoch: 00 [ 7604/20026 ( 38%)], Train Loss: 0.59980\n",
      "Epoch: 00 [ 7644/20026 ( 38%)], Train Loss: 0.59997\n",
      "Epoch: 00 [ 7684/20026 ( 38%)], Train Loss: 0.59861\n",
      "Epoch: 00 [ 7724/20026 ( 39%)], Train Loss: 0.59681\n",
      "Epoch: 00 [ 7764/20026 ( 39%)], Train Loss: 0.59583\n",
      "Epoch: 00 [ 7804/20026 ( 39%)], Train Loss: 0.59626\n",
      "Epoch: 00 [ 7844/20026 ( 39%)], Train Loss: 0.59526\n",
      "Epoch: 00 [ 7884/20026 ( 39%)], Train Loss: 0.59503\n",
      "Epoch: 00 [ 7924/20026 ( 40%)], Train Loss: 0.59481\n",
      "Epoch: 00 [ 7964/20026 ( 40%)], Train Loss: 0.59325\n",
      "Epoch: 00 [ 8004/20026 ( 40%)], Train Loss: 0.59232\n",
      "Epoch: 00 [ 8044/20026 ( 40%)], Train Loss: 0.59129\n",
      "Epoch: 00 [ 8084/20026 ( 40%)], Train Loss: 0.58996\n",
      "Epoch: 00 [ 8124/20026 ( 41%)], Train Loss: 0.58822\n",
      "Epoch: 00 [ 8164/20026 ( 41%)], Train Loss: 0.58648\n",
      "Epoch: 00 [ 8204/20026 ( 41%)], Train Loss: 0.58540\n",
      "Epoch: 00 [ 8244/20026 ( 41%)], Train Loss: 0.58385\n",
      "Epoch: 00 [ 8284/20026 ( 41%)], Train Loss: 0.58262\n",
      "Epoch: 00 [ 8324/20026 ( 42%)], Train Loss: 0.58220\n",
      "Epoch: 00 [ 8364/20026 ( 42%)], Train Loss: 0.58203\n",
      "Epoch: 00 [ 8404/20026 ( 42%)], Train Loss: 0.58125\n",
      "Epoch: 00 [ 8444/20026 ( 42%)], Train Loss: 0.58064\n",
      "Epoch: 00 [ 8484/20026 ( 42%)], Train Loss: 0.58068\n",
      "Epoch: 00 [ 8524/20026 ( 43%)], Train Loss: 0.57989\n",
      "Epoch: 00 [ 8564/20026 ( 43%)], Train Loss: 0.58004\n",
      "Epoch: 00 [ 8604/20026 ( 43%)], Train Loss: 0.57848\n",
      "Epoch: 00 [ 8644/20026 ( 43%)], Train Loss: 0.57788\n",
      "Epoch: 00 [ 8684/20026 ( 43%)], Train Loss: 0.57660\n",
      "Epoch: 00 [ 8724/20026 ( 44%)], Train Loss: 0.57616\n",
      "Epoch: 00 [ 8764/20026 ( 44%)], Train Loss: 0.57486\n",
      "Epoch: 00 [ 8804/20026 ( 44%)], Train Loss: 0.57401\n",
      "Epoch: 00 [ 8844/20026 ( 44%)], Train Loss: 0.57293\n",
      "Epoch: 00 [ 8884/20026 ( 44%)], Train Loss: 0.57165\n",
      "Epoch: 00 [ 8924/20026 ( 45%)], Train Loss: 0.57203\n",
      "Epoch: 00 [ 8964/20026 ( 45%)], Train Loss: 0.57102\n",
      "Epoch: 00 [ 9004/20026 ( 45%)], Train Loss: 0.56967\n",
      "Epoch: 00 [ 9044/20026 ( 45%)], Train Loss: 0.56826\n",
      "Epoch: 00 [ 9084/20026 ( 45%)], Train Loss: 0.56733\n",
      "Epoch: 00 [ 9124/20026 ( 46%)], Train Loss: 0.56538\n",
      "Epoch: 00 [ 9164/20026 ( 46%)], Train Loss: 0.56483\n",
      "Epoch: 00 [ 9204/20026 ( 46%)], Train Loss: 0.56338\n",
      "Epoch: 00 [ 9244/20026 ( 46%)], Train Loss: 0.56298\n",
      "Epoch: 00 [ 9284/20026 ( 46%)], Train Loss: 0.56234\n",
      "Epoch: 00 [ 9324/20026 ( 47%)], Train Loss: 0.56122\n",
      "Epoch: 00 [ 9364/20026 ( 47%)], Train Loss: 0.56040\n",
      "Epoch: 00 [ 9404/20026 ( 47%)], Train Loss: 0.55893\n",
      "Epoch: 00 [ 9444/20026 ( 47%)], Train Loss: 0.55851\n",
      "Epoch: 00 [ 9484/20026 ( 47%)], Train Loss: 0.55765\n",
      "Epoch: 00 [ 9524/20026 ( 48%)], Train Loss: 0.55651\n",
      "Epoch: 00 [ 9564/20026 ( 48%)], Train Loss: 0.55562\n",
      "Epoch: 00 [ 9604/20026 ( 48%)], Train Loss: 0.55561\n",
      "Epoch: 00 [ 9644/20026 ( 48%)], Train Loss: 0.55544\n",
      "Epoch: 00 [ 9684/20026 ( 48%)], Train Loss: 0.55511\n",
      "Epoch: 00 [ 9724/20026 ( 49%)], Train Loss: 0.55433\n",
      "Epoch: 00 [ 9764/20026 ( 49%)], Train Loss: 0.55374\n",
      "Epoch: 00 [ 9804/20026 ( 49%)], Train Loss: 0.55309\n",
      "Epoch: 00 [ 9844/20026 ( 49%)], Train Loss: 0.55230\n",
      "Epoch: 00 [ 9884/20026 ( 49%)], Train Loss: 0.55155\n",
      "Epoch: 00 [ 9924/20026 ( 50%)], Train Loss: 0.55113\n",
      "Epoch: 00 [ 9964/20026 ( 50%)], Train Loss: 0.55020\n",
      "Epoch: 00 [10004/20026 ( 50%)], Train Loss: 0.55009\n",
      "Epoch: 00 [10044/20026 ( 50%)], Train Loss: 0.54997\n",
      "Epoch: 00 [10084/20026 ( 50%)], Train Loss: 0.54914\n",
      "Epoch: 00 [10124/20026 ( 51%)], Train Loss: 0.54880\n",
      "Epoch: 00 [10164/20026 ( 51%)], Train Loss: 0.54822\n",
      "Epoch: 00 [10204/20026 ( 51%)], Train Loss: 0.54683\n",
      "Epoch: 00 [10244/20026 ( 51%)], Train Loss: 0.54653\n",
      "Epoch: 00 [10284/20026 ( 51%)], Train Loss: 0.54569\n",
      "Epoch: 00 [10324/20026 ( 52%)], Train Loss: 0.54406\n",
      "Epoch: 00 [10364/20026 ( 52%)], Train Loss: 0.54287\n",
      "Epoch: 00 [10404/20026 ( 52%)], Train Loss: 0.54258\n",
      "Epoch: 00 [10444/20026 ( 52%)], Train Loss: 0.54173\n",
      "Epoch: 00 [10484/20026 ( 52%)], Train Loss: 0.54110\n",
      "Epoch: 00 [10524/20026 ( 53%)], Train Loss: 0.54004\n",
      "Epoch: 00 [10564/20026 ( 53%)], Train Loss: 0.53896\n",
      "Epoch: 00 [10604/20026 ( 53%)], Train Loss: 0.53817\n",
      "Epoch: 00 [10644/20026 ( 53%)], Train Loss: 0.53778\n",
      "Epoch: 00 [10684/20026 ( 53%)], Train Loss: 0.53638\n",
      "Epoch: 00 [10724/20026 ( 54%)], Train Loss: 0.53522\n",
      "Epoch: 00 [10764/20026 ( 54%)], Train Loss: 0.53487\n",
      "Epoch: 00 [10804/20026 ( 54%)], Train Loss: 0.53382\n",
      "Epoch: 00 [10844/20026 ( 54%)], Train Loss: 0.53299\n",
      "Epoch: 00 [10884/20026 ( 54%)], Train Loss: 0.53233\n",
      "Epoch: 00 [10924/20026 ( 55%)], Train Loss: 0.53173\n",
      "Epoch: 00 [10964/20026 ( 55%)], Train Loss: 0.53097\n",
      "Epoch: 00 [11004/20026 ( 55%)], Train Loss: 0.53110\n",
      "Epoch: 00 [11044/20026 ( 55%)], Train Loss: 0.53096\n",
      "Epoch: 00 [11084/20026 ( 55%)], Train Loss: 0.53014\n",
      "Epoch: 00 [11124/20026 ( 56%)], Train Loss: 0.52989\n",
      "Epoch: 00 [11164/20026 ( 56%)], Train Loss: 0.52906\n",
      "Epoch: 00 [11204/20026 ( 56%)], Train Loss: 0.52904\n",
      "Epoch: 00 [11244/20026 ( 56%)], Train Loss: 0.52847\n",
      "Epoch: 00 [11284/20026 ( 56%)], Train Loss: 0.52846\n",
      "Epoch: 00 [11324/20026 ( 57%)], Train Loss: 0.52748\n",
      "Epoch: 00 [11364/20026 ( 57%)], Train Loss: 0.52747\n",
      "Epoch: 00 [11404/20026 ( 57%)], Train Loss: 0.52839\n",
      "Epoch: 00 [11444/20026 ( 57%)], Train Loss: 0.52795\n",
      "Epoch: 00 [11484/20026 ( 57%)], Train Loss: 0.52819\n",
      "Epoch: 00 [11524/20026 ( 58%)], Train Loss: 0.52735\n",
      "Epoch: 00 [11564/20026 ( 58%)], Train Loss: 0.52732\n",
      "Epoch: 00 [11604/20026 ( 58%)], Train Loss: 0.52701\n",
      "Epoch: 00 [11644/20026 ( 58%)], Train Loss: 0.52655\n",
      "Epoch: 00 [11684/20026 ( 58%)], Train Loss: 0.52628\n",
      "Epoch: 00 [11724/20026 ( 59%)], Train Loss: 0.52607\n",
      "Epoch: 00 [11764/20026 ( 59%)], Train Loss: 0.52475\n",
      "Epoch: 00 [11804/20026 ( 59%)], Train Loss: 0.52402\n",
      "Epoch: 00 [11844/20026 ( 59%)], Train Loss: 0.52345\n",
      "Epoch: 00 [11884/20026 ( 59%)], Train Loss: 0.52284\n",
      "Epoch: 00 [11924/20026 ( 60%)], Train Loss: 0.52195\n",
      "Epoch: 00 [11964/20026 ( 60%)], Train Loss: 0.52136\n",
      "Epoch: 00 [12004/20026 ( 60%)], Train Loss: 0.52037\n",
      "Epoch: 00 [12044/20026 ( 60%)], Train Loss: 0.51965\n",
      "Epoch: 00 [12084/20026 ( 60%)], Train Loss: 0.52009\n",
      "Epoch: 00 [12124/20026 ( 61%)], Train Loss: 0.51925\n",
      "Epoch: 00 [12164/20026 ( 61%)], Train Loss: 0.51830\n",
      "Epoch: 00 [12204/20026 ( 61%)], Train Loss: 0.51782\n",
      "Epoch: 00 [12244/20026 ( 61%)], Train Loss: 0.51757\n",
      "Epoch: 00 [12284/20026 ( 61%)], Train Loss: 0.51682\n",
      "Epoch: 00 [12324/20026 ( 62%)], Train Loss: 0.51587\n",
      "Epoch: 00 [12364/20026 ( 62%)], Train Loss: 0.51524\n",
      "Epoch: 00 [12404/20026 ( 62%)], Train Loss: 0.51469\n",
      "Epoch: 00 [12444/20026 ( 62%)], Train Loss: 0.51447\n",
      "Epoch: 00 [12484/20026 ( 62%)], Train Loss: 0.51373\n",
      "Epoch: 00 [12524/20026 ( 63%)], Train Loss: 0.51310\n",
      "Epoch: 00 [12564/20026 ( 63%)], Train Loss: 0.51213\n",
      "Epoch: 00 [12604/20026 ( 63%)], Train Loss: 0.51199\n",
      "Epoch: 00 [12644/20026 ( 63%)], Train Loss: 0.51126\n",
      "Epoch: 00 [12684/20026 ( 63%)], Train Loss: 0.51114\n",
      "Epoch: 00 [12724/20026 ( 64%)], Train Loss: 0.51086\n",
      "Epoch: 00 [12764/20026 ( 64%)], Train Loss: 0.51106\n",
      "Epoch: 00 [12804/20026 ( 64%)], Train Loss: 0.51071\n",
      "Epoch: 00 [12844/20026 ( 64%)], Train Loss: 0.51049\n",
      "Epoch: 00 [12884/20026 ( 64%)], Train Loss: 0.50960\n",
      "Epoch: 00 [12924/20026 ( 65%)], Train Loss: 0.50900\n",
      "Epoch: 00 [12964/20026 ( 65%)], Train Loss: 0.50835\n",
      "Epoch: 00 [13004/20026 ( 65%)], Train Loss: 0.50752\n",
      "Epoch: 00 [13044/20026 ( 65%)], Train Loss: 0.50690\n",
      "Epoch: 00 [13084/20026 ( 65%)], Train Loss: 0.50654\n",
      "Epoch: 00 [13124/20026 ( 66%)], Train Loss: 0.50613\n",
      "Epoch: 00 [13164/20026 ( 66%)], Train Loss: 0.50559\n",
      "Epoch: 00 [13204/20026 ( 66%)], Train Loss: 0.50460\n",
      "Epoch: 00 [13244/20026 ( 66%)], Train Loss: 0.50383\n",
      "Epoch: 00 [13284/20026 ( 66%)], Train Loss: 0.50335\n",
      "Epoch: 00 [13324/20026 ( 67%)], Train Loss: 0.50263\n",
      "Epoch: 00 [13364/20026 ( 67%)], Train Loss: 0.50254\n",
      "Epoch: 00 [13404/20026 ( 67%)], Train Loss: 0.50219\n",
      "Epoch: 00 [13444/20026 ( 67%)], Train Loss: 0.50200\n",
      "Epoch: 00 [13484/20026 ( 67%)], Train Loss: 0.50193\n",
      "Epoch: 00 [13524/20026 ( 68%)], Train Loss: 0.50166\n",
      "Epoch: 00 [13564/20026 ( 68%)], Train Loss: 0.50136\n",
      "Epoch: 00 [13604/20026 ( 68%)], Train Loss: 0.50071\n",
      "Epoch: 00 [13644/20026 ( 68%)], Train Loss: 0.50051\n",
      "Epoch: 00 [13684/20026 ( 68%)], Train Loss: 0.50005\n",
      "Epoch: 00 [13724/20026 ( 69%)], Train Loss: 0.49931\n",
      "Epoch: 00 [13764/20026 ( 69%)], Train Loss: 0.49851\n",
      "Epoch: 00 [13804/20026 ( 69%)], Train Loss: 0.49851\n",
      "Epoch: 00 [13844/20026 ( 69%)], Train Loss: 0.49828\n",
      "Epoch: 00 [13884/20026 ( 69%)], Train Loss: 0.49765\n",
      "Epoch: 00 [13924/20026 ( 70%)], Train Loss: 0.49762\n",
      "Epoch: 00 [13964/20026 ( 70%)], Train Loss: 0.49762\n",
      "Epoch: 00 [14004/20026 ( 70%)], Train Loss: 0.49768\n",
      "Epoch: 00 [14044/20026 ( 70%)], Train Loss: 0.49755\n",
      "Epoch: 00 [14084/20026 ( 70%)], Train Loss: 0.49725\n",
      "Epoch: 00 [14124/20026 ( 71%)], Train Loss: 0.49696\n",
      "Epoch: 00 [14164/20026 ( 71%)], Train Loss: 0.49715\n",
      "Epoch: 00 [14204/20026 ( 71%)], Train Loss: 0.49659\n",
      "Epoch: 00 [14244/20026 ( 71%)], Train Loss: 0.49639\n",
      "Epoch: 00 [14284/20026 ( 71%)], Train Loss: 0.49574\n",
      "Epoch: 00 [14324/20026 ( 72%)], Train Loss: 0.49557\n",
      "Epoch: 00 [14364/20026 ( 72%)], Train Loss: 0.49507\n",
      "Epoch: 00 [14404/20026 ( 72%)], Train Loss: 0.49456\n",
      "Epoch: 00 [14444/20026 ( 72%)], Train Loss: 0.49437\n",
      "Epoch: 00 [14484/20026 ( 72%)], Train Loss: 0.49405\n",
      "Epoch: 00 [14524/20026 ( 73%)], Train Loss: 0.49378\n",
      "Epoch: 00 [14564/20026 ( 73%)], Train Loss: 0.49310\n",
      "Epoch: 00 [14604/20026 ( 73%)], Train Loss: 0.49275\n",
      "Epoch: 00 [14644/20026 ( 73%)], Train Loss: 0.49205\n",
      "Epoch: 00 [14684/20026 ( 73%)], Train Loss: 0.49198\n",
      "Epoch: 00 [14724/20026 ( 74%)], Train Loss: 0.49148\n",
      "Epoch: 00 [14764/20026 ( 74%)], Train Loss: 0.49130\n",
      "Epoch: 00 [14804/20026 ( 74%)], Train Loss: 0.49097\n",
      "Epoch: 00 [14844/20026 ( 74%)], Train Loss: 0.49051\n",
      "Epoch: 00 [14884/20026 ( 74%)], Train Loss: 0.48993\n",
      "Epoch: 00 [14924/20026 ( 75%)], Train Loss: 0.48971\n",
      "Epoch: 00 [14964/20026 ( 75%)], Train Loss: 0.48904\n",
      "Epoch: 00 [15004/20026 ( 75%)], Train Loss: 0.48885\n",
      "Epoch: 00 [15044/20026 ( 75%)], Train Loss: 0.48808\n",
      "Epoch: 00 [15084/20026 ( 75%)], Train Loss: 0.48760\n",
      "Epoch: 00 [15124/20026 ( 76%)], Train Loss: 0.48711\n",
      "Epoch: 00 [15164/20026 ( 76%)], Train Loss: 0.48694\n",
      "Epoch: 00 [15204/20026 ( 76%)], Train Loss: 0.48611\n",
      "Epoch: 00 [15244/20026 ( 76%)], Train Loss: 0.48566\n",
      "Epoch: 00 [15284/20026 ( 76%)], Train Loss: 0.48517\n",
      "Epoch: 00 [15324/20026 ( 77%)], Train Loss: 0.48466\n",
      "Epoch: 00 [15364/20026 ( 77%)], Train Loss: 0.48402\n",
      "Epoch: 00 [15404/20026 ( 77%)], Train Loss: 0.48389\n",
      "Epoch: 00 [15444/20026 ( 77%)], Train Loss: 0.48339\n",
      "Epoch: 00 [15484/20026 ( 77%)], Train Loss: 0.48271\n",
      "Epoch: 00 [15524/20026 ( 78%)], Train Loss: 0.48221\n",
      "Epoch: 00 [15564/20026 ( 78%)], Train Loss: 0.48200\n",
      "Epoch: 00 [15604/20026 ( 78%)], Train Loss: 0.48159\n",
      "Epoch: 00 [15644/20026 ( 78%)], Train Loss: 0.48111\n",
      "Epoch: 00 [15684/20026 ( 78%)], Train Loss: 0.48046\n",
      "Epoch: 00 [15724/20026 ( 79%)], Train Loss: 0.48025\n",
      "Epoch: 00 [15764/20026 ( 79%)], Train Loss: 0.47996\n",
      "Epoch: 00 [15804/20026 ( 79%)], Train Loss: 0.47940\n",
      "Epoch: 00 [15844/20026 ( 79%)], Train Loss: 0.47905\n",
      "Epoch: 00 [15884/20026 ( 79%)], Train Loss: 0.47875\n",
      "Epoch: 00 [15924/20026 ( 80%)], Train Loss: 0.47806\n",
      "Epoch: 00 [15964/20026 ( 80%)], Train Loss: 0.47800\n",
      "Epoch: 00 [16004/20026 ( 80%)], Train Loss: 0.47799\n",
      "Epoch: 00 [16044/20026 ( 80%)], Train Loss: 0.47780\n",
      "Epoch: 00 [16084/20026 ( 80%)], Train Loss: 0.47778\n",
      "Epoch: 00 [16124/20026 ( 81%)], Train Loss: 0.47702\n",
      "Epoch: 00 [16164/20026 ( 81%)], Train Loss: 0.47702\n",
      "Epoch: 00 [16204/20026 ( 81%)], Train Loss: 0.47639\n",
      "Epoch: 00 [16244/20026 ( 81%)], Train Loss: 0.47576\n",
      "Epoch: 00 [16284/20026 ( 81%)], Train Loss: 0.47566\n",
      "Epoch: 00 [16324/20026 ( 82%)], Train Loss: 0.47534\n",
      "Epoch: 00 [16364/20026 ( 82%)], Train Loss: 0.47494\n",
      "Epoch: 00 [16404/20026 ( 82%)], Train Loss: 0.47437\n",
      "Epoch: 00 [16444/20026 ( 82%)], Train Loss: 0.47437\n",
      "Epoch: 00 [16484/20026 ( 82%)], Train Loss: 0.47435\n",
      "Epoch: 00 [16524/20026 ( 83%)], Train Loss: 0.47368\n",
      "Epoch: 00 [16564/20026 ( 83%)], Train Loss: 0.47381\n",
      "Epoch: 00 [16604/20026 ( 83%)], Train Loss: 0.47352\n",
      "Epoch: 00 [16644/20026 ( 83%)], Train Loss: 0.47276\n",
      "Epoch: 00 [16684/20026 ( 83%)], Train Loss: 0.47207\n",
      "Epoch: 00 [16724/20026 ( 84%)], Train Loss: 0.47149\n",
      "Epoch: 00 [16764/20026 ( 84%)], Train Loss: 0.47112\n",
      "Epoch: 00 [16804/20026 ( 84%)], Train Loss: 0.47127\n",
      "Epoch: 00 [16844/20026 ( 84%)], Train Loss: 0.47090\n",
      "Epoch: 00 [16884/20026 ( 84%)], Train Loss: 0.47054\n",
      "Epoch: 00 [16924/20026 ( 85%)], Train Loss: 0.47015\n",
      "Epoch: 00 [16964/20026 ( 85%)], Train Loss: 0.46928\n",
      "Epoch: 00 [17004/20026 ( 85%)], Train Loss: 0.46890\n",
      "Epoch: 00 [17044/20026 ( 85%)], Train Loss: 0.46855\n",
      "Epoch: 00 [17084/20026 ( 85%)], Train Loss: 0.46808\n",
      "Epoch: 00 [17124/20026 ( 86%)], Train Loss: 0.46757\n",
      "Epoch: 00 [17164/20026 ( 86%)], Train Loss: 0.46738\n",
      "Epoch: 00 [17204/20026 ( 86%)], Train Loss: 0.46751\n",
      "Epoch: 00 [17244/20026 ( 86%)], Train Loss: 0.46714\n",
      "Epoch: 00 [17284/20026 ( 86%)], Train Loss: 0.46677\n",
      "Epoch: 00 [17324/20026 ( 87%)], Train Loss: 0.46668\n",
      "Epoch: 00 [17364/20026 ( 87%)], Train Loss: 0.46616\n",
      "Epoch: 00 [17404/20026 ( 87%)], Train Loss: 0.46594\n",
      "Epoch: 00 [17444/20026 ( 87%)], Train Loss: 0.46577\n",
      "Epoch: 00 [17484/20026 ( 87%)], Train Loss: 0.46558\n",
      "Epoch: 00 [17524/20026 ( 88%)], Train Loss: 0.46546\n",
      "Epoch: 00 [17564/20026 ( 88%)], Train Loss: 0.46511\n",
      "Epoch: 00 [17604/20026 ( 88%)], Train Loss: 0.46479\n",
      "Epoch: 00 [17644/20026 ( 88%)], Train Loss: 0.46450\n",
      "Epoch: 00 [17684/20026 ( 88%)], Train Loss: 0.46402\n",
      "Epoch: 00 [17724/20026 ( 89%)], Train Loss: 0.46394\n",
      "Epoch: 00 [17764/20026 ( 89%)], Train Loss: 0.46409\n",
      "Epoch: 00 [17804/20026 ( 89%)], Train Loss: 0.46394\n",
      "Epoch: 00 [17844/20026 ( 89%)], Train Loss: 0.46361\n",
      "Epoch: 00 [17884/20026 ( 89%)], Train Loss: 0.46291\n",
      "Epoch: 00 [17924/20026 ( 90%)], Train Loss: 0.46273\n",
      "Epoch: 00 [17964/20026 ( 90%)], Train Loss: 0.46269\n",
      "Epoch: 00 [18004/20026 ( 90%)], Train Loss: 0.46209\n",
      "Epoch: 00 [18044/20026 ( 90%)], Train Loss: 0.46210\n",
      "Epoch: 00 [18084/20026 ( 90%)], Train Loss: 0.46233\n",
      "Epoch: 00 [18124/20026 ( 91%)], Train Loss: 0.46195\n",
      "Epoch: 00 [18164/20026 ( 91%)], Train Loss: 0.46128\n",
      "Epoch: 00 [18204/20026 ( 91%)], Train Loss: 0.46113\n",
      "Epoch: 00 [18244/20026 ( 91%)], Train Loss: 0.46071\n",
      "Epoch: 00 [18284/20026 ( 91%)], Train Loss: 0.46003\n",
      "Epoch: 00 [18324/20026 ( 92%)], Train Loss: 0.46034\n",
      "Epoch: 00 [18364/20026 ( 92%)], Train Loss: 0.46015\n",
      "Epoch: 00 [18404/20026 ( 92%)], Train Loss: 0.46021\n",
      "Epoch: 00 [18444/20026 ( 92%)], Train Loss: 0.45981\n",
      "Epoch: 00 [18484/20026 ( 92%)], Train Loss: 0.45976\n",
      "Epoch: 00 [18524/20026 ( 92%)], Train Loss: 0.45949\n",
      "Epoch: 00 [18564/20026 ( 93%)], Train Loss: 0.45931\n",
      "Epoch: 00 [18604/20026 ( 93%)], Train Loss: 0.45921\n",
      "Epoch: 00 [18644/20026 ( 93%)], Train Loss: 0.45888\n",
      "Epoch: 00 [18684/20026 ( 93%)], Train Loss: 0.45900\n",
      "Epoch: 00 [18724/20026 ( 93%)], Train Loss: 0.45883\n",
      "Epoch: 00 [18764/20026 ( 94%)], Train Loss: 0.45860\n",
      "Epoch: 00 [18804/20026 ( 94%)], Train Loss: 0.45869\n",
      "Epoch: 00 [18844/20026 ( 94%)], Train Loss: 0.45836\n",
      "Epoch: 00 [18884/20026 ( 94%)], Train Loss: 0.45829\n",
      "Epoch: 00 [18924/20026 ( 94%)], Train Loss: 0.45801\n",
      "Epoch: 00 [18964/20026 ( 95%)], Train Loss: 0.45745\n",
      "Epoch: 00 [19004/20026 ( 95%)], Train Loss: 0.45727\n",
      "Epoch: 00 [19044/20026 ( 95%)], Train Loss: 0.45720\n",
      "Epoch: 00 [19084/20026 ( 95%)], Train Loss: 0.45695\n",
      "Epoch: 00 [19124/20026 ( 95%)], Train Loss: 0.45668\n",
      "Epoch: 00 [19164/20026 ( 96%)], Train Loss: 0.45632\n",
      "Epoch: 00 [19204/20026 ( 96%)], Train Loss: 0.45692\n",
      "Epoch: 00 [19244/20026 ( 96%)], Train Loss: 0.45667\n",
      "Epoch: 00 [19284/20026 ( 96%)], Train Loss: 0.45657\n",
      "Epoch: 00 [19324/20026 ( 96%)], Train Loss: 0.45634\n",
      "Epoch: 00 [19364/20026 ( 97%)], Train Loss: 0.45583\n",
      "Epoch: 00 [19404/20026 ( 97%)], Train Loss: 0.45535\n",
      "Epoch: 00 [19444/20026 ( 97%)], Train Loss: 0.45503\n",
      "Epoch: 00 [19484/20026 ( 97%)], Train Loss: 0.45492\n",
      "Epoch: 00 [19524/20026 ( 97%)], Train Loss: 0.45469\n",
      "Epoch: 00 [19564/20026 ( 98%)], Train Loss: 0.45440\n",
      "Epoch: 00 [19604/20026 ( 98%)], Train Loss: 0.45432\n",
      "Epoch: 00 [19644/20026 ( 98%)], Train Loss: 0.45432\n",
      "Epoch: 00 [19684/20026 ( 98%)], Train Loss: 0.45463\n",
      "Epoch: 00 [19724/20026 ( 98%)], Train Loss: 0.45443\n",
      "Epoch: 00 [19764/20026 ( 99%)], Train Loss: 0.45425\n",
      "Epoch: 00 [19804/20026 ( 99%)], Train Loss: 0.45389\n",
      "Epoch: 00 [19844/20026 ( 99%)], Train Loss: 0.45385\n",
      "Epoch: 00 [19884/20026 ( 99%)], Train Loss: 0.45380\n",
      "Epoch: 00 [19924/20026 ( 99%)], Train Loss: 0.45343\n",
      "Epoch: 00 [19964/20026 (100%)], Train Loss: 0.45360\n",
      "Epoch: 00 [20004/20026 (100%)], Train Loss: 0.45331\n",
      "Epoch: 00 [20026/20026 (100%)], Train Loss: 0.45316\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.19741\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.19741\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 01 [    4/20026 (  0%)], Train Loss: 0.13155\n",
      "Epoch: 01 [   44/20026 (  0%)], Train Loss: 0.33410\n",
      "Epoch: 01 [   84/20026 (  0%)], Train Loss: 0.34850\n",
      "Epoch: 01 [  124/20026 (  1%)], Train Loss: 0.38887\n",
      "Epoch: 01 [  164/20026 (  1%)], Train Loss: 0.38546\n",
      "Epoch: 01 [  204/20026 (  1%)], Train Loss: 0.39002\n",
      "Epoch: 01 [  244/20026 (  1%)], Train Loss: 0.39499\n",
      "Epoch: 01 [  284/20026 (  1%)], Train Loss: 0.37742\n",
      "Epoch: 01 [  324/20026 (  2%)], Train Loss: 0.36693\n",
      "Epoch: 01 [  364/20026 (  2%)], Train Loss: 0.35173\n",
      "Epoch: 01 [  404/20026 (  2%)], Train Loss: 0.36585\n",
      "Epoch: 01 [  444/20026 (  2%)], Train Loss: 0.36905\n",
      "Epoch: 01 [  484/20026 (  2%)], Train Loss: 0.35712\n",
      "Epoch: 01 [  524/20026 (  3%)], Train Loss: 0.34752\n",
      "Epoch: 01 [  564/20026 (  3%)], Train Loss: 0.34708\n",
      "Epoch: 01 [  604/20026 (  3%)], Train Loss: 0.34143\n",
      "Epoch: 01 [  644/20026 (  3%)], Train Loss: 0.34967\n",
      "Epoch: 01 [  684/20026 (  3%)], Train Loss: 0.34036\n",
      "Epoch: 01 [  724/20026 (  4%)], Train Loss: 0.33681\n",
      "Epoch: 01 [  764/20026 (  4%)], Train Loss: 0.33309\n",
      "Epoch: 01 [  804/20026 (  4%)], Train Loss: 0.33422\n",
      "Epoch: 01 [  844/20026 (  4%)], Train Loss: 0.33627\n",
      "Epoch: 01 [  884/20026 (  4%)], Train Loss: 0.33201\n",
      "Epoch: 01 [  924/20026 (  5%)], Train Loss: 0.32828\n",
      "Epoch: 01 [  964/20026 (  5%)], Train Loss: 0.32486\n",
      "Epoch: 01 [ 1004/20026 (  5%)], Train Loss: 0.31942\n",
      "Epoch: 01 [ 1044/20026 (  5%)], Train Loss: 0.31875\n",
      "Epoch: 01 [ 1084/20026 (  5%)], Train Loss: 0.31213\n",
      "Epoch: 01 [ 1124/20026 (  6%)], Train Loss: 0.30781\n",
      "Epoch: 01 [ 1164/20026 (  6%)], Train Loss: 0.30302\n",
      "Epoch: 01 [ 1204/20026 (  6%)], Train Loss: 0.29774\n",
      "Epoch: 01 [ 1244/20026 (  6%)], Train Loss: 0.29532\n",
      "Epoch: 01 [ 1284/20026 (  6%)], Train Loss: 0.29022\n",
      "Epoch: 01 [ 1324/20026 (  7%)], Train Loss: 0.29784\n",
      "Epoch: 01 [ 1364/20026 (  7%)], Train Loss: 0.29484\n",
      "Epoch: 01 [ 1404/20026 (  7%)], Train Loss: 0.29377\n",
      "Epoch: 01 [ 1444/20026 (  7%)], Train Loss: 0.29283\n",
      "Epoch: 01 [ 1484/20026 (  7%)], Train Loss: 0.29246\n",
      "Epoch: 01 [ 1524/20026 (  8%)], Train Loss: 0.29387\n",
      "Epoch: 01 [ 1564/20026 (  8%)], Train Loss: 0.29974\n",
      "Epoch: 01 [ 1604/20026 (  8%)], Train Loss: 0.29811\n",
      "Epoch: 01 [ 1644/20026 (  8%)], Train Loss: 0.29867\n",
      "Epoch: 01 [ 1684/20026 (  8%)], Train Loss: 0.29619\n",
      "Epoch: 01 [ 1724/20026 (  9%)], Train Loss: 0.29685\n",
      "Epoch: 01 [ 1764/20026 (  9%)], Train Loss: 0.29703\n",
      "Epoch: 01 [ 1804/20026 (  9%)], Train Loss: 0.29402\n",
      "Epoch: 01 [ 1844/20026 (  9%)], Train Loss: 0.29156\n",
      "Epoch: 01 [ 1884/20026 (  9%)], Train Loss: 0.29186\n",
      "Epoch: 01 [ 1924/20026 ( 10%)], Train Loss: 0.28919\n",
      "Epoch: 01 [ 1964/20026 ( 10%)], Train Loss: 0.29045\n",
      "Epoch: 01 [ 2004/20026 ( 10%)], Train Loss: 0.28845\n",
      "Epoch: 01 [ 2044/20026 ( 10%)], Train Loss: 0.28701\n",
      "Epoch: 01 [ 2084/20026 ( 10%)], Train Loss: 0.28361\n",
      "Epoch: 01 [ 2124/20026 ( 11%)], Train Loss: 0.28375\n",
      "Epoch: 01 [ 2164/20026 ( 11%)], Train Loss: 0.28523\n",
      "Epoch: 01 [ 2204/20026 ( 11%)], Train Loss: 0.28304\n",
      "Epoch: 01 [ 2244/20026 ( 11%)], Train Loss: 0.28225\n",
      "Epoch: 01 [ 2284/20026 ( 11%)], Train Loss: 0.27929\n",
      "Epoch: 01 [ 2324/20026 ( 12%)], Train Loss: 0.28073\n",
      "Epoch: 01 [ 2364/20026 ( 12%)], Train Loss: 0.27822\n",
      "Epoch: 01 [ 2404/20026 ( 12%)], Train Loss: 0.27620\n",
      "Epoch: 01 [ 2444/20026 ( 12%)], Train Loss: 0.27596\n",
      "Epoch: 01 [ 2484/20026 ( 12%)], Train Loss: 0.27614\n",
      "Epoch: 01 [ 2524/20026 ( 13%)], Train Loss: 0.27355\n",
      "Epoch: 01 [ 2564/20026 ( 13%)], Train Loss: 0.27282\n",
      "Epoch: 01 [ 2604/20026 ( 13%)], Train Loss: 0.27395\n",
      "Epoch: 01 [ 2644/20026 ( 13%)], Train Loss: 0.27452\n",
      "Epoch: 01 [ 2684/20026 ( 13%)], Train Loss: 0.27261\n",
      "Epoch: 01 [ 2724/20026 ( 14%)], Train Loss: 0.27099\n",
      "Epoch: 01 [ 2764/20026 ( 14%)], Train Loss: 0.26917\n",
      "Epoch: 01 [ 2804/20026 ( 14%)], Train Loss: 0.26811\n",
      "Epoch: 01 [ 2844/20026 ( 14%)], Train Loss: 0.26680\n",
      "Epoch: 01 [ 2884/20026 ( 14%)], Train Loss: 0.26454\n",
      "Epoch: 01 [ 2924/20026 ( 15%)], Train Loss: 0.26247\n",
      "Epoch: 01 [ 2964/20026 ( 15%)], Train Loss: 0.26148\n",
      "Epoch: 01 [ 3004/20026 ( 15%)], Train Loss: 0.25916\n",
      "Epoch: 01 [ 3044/20026 ( 15%)], Train Loss: 0.25677\n",
      "Epoch: 01 [ 3084/20026 ( 15%)], Train Loss: 0.25522\n",
      "Epoch: 01 [ 3124/20026 ( 16%)], Train Loss: 0.25380\n",
      "Epoch: 01 [ 3164/20026 ( 16%)], Train Loss: 0.25340\n",
      "Epoch: 01 [ 3204/20026 ( 16%)], Train Loss: 0.25281\n",
      "Epoch: 01 [ 3244/20026 ( 16%)], Train Loss: 0.25295\n",
      "Epoch: 01 [ 3284/20026 ( 16%)], Train Loss: 0.25105\n",
      "Epoch: 01 [ 3324/20026 ( 17%)], Train Loss: 0.24952\n",
      "Epoch: 01 [ 3364/20026 ( 17%)], Train Loss: 0.24821\n",
      "Epoch: 01 [ 3404/20026 ( 17%)], Train Loss: 0.24786\n",
      "Epoch: 01 [ 3444/20026 ( 17%)], Train Loss: 0.25031\n",
      "Epoch: 01 [ 3484/20026 ( 17%)], Train Loss: 0.25023\n",
      "Epoch: 01 [ 3524/20026 ( 18%)], Train Loss: 0.25082\n",
      "Epoch: 01 [ 3564/20026 ( 18%)], Train Loss: 0.24957\n",
      "Epoch: 01 [ 3604/20026 ( 18%)], Train Loss: 0.24857\n",
      "Epoch: 01 [ 3644/20026 ( 18%)], Train Loss: 0.24767\n",
      "Epoch: 01 [ 3684/20026 ( 18%)], Train Loss: 0.24732\n",
      "Epoch: 01 [ 3724/20026 ( 19%)], Train Loss: 0.24632\n",
      "Epoch: 01 [ 3764/20026 ( 19%)], Train Loss: 0.24595\n",
      "Epoch: 01 [ 3804/20026 ( 19%)], Train Loss: 0.24439\n",
      "Epoch: 01 [ 3844/20026 ( 19%)], Train Loss: 0.24297\n",
      "Epoch: 01 [ 3884/20026 ( 19%)], Train Loss: 0.24137\n",
      "Epoch: 01 [ 3924/20026 ( 20%)], Train Loss: 0.24138\n",
      "Epoch: 01 [ 3964/20026 ( 20%)], Train Loss: 0.23937\n",
      "Epoch: 01 [ 4004/20026 ( 20%)], Train Loss: 0.23858\n",
      "Epoch: 01 [ 4044/20026 ( 20%)], Train Loss: 0.23680\n",
      "Epoch: 01 [ 4084/20026 ( 20%)], Train Loss: 0.23722\n",
      "Epoch: 01 [ 4124/20026 ( 21%)], Train Loss: 0.23668\n",
      "Epoch: 01 [ 4164/20026 ( 21%)], Train Loss: 0.23517\n",
      "Epoch: 01 [ 4204/20026 ( 21%)], Train Loss: 0.23435\n",
      "Epoch: 01 [ 4244/20026 ( 21%)], Train Loss: 0.23283\n",
      "Epoch: 01 [ 4284/20026 ( 21%)], Train Loss: 0.23201\n",
      "Epoch: 01 [ 4324/20026 ( 22%)], Train Loss: 0.23064\n",
      "Epoch: 01 [ 4364/20026 ( 22%)], Train Loss: 0.23099\n",
      "Epoch: 01 [ 4404/20026 ( 22%)], Train Loss: 0.22997\n",
      "Epoch: 01 [ 4444/20026 ( 22%)], Train Loss: 0.22884\n",
      "Epoch: 01 [ 4484/20026 ( 22%)], Train Loss: 0.22838\n",
      "Epoch: 01 [ 4524/20026 ( 23%)], Train Loss: 0.22733\n",
      "Epoch: 01 [ 4564/20026 ( 23%)], Train Loss: 0.22621\n",
      "Epoch: 01 [ 4604/20026 ( 23%)], Train Loss: 0.22489\n",
      "Epoch: 01 [ 4644/20026 ( 23%)], Train Loss: 0.22359\n",
      "Epoch: 01 [ 4684/20026 ( 23%)], Train Loss: 0.22233\n",
      "Epoch: 01 [ 4724/20026 ( 24%)], Train Loss: 0.22084\n",
      "Epoch: 01 [ 4764/20026 ( 24%)], Train Loss: 0.21978\n",
      "Epoch: 01 [ 4804/20026 ( 24%)], Train Loss: 0.21952\n",
      "Epoch: 01 [ 4844/20026 ( 24%)], Train Loss: 0.21909\n",
      "Epoch: 01 [ 4884/20026 ( 24%)], Train Loss: 0.21854\n",
      "Epoch: 01 [ 4924/20026 ( 25%)], Train Loss: 0.21885\n",
      "Epoch: 01 [ 4964/20026 ( 25%)], Train Loss: 0.21839\n",
      "Epoch: 01 [ 5004/20026 ( 25%)], Train Loss: 0.22032\n",
      "Epoch: 01 [ 5044/20026 ( 25%)], Train Loss: 0.21939\n",
      "Epoch: 01 [ 5084/20026 ( 25%)], Train Loss: 0.21822\n",
      "Epoch: 01 [ 5124/20026 ( 26%)], Train Loss: 0.21848\n",
      "Epoch: 01 [ 5164/20026 ( 26%)], Train Loss: 0.21785\n",
      "Epoch: 01 [ 5204/20026 ( 26%)], Train Loss: 0.21783\n",
      "Epoch: 01 [ 5244/20026 ( 26%)], Train Loss: 0.21720\n",
      "Epoch: 01 [ 5284/20026 ( 26%)], Train Loss: 0.21682\n",
      "Epoch: 01 [ 5324/20026 ( 27%)], Train Loss: 0.21730\n",
      "Epoch: 01 [ 5364/20026 ( 27%)], Train Loss: 0.21755\n",
      "Epoch: 01 [ 5404/20026 ( 27%)], Train Loss: 0.21648\n",
      "Epoch: 01 [ 5444/20026 ( 27%)], Train Loss: 0.21646\n",
      "Epoch: 01 [ 5484/20026 ( 27%)], Train Loss: 0.21582\n",
      "Epoch: 01 [ 5524/20026 ( 28%)], Train Loss: 0.21468\n",
      "Epoch: 01 [ 5564/20026 ( 28%)], Train Loss: 0.21365\n",
      "Epoch: 01 [ 5604/20026 ( 28%)], Train Loss: 0.21318\n",
      "Epoch: 01 [ 5644/20026 ( 28%)], Train Loss: 0.21241\n",
      "Epoch: 01 [ 5684/20026 ( 28%)], Train Loss: 0.21175\n",
      "Epoch: 01 [ 5724/20026 ( 29%)], Train Loss: 0.21091\n",
      "Epoch: 01 [ 5764/20026 ( 29%)], Train Loss: 0.21055\n",
      "Epoch: 01 [ 5804/20026 ( 29%)], Train Loss: 0.21041\n",
      "Epoch: 01 [ 5844/20026 ( 29%)], Train Loss: 0.21009\n",
      "Epoch: 01 [ 5884/20026 ( 29%)], Train Loss: 0.20952\n",
      "Epoch: 01 [ 5924/20026 ( 30%)], Train Loss: 0.20878\n",
      "Epoch: 01 [ 5964/20026 ( 30%)], Train Loss: 0.20867\n",
      "Epoch: 01 [ 6004/20026 ( 30%)], Train Loss: 0.20838\n",
      "Epoch: 01 [ 6044/20026 ( 30%)], Train Loss: 0.20816\n",
      "Epoch: 01 [ 6084/20026 ( 30%)], Train Loss: 0.20719\n",
      "Epoch: 01 [ 6124/20026 ( 31%)], Train Loss: 0.20658\n",
      "Epoch: 01 [ 6164/20026 ( 31%)], Train Loss: 0.20647\n",
      "Epoch: 01 [ 6204/20026 ( 31%)], Train Loss: 0.20584\n",
      "Epoch: 01 [ 6244/20026 ( 31%)], Train Loss: 0.20505\n",
      "Epoch: 01 [ 6284/20026 ( 31%)], Train Loss: 0.20420\n",
      "Epoch: 01 [ 6324/20026 ( 32%)], Train Loss: 0.20423\n",
      "Epoch: 01 [ 6364/20026 ( 32%)], Train Loss: 0.20355\n",
      "Epoch: 01 [ 6404/20026 ( 32%)], Train Loss: 0.20244\n",
      "Epoch: 01 [ 6444/20026 ( 32%)], Train Loss: 0.20145\n",
      "Epoch: 01 [ 6484/20026 ( 32%)], Train Loss: 0.20112\n",
      "Epoch: 01 [ 6524/20026 ( 33%)], Train Loss: 0.20049\n",
      "Epoch: 01 [ 6564/20026 ( 33%)], Train Loss: 0.19977\n",
      "Epoch: 01 [ 6604/20026 ( 33%)], Train Loss: 0.19933\n",
      "Epoch: 01 [ 6644/20026 ( 33%)], Train Loss: 0.19911\n",
      "Epoch: 01 [ 6684/20026 ( 33%)], Train Loss: 0.19853\n",
      "Epoch: 01 [ 6724/20026 ( 34%)], Train Loss: 0.19827\n",
      "Epoch: 01 [ 6764/20026 ( 34%)], Train Loss: 0.19785\n",
      "Epoch: 01 [ 6804/20026 ( 34%)], Train Loss: 0.19732\n",
      "Epoch: 01 [ 6844/20026 ( 34%)], Train Loss: 0.19648\n",
      "Epoch: 01 [ 6884/20026 ( 34%)], Train Loss: 0.19561\n",
      "Epoch: 01 [ 6924/20026 ( 35%)], Train Loss: 0.19599\n",
      "Epoch: 01 [ 6964/20026 ( 35%)], Train Loss: 0.19585\n",
      "Epoch: 01 [ 7004/20026 ( 35%)], Train Loss: 0.19519\n",
      "Epoch: 01 [ 7044/20026 ( 35%)], Train Loss: 0.19464\n",
      "Epoch: 01 [ 7084/20026 ( 35%)], Train Loss: 0.19418\n",
      "Epoch: 01 [ 7124/20026 ( 36%)], Train Loss: 0.19372\n",
      "Epoch: 01 [ 7164/20026 ( 36%)], Train Loss: 0.19456\n",
      "Epoch: 01 [ 7204/20026 ( 36%)], Train Loss: 0.19406\n",
      "Epoch: 01 [ 7244/20026 ( 36%)], Train Loss: 0.19359\n",
      "Epoch: 01 [ 7284/20026 ( 36%)], Train Loss: 0.19311\n",
      "Epoch: 01 [ 7324/20026 ( 37%)], Train Loss: 0.19317\n",
      "Epoch: 01 [ 7364/20026 ( 37%)], Train Loss: 0.19311\n",
      "Epoch: 01 [ 7404/20026 ( 37%)], Train Loss: 0.19300\n",
      "Epoch: 01 [ 7444/20026 ( 37%)], Train Loss: 0.19227\n",
      "Epoch: 01 [ 7484/20026 ( 37%)], Train Loss: 0.19176\n",
      "Epoch: 01 [ 7524/20026 ( 38%)], Train Loss: 0.19130\n",
      "Epoch: 01 [ 7564/20026 ( 38%)], Train Loss: 0.19057\n",
      "Epoch: 01 [ 7604/20026 ( 38%)], Train Loss: 0.19013\n",
      "Epoch: 01 [ 7644/20026 ( 38%)], Train Loss: 0.19032\n",
      "Epoch: 01 [ 7684/20026 ( 38%)], Train Loss: 0.19008\n",
      "Epoch: 01 [ 7724/20026 ( 39%)], Train Loss: 0.18938\n",
      "Epoch: 01 [ 7764/20026 ( 39%)], Train Loss: 0.18904\n",
      "Epoch: 01 [ 7804/20026 ( 39%)], Train Loss: 0.18936\n",
      "Epoch: 01 [ 7844/20026 ( 39%)], Train Loss: 0.18910\n",
      "Epoch: 01 [ 7884/20026 ( 39%)], Train Loss: 0.18938\n",
      "Epoch: 01 [ 7924/20026 ( 40%)], Train Loss: 0.18980\n",
      "Epoch: 01 [ 7964/20026 ( 40%)], Train Loss: 0.18923\n",
      "Epoch: 01 [ 8004/20026 ( 40%)], Train Loss: 0.18885\n",
      "Epoch: 01 [ 8044/20026 ( 40%)], Train Loss: 0.18877\n",
      "Epoch: 01 [ 8084/20026 ( 40%)], Train Loss: 0.18806\n",
      "Epoch: 01 [ 8124/20026 ( 41%)], Train Loss: 0.18744\n",
      "Epoch: 01 [ 8164/20026 ( 41%)], Train Loss: 0.18685\n",
      "Epoch: 01 [ 8204/20026 ( 41%)], Train Loss: 0.18626\n",
      "Epoch: 01 [ 8244/20026 ( 41%)], Train Loss: 0.18574\n",
      "Epoch: 01 [ 8284/20026 ( 41%)], Train Loss: 0.18506\n",
      "Epoch: 01 [ 8324/20026 ( 42%)], Train Loss: 0.18533\n",
      "Epoch: 01 [ 8364/20026 ( 42%)], Train Loss: 0.18510\n",
      "Epoch: 01 [ 8404/20026 ( 42%)], Train Loss: 0.18492\n",
      "Epoch: 01 [ 8444/20026 ( 42%)], Train Loss: 0.18495\n",
      "Epoch: 01 [ 8484/20026 ( 42%)], Train Loss: 0.18503\n",
      "Epoch: 01 [ 8524/20026 ( 43%)], Train Loss: 0.18459\n",
      "Epoch: 01 [ 8564/20026 ( 43%)], Train Loss: 0.18428\n",
      "Epoch: 01 [ 8604/20026 ( 43%)], Train Loss: 0.18375\n",
      "Epoch: 01 [ 8644/20026 ( 43%)], Train Loss: 0.18354\n",
      "Epoch: 01 [ 8684/20026 ( 43%)], Train Loss: 0.18300\n",
      "Epoch: 01 [ 8724/20026 ( 44%)], Train Loss: 0.18253\n",
      "Epoch: 01 [ 8764/20026 ( 44%)], Train Loss: 0.18223\n",
      "Epoch: 01 [ 8804/20026 ( 44%)], Train Loss: 0.18182\n",
      "Epoch: 01 [ 8844/20026 ( 44%)], Train Loss: 0.18124\n",
      "Epoch: 01 [ 8884/20026 ( 44%)], Train Loss: 0.18093\n",
      "Epoch: 01 [ 8924/20026 ( 45%)], Train Loss: 0.18091\n",
      "Epoch: 01 [ 8964/20026 ( 45%)], Train Loss: 0.18081\n",
      "Epoch: 01 [ 9004/20026 ( 45%)], Train Loss: 0.18060\n",
      "Epoch: 01 [ 9044/20026 ( 45%)], Train Loss: 0.18011\n",
      "Epoch: 01 [ 9084/20026 ( 45%)], Train Loss: 0.17986\n",
      "Epoch: 01 [ 9124/20026 ( 46%)], Train Loss: 0.17932\n",
      "Epoch: 01 [ 9164/20026 ( 46%)], Train Loss: 0.17895\n",
      "Epoch: 01 [ 9204/20026 ( 46%)], Train Loss: 0.17833\n",
      "Epoch: 01 [ 9244/20026 ( 46%)], Train Loss: 0.17827\n",
      "Epoch: 01 [ 9284/20026 ( 46%)], Train Loss: 0.17776\n",
      "Epoch: 01 [ 9324/20026 ( 47%)], Train Loss: 0.17746\n",
      "Epoch: 01 [ 9364/20026 ( 47%)], Train Loss: 0.17699\n",
      "Epoch: 01 [ 9404/20026 ( 47%)], Train Loss: 0.17657\n",
      "Epoch: 01 [ 9444/20026 ( 47%)], Train Loss: 0.17660\n",
      "Epoch: 01 [ 9484/20026 ( 47%)], Train Loss: 0.17612\n",
      "Epoch: 01 [ 9524/20026 ( 48%)], Train Loss: 0.17582\n",
      "Epoch: 01 [ 9564/20026 ( 48%)], Train Loss: 0.17552\n",
      "Epoch: 01 [ 9604/20026 ( 48%)], Train Loss: 0.17550\n",
      "Epoch: 01 [ 9644/20026 ( 48%)], Train Loss: 0.17577\n",
      "Epoch: 01 [ 9684/20026 ( 48%)], Train Loss: 0.17580\n",
      "Epoch: 01 [ 9724/20026 ( 49%)], Train Loss: 0.17544\n",
      "Epoch: 01 [ 9764/20026 ( 49%)], Train Loss: 0.17556\n",
      "Epoch: 01 [ 9804/20026 ( 49%)], Train Loss: 0.17531\n",
      "Epoch: 01 [ 9844/20026 ( 49%)], Train Loss: 0.17563\n",
      "Epoch: 01 [ 9884/20026 ( 49%)], Train Loss: 0.17521\n",
      "Epoch: 01 [ 9924/20026 ( 50%)], Train Loss: 0.17502\n",
      "Epoch: 01 [ 9964/20026 ( 50%)], Train Loss: 0.17459\n",
      "Epoch: 01 [10004/20026 ( 50%)], Train Loss: 0.17470\n",
      "Epoch: 01 [10044/20026 ( 50%)], Train Loss: 0.17475\n",
      "Epoch: 01 [10084/20026 ( 50%)], Train Loss: 0.17434\n",
      "Epoch: 01 [10124/20026 ( 51%)], Train Loss: 0.17413\n",
      "Epoch: 01 [10164/20026 ( 51%)], Train Loss: 0.17398\n",
      "Epoch: 01 [10204/20026 ( 51%)], Train Loss: 0.17353\n",
      "Epoch: 01 [10244/20026 ( 51%)], Train Loss: 0.17353\n",
      "Epoch: 01 [10284/20026 ( 51%)], Train Loss: 0.17317\n",
      "Epoch: 01 [10324/20026 ( 52%)], Train Loss: 0.17256\n",
      "Epoch: 01 [10364/20026 ( 52%)], Train Loss: 0.17207\n",
      "Epoch: 01 [10404/20026 ( 52%)], Train Loss: 0.17220\n",
      "Epoch: 01 [10444/20026 ( 52%)], Train Loss: 0.17199\n",
      "Epoch: 01 [10484/20026 ( 52%)], Train Loss: 0.17168\n",
      "Epoch: 01 [10524/20026 ( 53%)], Train Loss: 0.17109\n",
      "Epoch: 01 [10564/20026 ( 53%)], Train Loss: 0.17065\n",
      "Epoch: 01 [10604/20026 ( 53%)], Train Loss: 0.17032\n",
      "Epoch: 01 [10644/20026 ( 53%)], Train Loss: 0.17029\n",
      "Epoch: 01 [10684/20026 ( 53%)], Train Loss: 0.17006\n",
      "Epoch: 01 [10724/20026 ( 54%)], Train Loss: 0.16973\n",
      "Epoch: 01 [10764/20026 ( 54%)], Train Loss: 0.16959\n",
      "Epoch: 01 [10804/20026 ( 54%)], Train Loss: 0.16918\n",
      "Epoch: 01 [10844/20026 ( 54%)], Train Loss: 0.16886\n",
      "Epoch: 01 [10884/20026 ( 54%)], Train Loss: 0.16850\n",
      "Epoch: 01 [10924/20026 ( 55%)], Train Loss: 0.16814\n",
      "Epoch: 01 [10964/20026 ( 55%)], Train Loss: 0.16765\n",
      "Epoch: 01 [11004/20026 ( 55%)], Train Loss: 0.16798\n",
      "Epoch: 01 [11044/20026 ( 55%)], Train Loss: 0.16783\n",
      "Epoch: 01 [11084/20026 ( 55%)], Train Loss: 0.16757\n",
      "Epoch: 01 [11124/20026 ( 56%)], Train Loss: 0.16768\n",
      "Epoch: 01 [11164/20026 ( 56%)], Train Loss: 0.16745\n",
      "Epoch: 01 [11204/20026 ( 56%)], Train Loss: 0.16759\n",
      "Epoch: 01 [11244/20026 ( 56%)], Train Loss: 0.16720\n",
      "Epoch: 01 [11284/20026 ( 56%)], Train Loss: 0.16734\n",
      "Epoch: 01 [11324/20026 ( 57%)], Train Loss: 0.16694\n",
      "Epoch: 01 [11364/20026 ( 57%)], Train Loss: 0.16692\n",
      "Epoch: 01 [11404/20026 ( 57%)], Train Loss: 0.16785\n",
      "Epoch: 01 [11444/20026 ( 57%)], Train Loss: 0.16753\n",
      "Epoch: 01 [11484/20026 ( 57%)], Train Loss: 0.16767\n",
      "Epoch: 01 [11524/20026 ( 58%)], Train Loss: 0.16739\n",
      "Epoch: 01 [11564/20026 ( 58%)], Train Loss: 0.16732\n",
      "Epoch: 01 [11604/20026 ( 58%)], Train Loss: 0.16733\n",
      "Epoch: 01 [11644/20026 ( 58%)], Train Loss: 0.16722\n",
      "Epoch: 01 [11684/20026 ( 58%)], Train Loss: 0.16704\n",
      "Epoch: 01 [11724/20026 ( 59%)], Train Loss: 0.16701\n",
      "Epoch: 01 [11764/20026 ( 59%)], Train Loss: 0.16653\n",
      "Epoch: 01 [11804/20026 ( 59%)], Train Loss: 0.16626\n",
      "Epoch: 01 [11844/20026 ( 59%)], Train Loss: 0.16593\n",
      "Epoch: 01 [11884/20026 ( 59%)], Train Loss: 0.16566\n",
      "Epoch: 01 [11924/20026 ( 60%)], Train Loss: 0.16544\n",
      "Epoch: 01 [11964/20026 ( 60%)], Train Loss: 0.16523\n",
      "Epoch: 01 [12004/20026 ( 60%)], Train Loss: 0.16476\n",
      "Epoch: 01 [12044/20026 ( 60%)], Train Loss: 0.16439\n",
      "Epoch: 01 [12084/20026 ( 60%)], Train Loss: 0.16438\n",
      "Epoch: 01 [12124/20026 ( 61%)], Train Loss: 0.16398\n",
      "Epoch: 01 [12164/20026 ( 61%)], Train Loss: 0.16358\n",
      "Epoch: 01 [12204/20026 ( 61%)], Train Loss: 0.16349\n",
      "Epoch: 01 [12244/20026 ( 61%)], Train Loss: 0.16323\n",
      "Epoch: 01 [12284/20026 ( 61%)], Train Loss: 0.16287\n",
      "Epoch: 01 [12324/20026 ( 62%)], Train Loss: 0.16250\n",
      "Epoch: 01 [12364/20026 ( 62%)], Train Loss: 0.16234\n",
      "Epoch: 01 [12404/20026 ( 62%)], Train Loss: 0.16213\n",
      "Epoch: 01 [12444/20026 ( 62%)], Train Loss: 0.16217\n",
      "Epoch: 01 [12484/20026 ( 62%)], Train Loss: 0.16193\n",
      "Epoch: 01 [12524/20026 ( 63%)], Train Loss: 0.16176\n",
      "Epoch: 01 [12564/20026 ( 63%)], Train Loss: 0.16133\n",
      "Epoch: 01 [12604/20026 ( 63%)], Train Loss: 0.16171\n",
      "Epoch: 01 [12644/20026 ( 63%)], Train Loss: 0.16158\n",
      "Epoch: 01 [12684/20026 ( 63%)], Train Loss: 0.16156\n",
      "Epoch: 01 [12724/20026 ( 64%)], Train Loss: 0.16152\n",
      "Epoch: 01 [12764/20026 ( 64%)], Train Loss: 0.16179\n",
      "Epoch: 01 [12804/20026 ( 64%)], Train Loss: 0.16180\n",
      "Epoch: 01 [12844/20026 ( 64%)], Train Loss: 0.16174\n",
      "Epoch: 01 [12884/20026 ( 64%)], Train Loss: 0.16146\n",
      "Epoch: 01 [12924/20026 ( 65%)], Train Loss: 0.16127\n",
      "Epoch: 01 [12964/20026 ( 65%)], Train Loss: 0.16099\n",
      "Epoch: 01 [13004/20026 ( 65%)], Train Loss: 0.16074\n",
      "Epoch: 01 [13044/20026 ( 65%)], Train Loss: 0.16072\n",
      "Epoch: 01 [13084/20026 ( 65%)], Train Loss: 0.16068\n",
      "Epoch: 01 [13124/20026 ( 66%)], Train Loss: 0.16052\n",
      "Epoch: 01 [13164/20026 ( 66%)], Train Loss: 0.16030\n",
      "Epoch: 01 [13204/20026 ( 66%)], Train Loss: 0.15998\n",
      "Epoch: 01 [13244/20026 ( 66%)], Train Loss: 0.15973\n",
      "Epoch: 01 [13284/20026 ( 66%)], Train Loss: 0.15953\n",
      "Epoch: 01 [13324/20026 ( 67%)], Train Loss: 0.15928\n",
      "Epoch: 01 [13364/20026 ( 67%)], Train Loss: 0.15938\n",
      "Epoch: 01 [13404/20026 ( 67%)], Train Loss: 0.15916\n",
      "Epoch: 01 [13444/20026 ( 67%)], Train Loss: 0.15898\n",
      "Epoch: 01 [13484/20026 ( 67%)], Train Loss: 0.15904\n",
      "Epoch: 01 [13524/20026 ( 68%)], Train Loss: 0.15917\n",
      "Epoch: 01 [13564/20026 ( 68%)], Train Loss: 0.15905\n",
      "Epoch: 01 [13604/20026 ( 68%)], Train Loss: 0.15880\n",
      "Epoch: 01 [13644/20026 ( 68%)], Train Loss: 0.15875\n",
      "Epoch: 01 [13684/20026 ( 68%)], Train Loss: 0.15850\n",
      "Epoch: 01 [13724/20026 ( 69%)], Train Loss: 0.15828\n",
      "Epoch: 01 [13764/20026 ( 69%)], Train Loss: 0.15795\n",
      "Epoch: 01 [13804/20026 ( 69%)], Train Loss: 0.15777\n",
      "Epoch: 01 [13844/20026 ( 69%)], Train Loss: 0.15747\n",
      "Epoch: 01 [13884/20026 ( 69%)], Train Loss: 0.15721\n",
      "Epoch: 01 [13924/20026 ( 70%)], Train Loss: 0.15730\n",
      "Epoch: 01 [13964/20026 ( 70%)], Train Loss: 0.15743\n",
      "Epoch: 01 [14004/20026 ( 70%)], Train Loss: 0.15748\n",
      "Epoch: 01 [14044/20026 ( 70%)], Train Loss: 0.15732\n",
      "Epoch: 01 [14084/20026 ( 70%)], Train Loss: 0.15719\n",
      "Epoch: 01 [14124/20026 ( 71%)], Train Loss: 0.15703\n",
      "Epoch: 01 [14164/20026 ( 71%)], Train Loss: 0.15703\n",
      "Epoch: 01 [14204/20026 ( 71%)], Train Loss: 0.15672\n",
      "Epoch: 01 [14244/20026 ( 71%)], Train Loss: 0.15675\n",
      "Epoch: 01 [14284/20026 ( 71%)], Train Loss: 0.15655\n",
      "Epoch: 01 [14324/20026 ( 72%)], Train Loss: 0.15648\n",
      "Epoch: 01 [14364/20026 ( 72%)], Train Loss: 0.15648\n",
      "Epoch: 01 [14404/20026 ( 72%)], Train Loss: 0.15632\n",
      "Epoch: 01 [14444/20026 ( 72%)], Train Loss: 0.15632\n",
      "Epoch: 01 [14484/20026 ( 72%)], Train Loss: 0.15631\n",
      "Epoch: 01 [14524/20026 ( 73%)], Train Loss: 0.15614\n",
      "Epoch: 01 [14564/20026 ( 73%)], Train Loss: 0.15596\n",
      "Epoch: 01 [14604/20026 ( 73%)], Train Loss: 0.15587\n",
      "Epoch: 01 [14644/20026 ( 73%)], Train Loss: 0.15559\n",
      "Epoch: 01 [14684/20026 ( 73%)], Train Loss: 0.15540\n",
      "Epoch: 01 [14724/20026 ( 74%)], Train Loss: 0.15520\n",
      "Epoch: 01 [14764/20026 ( 74%)], Train Loss: 0.15537\n",
      "Epoch: 01 [14804/20026 ( 74%)], Train Loss: 0.15531\n",
      "Epoch: 01 [14844/20026 ( 74%)], Train Loss: 0.15504\n",
      "Epoch: 01 [14884/20026 ( 74%)], Train Loss: 0.15487\n",
      "Epoch: 01 [14924/20026 ( 75%)], Train Loss: 0.15490\n",
      "Epoch: 01 [14964/20026 ( 75%)], Train Loss: 0.15471\n",
      "Epoch: 01 [15004/20026 ( 75%)], Train Loss: 0.15464\n",
      "Epoch: 01 [15044/20026 ( 75%)], Train Loss: 0.15447\n",
      "Epoch: 01 [15084/20026 ( 75%)], Train Loss: 0.15429\n",
      "Epoch: 01 [15124/20026 ( 76%)], Train Loss: 0.15412\n",
      "Epoch: 01 [15164/20026 ( 76%)], Train Loss: 0.15423\n",
      "Epoch: 01 [15204/20026 ( 76%)], Train Loss: 0.15390\n",
      "Epoch: 01 [15244/20026 ( 76%)], Train Loss: 0.15362\n",
      "Epoch: 01 [15284/20026 ( 76%)], Train Loss: 0.15336\n",
      "Epoch: 01 [15324/20026 ( 77%)], Train Loss: 0.15317\n",
      "Epoch: 01 [15364/20026 ( 77%)], Train Loss: 0.15294\n",
      "Epoch: 01 [15404/20026 ( 77%)], Train Loss: 0.15275\n",
      "Epoch: 01 [15444/20026 ( 77%)], Train Loss: 0.15261\n",
      "Epoch: 01 [15484/20026 ( 77%)], Train Loss: 0.15235\n",
      "Epoch: 01 [15524/20026 ( 78%)], Train Loss: 0.15208\n",
      "Epoch: 01 [15564/20026 ( 78%)], Train Loss: 0.15200\n",
      "Epoch: 01 [15604/20026 ( 78%)], Train Loss: 0.15189\n",
      "Epoch: 01 [15644/20026 ( 78%)], Train Loss: 0.15191\n",
      "Epoch: 01 [15684/20026 ( 78%)], Train Loss: 0.15161\n",
      "Epoch: 01 [15724/20026 ( 79%)], Train Loss: 0.15151\n",
      "Epoch: 01 [15764/20026 ( 79%)], Train Loss: 0.15145\n",
      "Epoch: 01 [15804/20026 ( 79%)], Train Loss: 0.15123\n",
      "Epoch: 01 [15844/20026 ( 79%)], Train Loss: 0.15107\n",
      "Epoch: 01 [15884/20026 ( 79%)], Train Loss: 0.15095\n",
      "Epoch: 01 [15924/20026 ( 80%)], Train Loss: 0.15066\n",
      "Epoch: 01 [15964/20026 ( 80%)], Train Loss: 0.15082\n",
      "Epoch: 01 [16004/20026 ( 80%)], Train Loss: 0.15095\n",
      "Epoch: 01 [16044/20026 ( 80%)], Train Loss: 0.15121\n",
      "Epoch: 01 [16084/20026 ( 80%)], Train Loss: 0.15114\n",
      "Epoch: 01 [16124/20026 ( 81%)], Train Loss: 0.15094\n",
      "Epoch: 01 [16164/20026 ( 81%)], Train Loss: 0.15090\n",
      "Epoch: 01 [16204/20026 ( 81%)], Train Loss: 0.15061\n",
      "Epoch: 01 [16244/20026 ( 81%)], Train Loss: 0.15043\n",
      "Epoch: 01 [16284/20026 ( 81%)], Train Loss: 0.15038\n",
      "Epoch: 01 [16324/20026 ( 82%)], Train Loss: 0.15022\n",
      "Epoch: 01 [16364/20026 ( 82%)], Train Loss: 0.15003\n",
      "Epoch: 01 [16404/20026 ( 82%)], Train Loss: 0.14979\n",
      "Epoch: 01 [16444/20026 ( 82%)], Train Loss: 0.15002\n",
      "Epoch: 01 [16484/20026 ( 82%)], Train Loss: 0.15014\n",
      "Epoch: 01 [16524/20026 ( 83%)], Train Loss: 0.14992\n",
      "Epoch: 01 [16564/20026 ( 83%)], Train Loss: 0.14995\n",
      "Epoch: 01 [16604/20026 ( 83%)], Train Loss: 0.14979\n",
      "Epoch: 01 [16644/20026 ( 83%)], Train Loss: 0.14950\n",
      "Epoch: 01 [16684/20026 ( 83%)], Train Loss: 0.14919\n",
      "Epoch: 01 [16724/20026 ( 84%)], Train Loss: 0.14891\n",
      "Epoch: 01 [16764/20026 ( 84%)], Train Loss: 0.14865\n",
      "Epoch: 01 [16804/20026 ( 84%)], Train Loss: 0.14884\n",
      "Epoch: 01 [16844/20026 ( 84%)], Train Loss: 0.14863\n",
      "Epoch: 01 [16884/20026 ( 84%)], Train Loss: 0.14845\n",
      "Epoch: 01 [16924/20026 ( 85%)], Train Loss: 0.14845\n",
      "Epoch: 01 [16964/20026 ( 85%)], Train Loss: 0.14818\n",
      "Epoch: 01 [17004/20026 ( 85%)], Train Loss: 0.14824\n",
      "Epoch: 01 [17044/20026 ( 85%)], Train Loss: 0.14813\n",
      "Epoch: 01 [17084/20026 ( 85%)], Train Loss: 0.14787\n",
      "Epoch: 01 [17124/20026 ( 86%)], Train Loss: 0.14782\n",
      "Epoch: 01 [17164/20026 ( 86%)], Train Loss: 0.14778\n",
      "Epoch: 01 [17204/20026 ( 86%)], Train Loss: 0.14772\n",
      "Epoch: 01 [17244/20026 ( 86%)], Train Loss: 0.14756\n",
      "Epoch: 01 [17284/20026 ( 86%)], Train Loss: 0.14738\n",
      "Epoch: 01 [17324/20026 ( 87%)], Train Loss: 0.14733\n",
      "Epoch: 01 [17364/20026 ( 87%)], Train Loss: 0.14730\n",
      "Epoch: 01 [17404/20026 ( 87%)], Train Loss: 0.14713\n",
      "Epoch: 01 [17444/20026 ( 87%)], Train Loss: 0.14719\n",
      "Epoch: 01 [17484/20026 ( 87%)], Train Loss: 0.14725\n",
      "Epoch: 01 [17524/20026 ( 88%)], Train Loss: 0.14721\n",
      "Epoch: 01 [17564/20026 ( 88%)], Train Loss: 0.14708\n",
      "Epoch: 01 [17604/20026 ( 88%)], Train Loss: 0.14707\n",
      "Epoch: 01 [17644/20026 ( 88%)], Train Loss: 0.14701\n",
      "Epoch: 01 [17684/20026 ( 88%)], Train Loss: 0.14684\n",
      "Epoch: 01 [17724/20026 ( 89%)], Train Loss: 0.14690\n",
      "Epoch: 01 [17764/20026 ( 89%)], Train Loss: 0.14683\n",
      "Epoch: 01 [17804/20026 ( 89%)], Train Loss: 0.14701\n",
      "Epoch: 01 [17844/20026 ( 89%)], Train Loss: 0.14687\n",
      "Epoch: 01 [17884/20026 ( 89%)], Train Loss: 0.14660\n",
      "Epoch: 01 [17924/20026 ( 90%)], Train Loss: 0.14645\n",
      "Epoch: 01 [17964/20026 ( 90%)], Train Loss: 0.14645\n",
      "Epoch: 01 [18004/20026 ( 90%)], Train Loss: 0.14625\n",
      "Epoch: 01 [18044/20026 ( 90%)], Train Loss: 0.14628\n",
      "Epoch: 01 [18084/20026 ( 90%)], Train Loss: 0.14644\n",
      "Epoch: 01 [18124/20026 ( 91%)], Train Loss: 0.14637\n",
      "Epoch: 01 [18164/20026 ( 91%)], Train Loss: 0.14610\n",
      "Epoch: 01 [18204/20026 ( 91%)], Train Loss: 0.14590\n",
      "Epoch: 01 [18244/20026 ( 91%)], Train Loss: 0.14590\n",
      "Epoch: 01 [18284/20026 ( 91%)], Train Loss: 0.14565\n",
      "Epoch: 01 [18324/20026 ( 92%)], Train Loss: 0.14575\n",
      "Epoch: 01 [18364/20026 ( 92%)], Train Loss: 0.14575\n",
      "Epoch: 01 [18404/20026 ( 92%)], Train Loss: 0.14590\n",
      "Epoch: 01 [18444/20026 ( 92%)], Train Loss: 0.14576\n",
      "Epoch: 01 [18484/20026 ( 92%)], Train Loss: 0.14575\n",
      "Epoch: 01 [18524/20026 ( 92%)], Train Loss: 0.14575\n",
      "Epoch: 01 [18564/20026 ( 93%)], Train Loss: 0.14573\n",
      "Epoch: 01 [18604/20026 ( 93%)], Train Loss: 0.14567\n",
      "Epoch: 01 [18644/20026 ( 93%)], Train Loss: 0.14552\n",
      "Epoch: 01 [18684/20026 ( 93%)], Train Loss: 0.14562\n",
      "Epoch: 01 [18724/20026 ( 93%)], Train Loss: 0.14558\n",
      "Epoch: 01 [18764/20026 ( 94%)], Train Loss: 0.14540\n",
      "Epoch: 01 [18804/20026 ( 94%)], Train Loss: 0.14530\n",
      "Epoch: 01 [18844/20026 ( 94%)], Train Loss: 0.14524\n",
      "Epoch: 01 [18884/20026 ( 94%)], Train Loss: 0.14508\n",
      "Epoch: 01 [18924/20026 ( 94%)], Train Loss: 0.14490\n",
      "Epoch: 01 [18964/20026 ( 95%)], Train Loss: 0.14474\n",
      "Epoch: 01 [19004/20026 ( 95%)], Train Loss: 0.14459\n",
      "Epoch: 01 [19044/20026 ( 95%)], Train Loss: 0.14461\n",
      "Epoch: 01 [19084/20026 ( 95%)], Train Loss: 0.14450\n",
      "Epoch: 01 [19124/20026 ( 95%)], Train Loss: 0.14452\n",
      "Epoch: 01 [19164/20026 ( 96%)], Train Loss: 0.14432\n",
      "Epoch: 01 [19204/20026 ( 96%)], Train Loss: 0.14440\n",
      "Epoch: 01 [19244/20026 ( 96%)], Train Loss: 0.14434\n",
      "Epoch: 01 [19284/20026 ( 96%)], Train Loss: 0.14434\n",
      "Epoch: 01 [19324/20026 ( 96%)], Train Loss: 0.14429\n",
      "Epoch: 01 [19364/20026 ( 97%)], Train Loss: 0.14416\n",
      "Epoch: 01 [19404/20026 ( 97%)], Train Loss: 0.14400\n",
      "Epoch: 01 [19444/20026 ( 97%)], Train Loss: 0.14392\n",
      "Epoch: 01 [19484/20026 ( 97%)], Train Loss: 0.14398\n",
      "Epoch: 01 [19524/20026 ( 97%)], Train Loss: 0.14399\n",
      "Epoch: 01 [19564/20026 ( 98%)], Train Loss: 0.14388\n",
      "Epoch: 01 [19604/20026 ( 98%)], Train Loss: 0.14379\n",
      "Epoch: 01 [19644/20026 ( 98%)], Train Loss: 0.14375\n",
      "Epoch: 01 [19684/20026 ( 98%)], Train Loss: 0.14399\n",
      "Epoch: 01 [19724/20026 ( 98%)], Train Loss: 0.14396\n",
      "Epoch: 01 [19764/20026 ( 99%)], Train Loss: 0.14390\n",
      "Epoch: 01 [19804/20026 ( 99%)], Train Loss: 0.14376\n",
      "Epoch: 01 [19844/20026 ( 99%)], Train Loss: 0.14373\n",
      "Epoch: 01 [19884/20026 ( 99%)], Train Loss: 0.14385\n",
      "Epoch: 01 [19924/20026 ( 99%)], Train Loss: 0.14372\n",
      "Epoch: 01 [19964/20026 (100%)], Train Loss: 0.14384\n",
      "Epoch: 01 [20004/20026 (100%)], Train Loss: 0.14376\n",
      "Epoch: 01 [20026/20026 (100%)], Train Loss: 0.14367\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.23439\n",
      "\n",
      "Total Training Time: 5660.853477239609secs, Average Training Time per Epoch: 2830.4267386198044secs.\n",
      "Total Validation Time: 256.575558423996secs, Average Validation Time per Epoch: 128.287779211998secs.\n"
     ]
    }
   ],
   "source": [
    "for fold in range(1):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a9368",
   "metadata": {
    "papermill": {
     "duration": 0.274324,
     "end_time": "2021-10-03T04:57:38.304062",
     "exception": false,
     "start_time": "2021-10-03T04:57:38.029738",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79ec9b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T04:57:38.864711Z",
     "iopub.status.busy": "2021-10-03T04:57:38.863847Z",
     "iopub.status.idle": "2021-10-03T06:37:43.498503Z",
     "shell.execute_reply": "2021-10-03T06:37:43.498973Z"
    },
    "id": "DkjRIhdbwjHx",
    "outputId": "c62d2d45-cd44-4ca9-a775-3814971090d7",
    "papermill": {
     "duration": 6004.921403,
     "end_time": "2021-10-03T06:37:43.499128",
     "exception": false,
     "start_time": "2021-10-03T04:57:38.577725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 1\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20014, Num examples Valid=3055\n",
      "Total Training Steps: 5004, Total Warmup Steps: 500\n",
      "Epoch: 00 [    4/20014 (  0%)], Train Loss: 3.09328\n",
      "Epoch: 00 [   44/20014 (  0%)], Train Loss: 3.07146\n",
      "Epoch: 00 [   84/20014 (  0%)], Train Loss: 3.07145\n",
      "Epoch: 00 [  124/20014 (  1%)], Train Loss: 3.05305\n",
      "Epoch: 00 [  164/20014 (  1%)], Train Loss: 3.02134\n",
      "Epoch: 00 [  204/20014 (  1%)], Train Loss: 2.99929\n",
      "Epoch: 00 [  244/20014 (  1%)], Train Loss: 2.96616\n",
      "Epoch: 00 [  284/20014 (  1%)], Train Loss: 2.91868\n",
      "Epoch: 00 [  324/20014 (  2%)], Train Loss: 2.86386\n",
      "Epoch: 00 [  364/20014 (  2%)], Train Loss: 2.80970\n",
      "Epoch: 00 [  404/20014 (  2%)], Train Loss: 2.74047\n",
      "Epoch: 00 [  444/20014 (  2%)], Train Loss: 2.66361\n",
      "Epoch: 00 [  484/20014 (  2%)], Train Loss: 2.58988\n",
      "Epoch: 00 [  524/20014 (  3%)], Train Loss: 2.51178\n",
      "Epoch: 00 [  564/20014 (  3%)], Train Loss: 2.42396\n",
      "Epoch: 00 [  604/20014 (  3%)], Train Loss: 2.32527\n",
      "Epoch: 00 [  644/20014 (  3%)], Train Loss: 2.23992\n",
      "Epoch: 00 [  684/20014 (  3%)], Train Loss: 2.16184\n",
      "Epoch: 00 [  724/20014 (  4%)], Train Loss: 2.09062\n",
      "Epoch: 00 [  764/20014 (  4%)], Train Loss: 2.02059\n",
      "Epoch: 00 [  804/20014 (  4%)], Train Loss: 1.95985\n",
      "Epoch: 00 [  844/20014 (  4%)], Train Loss: 1.89281\n",
      "Epoch: 00 [  884/20014 (  4%)], Train Loss: 1.83202\n",
      "Epoch: 00 [  924/20014 (  5%)], Train Loss: 1.78338\n",
      "Epoch: 00 [  964/20014 (  5%)], Train Loss: 1.72827\n",
      "Epoch: 00 [ 1004/20014 (  5%)], Train Loss: 1.68104\n",
      "Epoch: 00 [ 1044/20014 (  5%)], Train Loss: 1.63826\n",
      "Epoch: 00 [ 1084/20014 (  5%)], Train Loss: 1.60157\n",
      "Epoch: 00 [ 1124/20014 (  6%)], Train Loss: 1.55842\n",
      "Epoch: 00 [ 1164/20014 (  6%)], Train Loss: 1.51815\n",
      "Epoch: 00 [ 1204/20014 (  6%)], Train Loss: 1.48600\n",
      "Epoch: 00 [ 1244/20014 (  6%)], Train Loss: 1.45222\n",
      "Epoch: 00 [ 1284/20014 (  6%)], Train Loss: 1.42036\n",
      "Epoch: 00 [ 1324/20014 (  7%)], Train Loss: 1.38962\n",
      "Epoch: 00 [ 1364/20014 (  7%)], Train Loss: 1.36721\n",
      "Epoch: 00 [ 1404/20014 (  7%)], Train Loss: 1.34418\n",
      "Epoch: 00 [ 1444/20014 (  7%)], Train Loss: 1.31811\n",
      "Epoch: 00 [ 1484/20014 (  7%)], Train Loss: 1.29345\n",
      "Epoch: 00 [ 1524/20014 (  8%)], Train Loss: 1.26435\n",
      "Epoch: 00 [ 1564/20014 (  8%)], Train Loss: 1.24064\n",
      "Epoch: 00 [ 1604/20014 (  8%)], Train Loss: 1.22268\n",
      "Epoch: 00 [ 1644/20014 (  8%)], Train Loss: 1.19989\n",
      "Epoch: 00 [ 1684/20014 (  8%)], Train Loss: 1.17707\n",
      "Epoch: 00 [ 1724/20014 (  9%)], Train Loss: 1.16013\n",
      "Epoch: 00 [ 1764/20014 (  9%)], Train Loss: 1.14378\n",
      "Epoch: 00 [ 1804/20014 (  9%)], Train Loss: 1.12555\n",
      "Epoch: 00 [ 1844/20014 (  9%)], Train Loss: 1.11023\n",
      "Epoch: 00 [ 1884/20014 (  9%)], Train Loss: 1.09807\n",
      "Epoch: 00 [ 1924/20014 ( 10%)], Train Loss: 1.08884\n",
      "Epoch: 00 [ 1964/20014 ( 10%)], Train Loss: 1.07469\n",
      "Epoch: 00 [ 2004/20014 ( 10%)], Train Loss: 1.06117\n",
      "Epoch: 00 [ 2044/20014 ( 10%)], Train Loss: 1.04980\n",
      "Epoch: 00 [ 2084/20014 ( 10%)], Train Loss: 1.03665\n",
      "Epoch: 00 [ 2124/20014 ( 11%)], Train Loss: 1.02330\n",
      "Epoch: 00 [ 2164/20014 ( 11%)], Train Loss: 1.01478\n",
      "Epoch: 00 [ 2204/20014 ( 11%)], Train Loss: 1.00467\n",
      "Epoch: 00 [ 2244/20014 ( 11%)], Train Loss: 0.99380\n",
      "Epoch: 00 [ 2284/20014 ( 11%)], Train Loss: 0.98532\n",
      "Epoch: 00 [ 2324/20014 ( 12%)], Train Loss: 0.97626\n",
      "Epoch: 00 [ 2364/20014 ( 12%)], Train Loss: 0.96724\n",
      "Epoch: 00 [ 2404/20014 ( 12%)], Train Loss: 0.96005\n",
      "Epoch: 00 [ 2444/20014 ( 12%)], Train Loss: 0.95111\n",
      "Epoch: 00 [ 2484/20014 ( 12%)], Train Loss: 0.94292\n",
      "Epoch: 00 [ 2524/20014 ( 13%)], Train Loss: 0.93390\n",
      "Epoch: 00 [ 2564/20014 ( 13%)], Train Loss: 0.92261\n",
      "Epoch: 00 [ 2604/20014 ( 13%)], Train Loss: 0.91356\n",
      "Epoch: 00 [ 2644/20014 ( 13%)], Train Loss: 0.90482\n",
      "Epoch: 00 [ 2684/20014 ( 13%)], Train Loss: 0.89577\n",
      "Epoch: 00 [ 2724/20014 ( 14%)], Train Loss: 0.89187\n",
      "Epoch: 00 [ 2764/20014 ( 14%)], Train Loss: 0.88630\n",
      "Epoch: 00 [ 2804/20014 ( 14%)], Train Loss: 0.87994\n",
      "Epoch: 00 [ 2844/20014 ( 14%)], Train Loss: 0.87132\n",
      "Epoch: 00 [ 2884/20014 ( 14%)], Train Loss: 0.86830\n",
      "Epoch: 00 [ 2924/20014 ( 15%)], Train Loss: 0.86297\n",
      "Epoch: 00 [ 2964/20014 ( 15%)], Train Loss: 0.85924\n",
      "Epoch: 00 [ 3004/20014 ( 15%)], Train Loss: 0.85334\n",
      "Epoch: 00 [ 3044/20014 ( 15%)], Train Loss: 0.84760\n",
      "Epoch: 00 [ 3084/20014 ( 15%)], Train Loss: 0.84251\n",
      "Epoch: 00 [ 3124/20014 ( 16%)], Train Loss: 0.83433\n",
      "Epoch: 00 [ 3164/20014 ( 16%)], Train Loss: 0.82590\n",
      "Epoch: 00 [ 3204/20014 ( 16%)], Train Loss: 0.81933\n",
      "Epoch: 00 [ 3244/20014 ( 16%)], Train Loss: 0.81452\n",
      "Epoch: 00 [ 3284/20014 ( 16%)], Train Loss: 0.81056\n",
      "Epoch: 00 [ 3324/20014 ( 17%)], Train Loss: 0.80662\n",
      "Epoch: 00 [ 3364/20014 ( 17%)], Train Loss: 0.80278\n",
      "Epoch: 00 [ 3404/20014 ( 17%)], Train Loss: 0.79929\n",
      "Epoch: 00 [ 3444/20014 ( 17%)], Train Loss: 0.79694\n",
      "Epoch: 00 [ 3484/20014 ( 17%)], Train Loss: 0.79169\n",
      "Epoch: 00 [ 3524/20014 ( 18%)], Train Loss: 0.78552\n",
      "Epoch: 00 [ 3564/20014 ( 18%)], Train Loss: 0.77850\n",
      "Epoch: 00 [ 3604/20014 ( 18%)], Train Loss: 0.77719\n",
      "Epoch: 00 [ 3644/20014 ( 18%)], Train Loss: 0.77302\n",
      "Epoch: 00 [ 3684/20014 ( 18%)], Train Loss: 0.77151\n",
      "Epoch: 00 [ 3724/20014 ( 19%)], Train Loss: 0.76680\n",
      "Epoch: 00 [ 3764/20014 ( 19%)], Train Loss: 0.76226\n",
      "Epoch: 00 [ 3804/20014 ( 19%)], Train Loss: 0.75902\n",
      "Epoch: 00 [ 3844/20014 ( 19%)], Train Loss: 0.75567\n",
      "Epoch: 00 [ 3884/20014 ( 19%)], Train Loss: 0.75159\n",
      "Epoch: 00 [ 3924/20014 ( 20%)], Train Loss: 0.74651\n",
      "Epoch: 00 [ 3964/20014 ( 20%)], Train Loss: 0.74254\n",
      "Epoch: 00 [ 4004/20014 ( 20%)], Train Loss: 0.73823\n",
      "Epoch: 00 [ 4044/20014 ( 20%)], Train Loss: 0.73450\n",
      "Epoch: 00 [ 4084/20014 ( 20%)], Train Loss: 0.73561\n",
      "Epoch: 00 [ 4124/20014 ( 21%)], Train Loss: 0.73169\n",
      "Epoch: 00 [ 4164/20014 ( 21%)], Train Loss: 0.72968\n",
      "Epoch: 00 [ 4204/20014 ( 21%)], Train Loss: 0.72631\n",
      "Epoch: 00 [ 4244/20014 ( 21%)], Train Loss: 0.72406\n",
      "Epoch: 00 [ 4284/20014 ( 21%)], Train Loss: 0.72176\n",
      "Epoch: 00 [ 4324/20014 ( 22%)], Train Loss: 0.71842\n",
      "Epoch: 00 [ 4364/20014 ( 22%)], Train Loss: 0.71475\n",
      "Epoch: 00 [ 4404/20014 ( 22%)], Train Loss: 0.71089\n",
      "Epoch: 00 [ 4444/20014 ( 22%)], Train Loss: 0.70887\n",
      "Epoch: 00 [ 4484/20014 ( 22%)], Train Loss: 0.70928\n",
      "Epoch: 00 [ 4524/20014 ( 23%)], Train Loss: 0.70583\n",
      "Epoch: 00 [ 4564/20014 ( 23%)], Train Loss: 0.70293\n",
      "Epoch: 00 [ 4604/20014 ( 23%)], Train Loss: 0.70146\n",
      "Epoch: 00 [ 4644/20014 ( 23%)], Train Loss: 0.69836\n",
      "Epoch: 00 [ 4684/20014 ( 23%)], Train Loss: 0.69429\n",
      "Epoch: 00 [ 4724/20014 ( 24%)], Train Loss: 0.69190\n",
      "Epoch: 00 [ 4764/20014 ( 24%)], Train Loss: 0.68984\n",
      "Epoch: 00 [ 4804/20014 ( 24%)], Train Loss: 0.68828\n",
      "Epoch: 00 [ 4844/20014 ( 24%)], Train Loss: 0.68643\n",
      "Epoch: 00 [ 4884/20014 ( 24%)], Train Loss: 0.68261\n",
      "Epoch: 00 [ 4924/20014 ( 25%)], Train Loss: 0.67951\n",
      "Epoch: 00 [ 4964/20014 ( 25%)], Train Loss: 0.67773\n",
      "Epoch: 00 [ 5004/20014 ( 25%)], Train Loss: 0.67621\n",
      "Epoch: 00 [ 5044/20014 ( 25%)], Train Loss: 0.67411\n",
      "Epoch: 00 [ 5084/20014 ( 25%)], Train Loss: 0.67120\n",
      "Epoch: 00 [ 5124/20014 ( 26%)], Train Loss: 0.66910\n",
      "Epoch: 00 [ 5164/20014 ( 26%)], Train Loss: 0.66754\n",
      "Epoch: 00 [ 5204/20014 ( 26%)], Train Loss: 0.66507\n",
      "Epoch: 00 [ 5244/20014 ( 26%)], Train Loss: 0.66197\n",
      "Epoch: 00 [ 5284/20014 ( 26%)], Train Loss: 0.66042\n",
      "Epoch: 00 [ 5324/20014 ( 27%)], Train Loss: 0.65903\n",
      "Epoch: 00 [ 5364/20014 ( 27%)], Train Loss: 0.65915\n",
      "Epoch: 00 [ 5404/20014 ( 27%)], Train Loss: 0.65617\n",
      "Epoch: 00 [ 5444/20014 ( 27%)], Train Loss: 0.65402\n",
      "Epoch: 00 [ 5484/20014 ( 27%)], Train Loss: 0.65214\n",
      "Epoch: 00 [ 5524/20014 ( 28%)], Train Loss: 0.65021\n",
      "Epoch: 00 [ 5564/20014 ( 28%)], Train Loss: 0.64815\n",
      "Epoch: 00 [ 5604/20014 ( 28%)], Train Loss: 0.64675\n",
      "Epoch: 00 [ 5644/20014 ( 28%)], Train Loss: 0.64535\n",
      "Epoch: 00 [ 5684/20014 ( 28%)], Train Loss: 0.64350\n",
      "Epoch: 00 [ 5724/20014 ( 29%)], Train Loss: 0.64204\n",
      "Epoch: 00 [ 5764/20014 ( 29%)], Train Loss: 0.63886\n",
      "Epoch: 00 [ 5804/20014 ( 29%)], Train Loss: 0.63701\n",
      "Epoch: 00 [ 5844/20014 ( 29%)], Train Loss: 0.63663\n",
      "Epoch: 00 [ 5884/20014 ( 29%)], Train Loss: 0.63583\n",
      "Epoch: 00 [ 5924/20014 ( 30%)], Train Loss: 0.63573\n",
      "Epoch: 00 [ 5964/20014 ( 30%)], Train Loss: 0.63443\n",
      "Epoch: 00 [ 6004/20014 ( 30%)], Train Loss: 0.63279\n",
      "Epoch: 00 [ 6044/20014 ( 30%)], Train Loss: 0.63047\n",
      "Epoch: 00 [ 6084/20014 ( 30%)], Train Loss: 0.62776\n",
      "Epoch: 00 [ 6124/20014 ( 31%)], Train Loss: 0.62757\n",
      "Epoch: 00 [ 6164/20014 ( 31%)], Train Loss: 0.62603\n",
      "Epoch: 00 [ 6204/20014 ( 31%)], Train Loss: 0.62621\n",
      "Epoch: 00 [ 6244/20014 ( 31%)], Train Loss: 0.62477\n",
      "Epoch: 00 [ 6284/20014 ( 31%)], Train Loss: 0.62354\n",
      "Epoch: 00 [ 6324/20014 ( 32%)], Train Loss: 0.62217\n",
      "Epoch: 00 [ 6364/20014 ( 32%)], Train Loss: 0.62018\n",
      "Epoch: 00 [ 6404/20014 ( 32%)], Train Loss: 0.61894\n",
      "Epoch: 00 [ 6444/20014 ( 32%)], Train Loss: 0.61690\n",
      "Epoch: 00 [ 6484/20014 ( 32%)], Train Loss: 0.61536\n",
      "Epoch: 00 [ 6524/20014 ( 33%)], Train Loss: 0.61440\n",
      "Epoch: 00 [ 6564/20014 ( 33%)], Train Loss: 0.61407\n",
      "Epoch: 00 [ 6604/20014 ( 33%)], Train Loss: 0.61304\n",
      "Epoch: 00 [ 6644/20014 ( 33%)], Train Loss: 0.61317\n",
      "Epoch: 00 [ 6684/20014 ( 33%)], Train Loss: 0.61271\n",
      "Epoch: 00 [ 6724/20014 ( 34%)], Train Loss: 0.61100\n",
      "Epoch: 00 [ 6764/20014 ( 34%)], Train Loss: 0.60978\n",
      "Epoch: 00 [ 6804/20014 ( 34%)], Train Loss: 0.60888\n",
      "Epoch: 00 [ 6844/20014 ( 34%)], Train Loss: 0.60776\n",
      "Epoch: 00 [ 6884/20014 ( 34%)], Train Loss: 0.60687\n",
      "Epoch: 00 [ 6924/20014 ( 35%)], Train Loss: 0.60587\n",
      "Epoch: 00 [ 6964/20014 ( 35%)], Train Loss: 0.60431\n",
      "Epoch: 00 [ 7004/20014 ( 35%)], Train Loss: 0.60387\n",
      "Epoch: 00 [ 7044/20014 ( 35%)], Train Loss: 0.60266\n",
      "Epoch: 00 [ 7084/20014 ( 35%)], Train Loss: 0.60191\n",
      "Epoch: 00 [ 7124/20014 ( 36%)], Train Loss: 0.60157\n",
      "Epoch: 00 [ 7164/20014 ( 36%)], Train Loss: 0.59919\n",
      "Epoch: 00 [ 7204/20014 ( 36%)], Train Loss: 0.59858\n",
      "Epoch: 00 [ 7244/20014 ( 36%)], Train Loss: 0.59767\n",
      "Epoch: 00 [ 7284/20014 ( 36%)], Train Loss: 0.59657\n",
      "Epoch: 00 [ 7324/20014 ( 37%)], Train Loss: 0.59444\n",
      "Epoch: 00 [ 7364/20014 ( 37%)], Train Loss: 0.59364\n",
      "Epoch: 00 [ 7404/20014 ( 37%)], Train Loss: 0.59347\n",
      "Epoch: 00 [ 7444/20014 ( 37%)], Train Loss: 0.59235\n",
      "Epoch: 00 [ 7484/20014 ( 37%)], Train Loss: 0.59055\n",
      "Epoch: 00 [ 7524/20014 ( 38%)], Train Loss: 0.58965\n",
      "Epoch: 00 [ 7564/20014 ( 38%)], Train Loss: 0.58867\n",
      "Epoch: 00 [ 7604/20014 ( 38%)], Train Loss: 0.58651\n",
      "Epoch: 00 [ 7644/20014 ( 38%)], Train Loss: 0.58522\n",
      "Epoch: 00 [ 7684/20014 ( 38%)], Train Loss: 0.58403\n",
      "Epoch: 00 [ 7724/20014 ( 39%)], Train Loss: 0.58257\n",
      "Epoch: 00 [ 7764/20014 ( 39%)], Train Loss: 0.58172\n",
      "Epoch: 00 [ 7804/20014 ( 39%)], Train Loss: 0.58084\n",
      "Epoch: 00 [ 7844/20014 ( 39%)], Train Loss: 0.57941\n",
      "Epoch: 00 [ 7884/20014 ( 39%)], Train Loss: 0.57977\n",
      "Epoch: 00 [ 7924/20014 ( 40%)], Train Loss: 0.57842\n",
      "Epoch: 00 [ 7964/20014 ( 40%)], Train Loss: 0.57677\n",
      "Epoch: 00 [ 8004/20014 ( 40%)], Train Loss: 0.57571\n",
      "Epoch: 00 [ 8044/20014 ( 40%)], Train Loss: 0.57474\n",
      "Epoch: 00 [ 8084/20014 ( 40%)], Train Loss: 0.57435\n",
      "Epoch: 00 [ 8124/20014 ( 41%)], Train Loss: 0.57307\n",
      "Epoch: 00 [ 8164/20014 ( 41%)], Train Loss: 0.57192\n",
      "Epoch: 00 [ 8204/20014 ( 41%)], Train Loss: 0.57133\n",
      "Epoch: 00 [ 8244/20014 ( 41%)], Train Loss: 0.56969\n",
      "Epoch: 00 [ 8284/20014 ( 41%)], Train Loss: 0.56925\n",
      "Epoch: 00 [ 8324/20014 ( 42%)], Train Loss: 0.56742\n",
      "Epoch: 00 [ 8364/20014 ( 42%)], Train Loss: 0.56690\n",
      "Epoch: 00 [ 8404/20014 ( 42%)], Train Loss: 0.56694\n",
      "Epoch: 00 [ 8444/20014 ( 42%)], Train Loss: 0.56607\n",
      "Epoch: 00 [ 8484/20014 ( 42%)], Train Loss: 0.56581\n",
      "Epoch: 00 [ 8524/20014 ( 43%)], Train Loss: 0.56430\n",
      "Epoch: 00 [ 8564/20014 ( 43%)], Train Loss: 0.56340\n",
      "Epoch: 00 [ 8604/20014 ( 43%)], Train Loss: 0.56332\n",
      "Epoch: 00 [ 8644/20014 ( 43%)], Train Loss: 0.56338\n",
      "Epoch: 00 [ 8684/20014 ( 43%)], Train Loss: 0.56266\n",
      "Epoch: 00 [ 8724/20014 ( 44%)], Train Loss: 0.56268\n",
      "Epoch: 00 [ 8764/20014 ( 44%)], Train Loss: 0.56207\n",
      "Epoch: 00 [ 8804/20014 ( 44%)], Train Loss: 0.56090\n",
      "Epoch: 00 [ 8844/20014 ( 44%)], Train Loss: 0.56097\n",
      "Epoch: 00 [ 8884/20014 ( 44%)], Train Loss: 0.55974\n",
      "Epoch: 00 [ 8924/20014 ( 45%)], Train Loss: 0.55882\n",
      "Epoch: 00 [ 8964/20014 ( 45%)], Train Loss: 0.55796\n",
      "Epoch: 00 [ 9004/20014 ( 45%)], Train Loss: 0.55689\n",
      "Epoch: 00 [ 9044/20014 ( 45%)], Train Loss: 0.55644\n",
      "Epoch: 00 [ 9084/20014 ( 45%)], Train Loss: 0.55634\n",
      "Epoch: 00 [ 9124/20014 ( 46%)], Train Loss: 0.55595\n",
      "Epoch: 00 [ 9164/20014 ( 46%)], Train Loss: 0.55505\n",
      "Epoch: 00 [ 9204/20014 ( 46%)], Train Loss: 0.55407\n",
      "Epoch: 00 [ 9244/20014 ( 46%)], Train Loss: 0.55281\n",
      "Epoch: 00 [ 9284/20014 ( 46%)], Train Loss: 0.55199\n",
      "Epoch: 00 [ 9324/20014 ( 47%)], Train Loss: 0.55085\n",
      "Epoch: 00 [ 9364/20014 ( 47%)], Train Loss: 0.55088\n",
      "Epoch: 00 [ 9404/20014 ( 47%)], Train Loss: 0.55036\n",
      "Epoch: 00 [ 9444/20014 ( 47%)], Train Loss: 0.54911\n",
      "Epoch: 00 [ 9484/20014 ( 47%)], Train Loss: 0.54792\n",
      "Epoch: 00 [ 9524/20014 ( 48%)], Train Loss: 0.54799\n",
      "Epoch: 00 [ 9564/20014 ( 48%)], Train Loss: 0.54721\n",
      "Epoch: 00 [ 9604/20014 ( 48%)], Train Loss: 0.54683\n",
      "Epoch: 00 [ 9644/20014 ( 48%)], Train Loss: 0.54643\n",
      "Epoch: 00 [ 9684/20014 ( 48%)], Train Loss: 0.54572\n",
      "Epoch: 00 [ 9724/20014 ( 49%)], Train Loss: 0.54512\n",
      "Epoch: 00 [ 9764/20014 ( 49%)], Train Loss: 0.54385\n",
      "Epoch: 00 [ 9804/20014 ( 49%)], Train Loss: 0.54387\n",
      "Epoch: 00 [ 9844/20014 ( 49%)], Train Loss: 0.54292\n",
      "Epoch: 00 [ 9884/20014 ( 49%)], Train Loss: 0.54159\n",
      "Epoch: 00 [ 9924/20014 ( 50%)], Train Loss: 0.54070\n",
      "Epoch: 00 [ 9964/20014 ( 50%)], Train Loss: 0.53940\n",
      "Epoch: 00 [10004/20014 ( 50%)], Train Loss: 0.53827\n",
      "Epoch: 00 [10044/20014 ( 50%)], Train Loss: 0.53780\n",
      "Epoch: 00 [10084/20014 ( 50%)], Train Loss: 0.53717\n",
      "Epoch: 00 [10124/20014 ( 51%)], Train Loss: 0.53647\n",
      "Epoch: 00 [10164/20014 ( 51%)], Train Loss: 0.53589\n",
      "Epoch: 00 [10204/20014 ( 51%)], Train Loss: 0.53546\n",
      "Epoch: 00 [10244/20014 ( 51%)], Train Loss: 0.53474\n",
      "Epoch: 00 [10284/20014 ( 51%)], Train Loss: 0.53487\n",
      "Epoch: 00 [10324/20014 ( 52%)], Train Loss: 0.53404\n",
      "Epoch: 00 [10364/20014 ( 52%)], Train Loss: 0.53326\n",
      "Epoch: 00 [10404/20014 ( 52%)], Train Loss: 0.53305\n",
      "Epoch: 00 [10444/20014 ( 52%)], Train Loss: 0.53271\n",
      "Epoch: 00 [10484/20014 ( 52%)], Train Loss: 0.53211\n",
      "Epoch: 00 [10524/20014 ( 53%)], Train Loss: 0.53192\n",
      "Epoch: 00 [10564/20014 ( 53%)], Train Loss: 0.53120\n",
      "Epoch: 00 [10604/20014 ( 53%)], Train Loss: 0.53016\n",
      "Epoch: 00 [10644/20014 ( 53%)], Train Loss: 0.52906\n",
      "Epoch: 00 [10684/20014 ( 53%)], Train Loss: 0.52796\n",
      "Epoch: 00 [10724/20014 ( 54%)], Train Loss: 0.52698\n",
      "Epoch: 00 [10764/20014 ( 54%)], Train Loss: 0.52636\n",
      "Epoch: 00 [10804/20014 ( 54%)], Train Loss: 0.52594\n",
      "Epoch: 00 [10844/20014 ( 54%)], Train Loss: 0.52520\n",
      "Epoch: 00 [10884/20014 ( 54%)], Train Loss: 0.52382\n",
      "Epoch: 00 [10924/20014 ( 55%)], Train Loss: 0.52287\n",
      "Epoch: 00 [10964/20014 ( 55%)], Train Loss: 0.52209\n",
      "Epoch: 00 [11004/20014 ( 55%)], Train Loss: 0.52189\n",
      "Epoch: 00 [11044/20014 ( 55%)], Train Loss: 0.52108\n",
      "Epoch: 00 [11084/20014 ( 55%)], Train Loss: 0.52126\n",
      "Epoch: 00 [11124/20014 ( 56%)], Train Loss: 0.52000\n",
      "Epoch: 00 [11164/20014 ( 56%)], Train Loss: 0.51989\n",
      "Epoch: 00 [11204/20014 ( 56%)], Train Loss: 0.51916\n",
      "Epoch: 00 [11244/20014 ( 56%)], Train Loss: 0.51824\n",
      "Epoch: 00 [11284/20014 ( 56%)], Train Loss: 0.51774\n",
      "Epoch: 00 [11324/20014 ( 57%)], Train Loss: 0.51829\n",
      "Epoch: 00 [11364/20014 ( 57%)], Train Loss: 0.51701\n",
      "Epoch: 00 [11404/20014 ( 57%)], Train Loss: 0.51609\n",
      "Epoch: 00 [11444/20014 ( 57%)], Train Loss: 0.51554\n",
      "Epoch: 00 [11484/20014 ( 57%)], Train Loss: 0.51501\n",
      "Epoch: 00 [11524/20014 ( 58%)], Train Loss: 0.51462\n",
      "Epoch: 00 [11564/20014 ( 58%)], Train Loss: 0.51337\n",
      "Epoch: 00 [11604/20014 ( 58%)], Train Loss: 0.51250\n",
      "Epoch: 00 [11644/20014 ( 58%)], Train Loss: 0.51227\n",
      "Epoch: 00 [11684/20014 ( 58%)], Train Loss: 0.51143\n",
      "Epoch: 00 [11724/20014 ( 59%)], Train Loss: 0.51145\n",
      "Epoch: 00 [11764/20014 ( 59%)], Train Loss: 0.51103\n",
      "Epoch: 00 [11804/20014 ( 59%)], Train Loss: 0.51014\n",
      "Epoch: 00 [11844/20014 ( 59%)], Train Loss: 0.50941\n",
      "Epoch: 00 [11884/20014 ( 59%)], Train Loss: 0.50892\n",
      "Epoch: 00 [11924/20014 ( 60%)], Train Loss: 0.50889\n",
      "Epoch: 00 [11964/20014 ( 60%)], Train Loss: 0.50836\n",
      "Epoch: 00 [12004/20014 ( 60%)], Train Loss: 0.50761\n",
      "Epoch: 00 [12044/20014 ( 60%)], Train Loss: 0.50709\n",
      "Epoch: 00 [12084/20014 ( 60%)], Train Loss: 0.50633\n",
      "Epoch: 00 [12124/20014 ( 61%)], Train Loss: 0.50692\n",
      "Epoch: 00 [12164/20014 ( 61%)], Train Loss: 0.50670\n",
      "Epoch: 00 [12204/20014 ( 61%)], Train Loss: 0.50582\n",
      "Epoch: 00 [12244/20014 ( 61%)], Train Loss: 0.50535\n",
      "Epoch: 00 [12284/20014 ( 61%)], Train Loss: 0.50549\n",
      "Epoch: 00 [12324/20014 ( 62%)], Train Loss: 0.50542\n",
      "Epoch: 00 [12364/20014 ( 62%)], Train Loss: 0.50502\n",
      "Epoch: 00 [12404/20014 ( 62%)], Train Loss: 0.50465\n",
      "Epoch: 00 [12444/20014 ( 62%)], Train Loss: 0.50463\n",
      "Epoch: 00 [12484/20014 ( 62%)], Train Loss: 0.50407\n",
      "Epoch: 00 [12524/20014 ( 63%)], Train Loss: 0.50372\n",
      "Epoch: 00 [12564/20014 ( 63%)], Train Loss: 0.50347\n",
      "Epoch: 00 [12604/20014 ( 63%)], Train Loss: 0.50319\n",
      "Epoch: 00 [12644/20014 ( 63%)], Train Loss: 0.50233\n",
      "Epoch: 00 [12684/20014 ( 63%)], Train Loss: 0.50268\n",
      "Epoch: 00 [12724/20014 ( 64%)], Train Loss: 0.50186\n",
      "Epoch: 00 [12764/20014 ( 64%)], Train Loss: 0.50122\n",
      "Epoch: 00 [12804/20014 ( 64%)], Train Loss: 0.50103\n",
      "Epoch: 00 [12844/20014 ( 64%)], Train Loss: 0.50101\n",
      "Epoch: 00 [12884/20014 ( 64%)], Train Loss: 0.50045\n",
      "Epoch: 00 [12924/20014 ( 65%)], Train Loss: 0.49986\n",
      "Epoch: 00 [12964/20014 ( 65%)], Train Loss: 0.50002\n",
      "Epoch: 00 [13004/20014 ( 65%)], Train Loss: 0.49993\n",
      "Epoch: 00 [13044/20014 ( 65%)], Train Loss: 0.49982\n",
      "Epoch: 00 [13084/20014 ( 65%)], Train Loss: 0.49925\n",
      "Epoch: 00 [13124/20014 ( 66%)], Train Loss: 0.49846\n",
      "Epoch: 00 [13164/20014 ( 66%)], Train Loss: 0.49852\n",
      "Epoch: 00 [13204/20014 ( 66%)], Train Loss: 0.49780\n",
      "Epoch: 00 [13244/20014 ( 66%)], Train Loss: 0.49737\n",
      "Epoch: 00 [13284/20014 ( 66%)], Train Loss: 0.49694\n",
      "Epoch: 00 [13324/20014 ( 67%)], Train Loss: 0.49641\n",
      "Epoch: 00 [13364/20014 ( 67%)], Train Loss: 0.49600\n",
      "Epoch: 00 [13404/20014 ( 67%)], Train Loss: 0.49613\n",
      "Epoch: 00 [13444/20014 ( 67%)], Train Loss: 0.49565\n",
      "Epoch: 00 [13484/20014 ( 67%)], Train Loss: 0.49517\n",
      "Epoch: 00 [13524/20014 ( 68%)], Train Loss: 0.49444\n",
      "Epoch: 00 [13564/20014 ( 68%)], Train Loss: 0.49420\n",
      "Epoch: 00 [13604/20014 ( 68%)], Train Loss: 0.49357\n",
      "Epoch: 00 [13644/20014 ( 68%)], Train Loss: 0.49278\n",
      "Epoch: 00 [13684/20014 ( 68%)], Train Loss: 0.49174\n",
      "Epoch: 00 [13724/20014 ( 69%)], Train Loss: 0.49114\n",
      "Epoch: 00 [13764/20014 ( 69%)], Train Loss: 0.49137\n",
      "Epoch: 00 [13804/20014 ( 69%)], Train Loss: 0.49100\n",
      "Epoch: 00 [13844/20014 ( 69%)], Train Loss: 0.49098\n",
      "Epoch: 00 [13884/20014 ( 69%)], Train Loss: 0.49054\n",
      "Epoch: 00 [13924/20014 ( 70%)], Train Loss: 0.48977\n",
      "Epoch: 00 [13964/20014 ( 70%)], Train Loss: 0.48930\n",
      "Epoch: 00 [14004/20014 ( 70%)], Train Loss: 0.48903\n",
      "Epoch: 00 [14044/20014 ( 70%)], Train Loss: 0.48858\n",
      "Epoch: 00 [14084/20014 ( 70%)], Train Loss: 0.48841\n",
      "Epoch: 00 [14124/20014 ( 71%)], Train Loss: 0.48756\n",
      "Epoch: 00 [14164/20014 ( 71%)], Train Loss: 0.48691\n",
      "Epoch: 00 [14204/20014 ( 71%)], Train Loss: 0.48640\n",
      "Epoch: 00 [14244/20014 ( 71%)], Train Loss: 0.48633\n",
      "Epoch: 00 [14284/20014 ( 71%)], Train Loss: 0.48544\n",
      "Epoch: 00 [14324/20014 ( 72%)], Train Loss: 0.48484\n",
      "Epoch: 00 [14364/20014 ( 72%)], Train Loss: 0.48525\n",
      "Epoch: 00 [14404/20014 ( 72%)], Train Loss: 0.48469\n",
      "Epoch: 00 [14444/20014 ( 72%)], Train Loss: 0.48437\n",
      "Epoch: 00 [14484/20014 ( 72%)], Train Loss: 0.48393\n",
      "Epoch: 00 [14524/20014 ( 73%)], Train Loss: 0.48376\n",
      "Epoch: 00 [14564/20014 ( 73%)], Train Loss: 0.48310\n",
      "Epoch: 00 [14604/20014 ( 73%)], Train Loss: 0.48285\n",
      "Epoch: 00 [14644/20014 ( 73%)], Train Loss: 0.48206\n",
      "Epoch: 00 [14684/20014 ( 73%)], Train Loss: 0.48172\n",
      "Epoch: 00 [14724/20014 ( 74%)], Train Loss: 0.48133\n",
      "Epoch: 00 [14764/20014 ( 74%)], Train Loss: 0.48139\n",
      "Epoch: 00 [14804/20014 ( 74%)], Train Loss: 0.48080\n",
      "Epoch: 00 [14844/20014 ( 74%)], Train Loss: 0.48039\n",
      "Epoch: 00 [14884/20014 ( 74%)], Train Loss: 0.48031\n",
      "Epoch: 00 [14924/20014 ( 75%)], Train Loss: 0.47984\n",
      "Epoch: 00 [14964/20014 ( 75%)], Train Loss: 0.48037\n",
      "Epoch: 00 [15004/20014 ( 75%)], Train Loss: 0.47985\n",
      "Epoch: 00 [15044/20014 ( 75%)], Train Loss: 0.47937\n",
      "Epoch: 00 [15084/20014 ( 75%)], Train Loss: 0.47930\n",
      "Epoch: 00 [15124/20014 ( 76%)], Train Loss: 0.47872\n",
      "Epoch: 00 [15164/20014 ( 76%)], Train Loss: 0.47861\n",
      "Epoch: 00 [15204/20014 ( 76%)], Train Loss: 0.47852\n",
      "Epoch: 00 [15244/20014 ( 76%)], Train Loss: 0.47916\n",
      "Epoch: 00 [15284/20014 ( 76%)], Train Loss: 0.47876\n",
      "Epoch: 00 [15324/20014 ( 77%)], Train Loss: 0.47872\n",
      "Epoch: 00 [15364/20014 ( 77%)], Train Loss: 0.47837\n",
      "Epoch: 00 [15404/20014 ( 77%)], Train Loss: 0.47816\n",
      "Epoch: 00 [15444/20014 ( 77%)], Train Loss: 0.47832\n",
      "Epoch: 00 [15484/20014 ( 77%)], Train Loss: 0.47791\n",
      "Epoch: 00 [15524/20014 ( 78%)], Train Loss: 0.47734\n",
      "Epoch: 00 [15564/20014 ( 78%)], Train Loss: 0.47708\n",
      "Epoch: 00 [15604/20014 ( 78%)], Train Loss: 0.47654\n",
      "Epoch: 00 [15644/20014 ( 78%)], Train Loss: 0.47600\n",
      "Epoch: 00 [15684/20014 ( 78%)], Train Loss: 0.47540\n",
      "Epoch: 00 [15724/20014 ( 79%)], Train Loss: 0.47502\n",
      "Epoch: 00 [15764/20014 ( 79%)], Train Loss: 0.47441\n",
      "Epoch: 00 [15804/20014 ( 79%)], Train Loss: 0.47390\n",
      "Epoch: 00 [15844/20014 ( 79%)], Train Loss: 0.47359\n",
      "Epoch: 00 [15884/20014 ( 79%)], Train Loss: 0.47371\n",
      "Epoch: 00 [15924/20014 ( 80%)], Train Loss: 0.47345\n",
      "Epoch: 00 [15964/20014 ( 80%)], Train Loss: 0.47302\n",
      "Epoch: 00 [16004/20014 ( 80%)], Train Loss: 0.47254\n",
      "Epoch: 00 [16044/20014 ( 80%)], Train Loss: 0.47199\n",
      "Epoch: 00 [16084/20014 ( 80%)], Train Loss: 0.47215\n",
      "Epoch: 00 [16124/20014 ( 81%)], Train Loss: 0.47176\n",
      "Epoch: 00 [16164/20014 ( 81%)], Train Loss: 0.47128\n",
      "Epoch: 00 [16204/20014 ( 81%)], Train Loss: 0.47101\n",
      "Epoch: 00 [16244/20014 ( 81%)], Train Loss: 0.47043\n",
      "Epoch: 00 [16284/20014 ( 81%)], Train Loss: 0.47033\n",
      "Epoch: 00 [16324/20014 ( 82%)], Train Loss: 0.47013\n",
      "Epoch: 00 [16364/20014 ( 82%)], Train Loss: 0.46988\n",
      "Epoch: 00 [16404/20014 ( 82%)], Train Loss: 0.46957\n",
      "Epoch: 00 [16444/20014 ( 82%)], Train Loss: 0.46892\n",
      "Epoch: 00 [16484/20014 ( 82%)], Train Loss: 0.46921\n",
      "Epoch: 00 [16524/20014 ( 83%)], Train Loss: 0.46870\n",
      "Epoch: 00 [16564/20014 ( 83%)], Train Loss: 0.46867\n",
      "Epoch: 00 [16604/20014 ( 83%)], Train Loss: 0.46829\n",
      "Epoch: 00 [16644/20014 ( 83%)], Train Loss: 0.46777\n",
      "Epoch: 00 [16684/20014 ( 83%)], Train Loss: 0.46730\n",
      "Epoch: 00 [16724/20014 ( 84%)], Train Loss: 0.46686\n",
      "Epoch: 00 [16764/20014 ( 84%)], Train Loss: 0.46684\n",
      "Epoch: 00 [16804/20014 ( 84%)], Train Loss: 0.46651\n",
      "Epoch: 00 [16844/20014 ( 84%)], Train Loss: 0.46580\n",
      "Epoch: 00 [16884/20014 ( 84%)], Train Loss: 0.46554\n",
      "Epoch: 00 [16924/20014 ( 85%)], Train Loss: 0.46550\n",
      "Epoch: 00 [16964/20014 ( 85%)], Train Loss: 0.46505\n",
      "Epoch: 00 [17004/20014 ( 85%)], Train Loss: 0.46483\n",
      "Epoch: 00 [17044/20014 ( 85%)], Train Loss: 0.46464\n",
      "Epoch: 00 [17084/20014 ( 85%)], Train Loss: 0.46375\n",
      "Epoch: 00 [17124/20014 ( 86%)], Train Loss: 0.46374\n",
      "Epoch: 00 [17164/20014 ( 86%)], Train Loss: 0.46348\n",
      "Epoch: 00 [17204/20014 ( 86%)], Train Loss: 0.46315\n",
      "Epoch: 00 [17244/20014 ( 86%)], Train Loss: 0.46286\n",
      "Epoch: 00 [17284/20014 ( 86%)], Train Loss: 0.46259\n",
      "Epoch: 00 [17324/20014 ( 87%)], Train Loss: 0.46206\n",
      "Epoch: 00 [17364/20014 ( 87%)], Train Loss: 0.46159\n",
      "Epoch: 00 [17404/20014 ( 87%)], Train Loss: 0.46123\n",
      "Epoch: 00 [17444/20014 ( 87%)], Train Loss: 0.46083\n",
      "Epoch: 00 [17484/20014 ( 87%)], Train Loss: 0.46036\n",
      "Epoch: 00 [17524/20014 ( 88%)], Train Loss: 0.46055\n",
      "Epoch: 00 [17564/20014 ( 88%)], Train Loss: 0.46051\n",
      "Epoch: 00 [17604/20014 ( 88%)], Train Loss: 0.46042\n",
      "Epoch: 00 [17644/20014 ( 88%)], Train Loss: 0.46033\n",
      "Epoch: 00 [17684/20014 ( 88%)], Train Loss: 0.46002\n",
      "Epoch: 00 [17724/20014 ( 89%)], Train Loss: 0.45973\n",
      "Epoch: 00 [17764/20014 ( 89%)], Train Loss: 0.45928\n",
      "Epoch: 00 [17804/20014 ( 89%)], Train Loss: 0.45941\n",
      "Epoch: 00 [17844/20014 ( 89%)], Train Loss: 0.45893\n",
      "Epoch: 00 [17884/20014 ( 89%)], Train Loss: 0.45890\n",
      "Epoch: 00 [17924/20014 ( 90%)], Train Loss: 0.45830\n",
      "Epoch: 00 [17964/20014 ( 90%)], Train Loss: 0.45816\n",
      "Epoch: 00 [18004/20014 ( 90%)], Train Loss: 0.45788\n",
      "Epoch: 00 [18044/20014 ( 90%)], Train Loss: 0.45795\n",
      "Epoch: 00 [18084/20014 ( 90%)], Train Loss: 0.45764\n",
      "Epoch: 00 [18124/20014 ( 91%)], Train Loss: 0.45723\n",
      "Epoch: 00 [18164/20014 ( 91%)], Train Loss: 0.45721\n",
      "Epoch: 00 [18204/20014 ( 91%)], Train Loss: 0.45724\n",
      "Epoch: 00 [18244/20014 ( 91%)], Train Loss: 0.45742\n",
      "Epoch: 00 [18284/20014 ( 91%)], Train Loss: 0.45754\n",
      "Epoch: 00 [18324/20014 ( 92%)], Train Loss: 0.45699\n",
      "Epoch: 00 [18364/20014 ( 92%)], Train Loss: 0.45708\n",
      "Epoch: 00 [18404/20014 ( 92%)], Train Loss: 0.45656\n",
      "Epoch: 00 [18444/20014 ( 92%)], Train Loss: 0.45627\n",
      "Epoch: 00 [18484/20014 ( 92%)], Train Loss: 0.45604\n",
      "Epoch: 00 [18524/20014 ( 93%)], Train Loss: 0.45557\n",
      "Epoch: 00 [18564/20014 ( 93%)], Train Loss: 0.45516\n",
      "Epoch: 00 [18604/20014 ( 93%)], Train Loss: 0.45484\n",
      "Epoch: 00 [18644/20014 ( 93%)], Train Loss: 0.45465\n",
      "Epoch: 00 [18684/20014 ( 93%)], Train Loss: 0.45439\n",
      "Epoch: 00 [18724/20014 ( 94%)], Train Loss: 0.45452\n",
      "Epoch: 00 [18764/20014 ( 94%)], Train Loss: 0.45423\n",
      "Epoch: 00 [18804/20014 ( 94%)], Train Loss: 0.45375\n",
      "Epoch: 00 [18844/20014 ( 94%)], Train Loss: 0.45380\n",
      "Epoch: 00 [18884/20014 ( 94%)], Train Loss: 0.45358\n",
      "Epoch: 00 [18924/20014 ( 95%)], Train Loss: 0.45316\n",
      "Epoch: 00 [18964/20014 ( 95%)], Train Loss: 0.45290\n",
      "Epoch: 00 [19004/20014 ( 95%)], Train Loss: 0.45276\n",
      "Epoch: 00 [19044/20014 ( 95%)], Train Loss: 0.45275\n",
      "Epoch: 00 [19084/20014 ( 95%)], Train Loss: 0.45268\n",
      "Epoch: 00 [19124/20014 ( 96%)], Train Loss: 0.45251\n",
      "Epoch: 00 [19164/20014 ( 96%)], Train Loss: 0.45271\n",
      "Epoch: 00 [19204/20014 ( 96%)], Train Loss: 0.45257\n",
      "Epoch: 00 [19244/20014 ( 96%)], Train Loss: 0.45233\n",
      "Epoch: 00 [19284/20014 ( 96%)], Train Loss: 0.45210\n",
      "Epoch: 00 [19324/20014 ( 97%)], Train Loss: 0.45200\n",
      "Epoch: 00 [19364/20014 ( 97%)], Train Loss: 0.45205\n",
      "Epoch: 00 [19404/20014 ( 97%)], Train Loss: 0.45165\n",
      "Epoch: 00 [19444/20014 ( 97%)], Train Loss: 0.45122\n",
      "Epoch: 00 [19484/20014 ( 97%)], Train Loss: 0.45078\n",
      "Epoch: 00 [19524/20014 ( 98%)], Train Loss: 0.45085\n",
      "Epoch: 00 [19564/20014 ( 98%)], Train Loss: 0.45042\n",
      "Epoch: 00 [19604/20014 ( 98%)], Train Loss: 0.45002\n",
      "Epoch: 00 [19644/20014 ( 98%)], Train Loss: 0.44986\n",
      "Epoch: 00 [19684/20014 ( 98%)], Train Loss: 0.44981\n",
      "Epoch: 00 [19724/20014 ( 99%)], Train Loss: 0.44990\n",
      "Epoch: 00 [19764/20014 ( 99%)], Train Loss: 0.44971\n",
      "Epoch: 00 [19804/20014 ( 99%)], Train Loss: 0.44982\n",
      "Epoch: 00 [19844/20014 ( 99%)], Train Loss: 0.44947\n",
      "Epoch: 00 [19884/20014 ( 99%)], Train Loss: 0.44921\n",
      "Epoch: 00 [19924/20014 (100%)], Train Loss: 0.44861\n",
      "Epoch: 00 [19964/20014 (100%)], Train Loss: 0.44871\n",
      "Epoch: 00 [20004/20014 (100%)], Train Loss: 0.44809\n",
      "Epoch: 00 [20014/20014 (100%)], Train Loss: 0.44793\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.25374\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.25374\n",
      "Saving model checkpoint to output/checkpoint-fold-1.\n",
      "\n",
      "Epoch: 01 [    4/20014 (  0%)], Train Loss: 0.20749\n",
      "Epoch: 01 [   44/20014 (  0%)], Train Loss: 0.23301\n",
      "Epoch: 01 [   84/20014 (  0%)], Train Loss: 0.26124\n",
      "Epoch: 01 [  124/20014 (  1%)], Train Loss: 0.30409\n",
      "Epoch: 01 [  164/20014 (  1%)], Train Loss: 0.31727\n",
      "Epoch: 01 [  204/20014 (  1%)], Train Loss: 0.33296\n",
      "Epoch: 01 [  244/20014 (  1%)], Train Loss: 0.35148\n",
      "Epoch: 01 [  284/20014 (  1%)], Train Loss: 0.33791\n",
      "Epoch: 01 [  324/20014 (  2%)], Train Loss: 0.34424\n",
      "Epoch: 01 [  364/20014 (  2%)], Train Loss: 0.36055\n",
      "Epoch: 01 [  404/20014 (  2%)], Train Loss: 0.35211\n",
      "Epoch: 01 [  444/20014 (  2%)], Train Loss: 0.34663\n",
      "Epoch: 01 [  484/20014 (  2%)], Train Loss: 0.34589\n",
      "Epoch: 01 [  524/20014 (  3%)], Train Loss: 0.34864\n",
      "Epoch: 01 [  564/20014 (  3%)], Train Loss: 0.33900\n",
      "Epoch: 01 [  604/20014 (  3%)], Train Loss: 0.32850\n",
      "Epoch: 01 [  644/20014 (  3%)], Train Loss: 0.32164\n",
      "Epoch: 01 [  684/20014 (  3%)], Train Loss: 0.32764\n",
      "Epoch: 01 [  724/20014 (  4%)], Train Loss: 0.33126\n",
      "Epoch: 01 [  764/20014 (  4%)], Train Loss: 0.32626\n",
      "Epoch: 01 [  804/20014 (  4%)], Train Loss: 0.33171\n",
      "Epoch: 01 [  844/20014 (  4%)], Train Loss: 0.32878\n",
      "Epoch: 01 [  884/20014 (  4%)], Train Loss: 0.32585\n",
      "Epoch: 01 [  924/20014 (  5%)], Train Loss: 0.32438\n",
      "Epoch: 01 [  964/20014 (  5%)], Train Loss: 0.32047\n",
      "Epoch: 01 [ 1004/20014 (  5%)], Train Loss: 0.31541\n",
      "Epoch: 01 [ 1044/20014 (  5%)], Train Loss: 0.31563\n",
      "Epoch: 01 [ 1084/20014 (  5%)], Train Loss: 0.31535\n",
      "Epoch: 01 [ 1124/20014 (  6%)], Train Loss: 0.31226\n",
      "Epoch: 01 [ 1164/20014 (  6%)], Train Loss: 0.30831\n",
      "Epoch: 01 [ 1204/20014 (  6%)], Train Loss: 0.30805\n",
      "Epoch: 01 [ 1244/20014 (  6%)], Train Loss: 0.30214\n",
      "Epoch: 01 [ 1284/20014 (  6%)], Train Loss: 0.29881\n",
      "Epoch: 01 [ 1324/20014 (  7%)], Train Loss: 0.29557\n",
      "Epoch: 01 [ 1364/20014 (  7%)], Train Loss: 0.29657\n",
      "Epoch: 01 [ 1404/20014 (  7%)], Train Loss: 0.29699\n",
      "Epoch: 01 [ 1444/20014 (  7%)], Train Loss: 0.29498\n",
      "Epoch: 01 [ 1484/20014 (  7%)], Train Loss: 0.29138\n",
      "Epoch: 01 [ 1524/20014 (  8%)], Train Loss: 0.28749\n",
      "Epoch: 01 [ 1564/20014 (  8%)], Train Loss: 0.28435\n",
      "Epoch: 01 [ 1604/20014 (  8%)], Train Loss: 0.28133\n",
      "Epoch: 01 [ 1644/20014 (  8%)], Train Loss: 0.27856\n",
      "Epoch: 01 [ 1684/20014 (  8%)], Train Loss: 0.27473\n",
      "Epoch: 01 [ 1724/20014 (  9%)], Train Loss: 0.27447\n",
      "Epoch: 01 [ 1764/20014 (  9%)], Train Loss: 0.27236\n",
      "Epoch: 01 [ 1804/20014 (  9%)], Train Loss: 0.27011\n",
      "Epoch: 01 [ 1844/20014 (  9%)], Train Loss: 0.26817\n",
      "Epoch: 01 [ 1884/20014 (  9%)], Train Loss: 0.26727\n",
      "Epoch: 01 [ 1924/20014 ( 10%)], Train Loss: 0.26878\n",
      "Epoch: 01 [ 1964/20014 ( 10%)], Train Loss: 0.26749\n",
      "Epoch: 01 [ 2004/20014 ( 10%)], Train Loss: 0.26604\n",
      "Epoch: 01 [ 2044/20014 ( 10%)], Train Loss: 0.26564\n",
      "Epoch: 01 [ 2084/20014 ( 10%)], Train Loss: 0.26402\n",
      "Epoch: 01 [ 2124/20014 ( 11%)], Train Loss: 0.26240\n",
      "Epoch: 01 [ 2164/20014 ( 11%)], Train Loss: 0.26323\n",
      "Epoch: 01 [ 2204/20014 ( 11%)], Train Loss: 0.26193\n",
      "Epoch: 01 [ 2244/20014 ( 11%)], Train Loss: 0.26107\n",
      "Epoch: 01 [ 2284/20014 ( 11%)], Train Loss: 0.26113\n",
      "Epoch: 01 [ 2324/20014 ( 12%)], Train Loss: 0.25845\n",
      "Epoch: 01 [ 2364/20014 ( 12%)], Train Loss: 0.25714\n",
      "Epoch: 01 [ 2404/20014 ( 12%)], Train Loss: 0.25465\n",
      "Epoch: 01 [ 2444/20014 ( 12%)], Train Loss: 0.25243\n",
      "Epoch: 01 [ 2484/20014 ( 12%)], Train Loss: 0.25197\n",
      "Epoch: 01 [ 2524/20014 ( 13%)], Train Loss: 0.24885\n",
      "Epoch: 01 [ 2564/20014 ( 13%)], Train Loss: 0.24581\n",
      "Epoch: 01 [ 2604/20014 ( 13%)], Train Loss: 0.24529\n",
      "Epoch: 01 [ 2644/20014 ( 13%)], Train Loss: 0.24282\n",
      "Epoch: 01 [ 2684/20014 ( 13%)], Train Loss: 0.24123\n",
      "Epoch: 01 [ 2724/20014 ( 14%)], Train Loss: 0.24201\n",
      "Epoch: 01 [ 2764/20014 ( 14%)], Train Loss: 0.24016\n",
      "Epoch: 01 [ 2804/20014 ( 14%)], Train Loss: 0.23866\n",
      "Epoch: 01 [ 2844/20014 ( 14%)], Train Loss: 0.23745\n",
      "Epoch: 01 [ 2884/20014 ( 14%)], Train Loss: 0.23938\n",
      "Epoch: 01 [ 2924/20014 ( 15%)], Train Loss: 0.23876\n",
      "Epoch: 01 [ 2964/20014 ( 15%)], Train Loss: 0.23830\n",
      "Epoch: 01 [ 3004/20014 ( 15%)], Train Loss: 0.23658\n",
      "Epoch: 01 [ 3044/20014 ( 15%)], Train Loss: 0.23630\n",
      "Epoch: 01 [ 3084/20014 ( 15%)], Train Loss: 0.23503\n",
      "Epoch: 01 [ 3124/20014 ( 16%)], Train Loss: 0.23313\n",
      "Epoch: 01 [ 3164/20014 ( 16%)], Train Loss: 0.23102\n",
      "Epoch: 01 [ 3204/20014 ( 16%)], Train Loss: 0.22999\n",
      "Epoch: 01 [ 3244/20014 ( 16%)], Train Loss: 0.22844\n",
      "Epoch: 01 [ 3284/20014 ( 16%)], Train Loss: 0.22745\n",
      "Epoch: 01 [ 3324/20014 ( 17%)], Train Loss: 0.22805\n",
      "Epoch: 01 [ 3364/20014 ( 17%)], Train Loss: 0.22738\n",
      "Epoch: 01 [ 3404/20014 ( 17%)], Train Loss: 0.22816\n",
      "Epoch: 01 [ 3444/20014 ( 17%)], Train Loss: 0.22900\n",
      "Epoch: 01 [ 3484/20014 ( 17%)], Train Loss: 0.22780\n",
      "Epoch: 01 [ 3524/20014 ( 18%)], Train Loss: 0.22639\n",
      "Epoch: 01 [ 3564/20014 ( 18%)], Train Loss: 0.22441\n",
      "Epoch: 01 [ 3604/20014 ( 18%)], Train Loss: 0.22543\n",
      "Epoch: 01 [ 3644/20014 ( 18%)], Train Loss: 0.22478\n",
      "Epoch: 01 [ 3684/20014 ( 18%)], Train Loss: 0.22522\n",
      "Epoch: 01 [ 3724/20014 ( 19%)], Train Loss: 0.22363\n",
      "Epoch: 01 [ 3764/20014 ( 19%)], Train Loss: 0.22308\n",
      "Epoch: 01 [ 3804/20014 ( 19%)], Train Loss: 0.22243\n",
      "Epoch: 01 [ 3844/20014 ( 19%)], Train Loss: 0.22279\n",
      "Epoch: 01 [ 3884/20014 ( 19%)], Train Loss: 0.22158\n",
      "Epoch: 01 [ 3924/20014 ( 20%)], Train Loss: 0.22020\n",
      "Epoch: 01 [ 3964/20014 ( 20%)], Train Loss: 0.21890\n",
      "Epoch: 01 [ 4004/20014 ( 20%)], Train Loss: 0.21784\n",
      "Epoch: 01 [ 4044/20014 ( 20%)], Train Loss: 0.21625\n",
      "Epoch: 01 [ 4084/20014 ( 20%)], Train Loss: 0.21667\n",
      "Epoch: 01 [ 4124/20014 ( 21%)], Train Loss: 0.21576\n",
      "Epoch: 01 [ 4164/20014 ( 21%)], Train Loss: 0.21617\n",
      "Epoch: 01 [ 4204/20014 ( 21%)], Train Loss: 0.21530\n",
      "Epoch: 01 [ 4244/20014 ( 21%)], Train Loss: 0.21502\n",
      "Epoch: 01 [ 4284/20014 ( 21%)], Train Loss: 0.21473\n",
      "Epoch: 01 [ 4324/20014 ( 22%)], Train Loss: 0.21344\n",
      "Epoch: 01 [ 4364/20014 ( 22%)], Train Loss: 0.21276\n",
      "Epoch: 01 [ 4404/20014 ( 22%)], Train Loss: 0.21225\n",
      "Epoch: 01 [ 4444/20014 ( 22%)], Train Loss: 0.21223\n",
      "Epoch: 01 [ 4484/20014 ( 22%)], Train Loss: 0.21161\n",
      "Epoch: 01 [ 4524/20014 ( 23%)], Train Loss: 0.21039\n",
      "Epoch: 01 [ 4564/20014 ( 23%)], Train Loss: 0.20977\n",
      "Epoch: 01 [ 4604/20014 ( 23%)], Train Loss: 0.20980\n",
      "Epoch: 01 [ 4644/20014 ( 23%)], Train Loss: 0.20885\n",
      "Epoch: 01 [ 4684/20014 ( 23%)], Train Loss: 0.20766\n",
      "Epoch: 01 [ 4724/20014 ( 24%)], Train Loss: 0.20723\n",
      "Epoch: 01 [ 4764/20014 ( 24%)], Train Loss: 0.20676\n",
      "Epoch: 01 [ 4804/20014 ( 24%)], Train Loss: 0.20686\n",
      "Epoch: 01 [ 4844/20014 ( 24%)], Train Loss: 0.20649\n",
      "Epoch: 01 [ 4884/20014 ( 24%)], Train Loss: 0.20553\n",
      "Epoch: 01 [ 4924/20014 ( 25%)], Train Loss: 0.20453\n",
      "Epoch: 01 [ 4964/20014 ( 25%)], Train Loss: 0.20459\n",
      "Epoch: 01 [ 5004/20014 ( 25%)], Train Loss: 0.20437\n",
      "Epoch: 01 [ 5044/20014 ( 25%)], Train Loss: 0.20375\n",
      "Epoch: 01 [ 5084/20014 ( 25%)], Train Loss: 0.20275\n",
      "Epoch: 01 [ 5124/20014 ( 26%)], Train Loss: 0.20307\n",
      "Epoch: 01 [ 5164/20014 ( 26%)], Train Loss: 0.20261\n",
      "Epoch: 01 [ 5204/20014 ( 26%)], Train Loss: 0.20196\n",
      "Epoch: 01 [ 5244/20014 ( 26%)], Train Loss: 0.20129\n",
      "Epoch: 01 [ 5284/20014 ( 26%)], Train Loss: 0.20042\n",
      "Epoch: 01 [ 5324/20014 ( 27%)], Train Loss: 0.20026\n",
      "Epoch: 01 [ 5364/20014 ( 27%)], Train Loss: 0.20056\n",
      "Epoch: 01 [ 5404/20014 ( 27%)], Train Loss: 0.19991\n",
      "Epoch: 01 [ 5444/20014 ( 27%)], Train Loss: 0.19970\n",
      "Epoch: 01 [ 5484/20014 ( 27%)], Train Loss: 0.20017\n",
      "Epoch: 01 [ 5524/20014 ( 28%)], Train Loss: 0.19998\n",
      "Epoch: 01 [ 5564/20014 ( 28%)], Train Loss: 0.20010\n",
      "Epoch: 01 [ 5604/20014 ( 28%)], Train Loss: 0.19958\n",
      "Epoch: 01 [ 5644/20014 ( 28%)], Train Loss: 0.19913\n",
      "Epoch: 01 [ 5684/20014 ( 28%)], Train Loss: 0.19882\n",
      "Epoch: 01 [ 5724/20014 ( 29%)], Train Loss: 0.19792\n",
      "Epoch: 01 [ 5764/20014 ( 29%)], Train Loss: 0.19722\n",
      "Epoch: 01 [ 5804/20014 ( 29%)], Train Loss: 0.19730\n",
      "Epoch: 01 [ 5844/20014 ( 29%)], Train Loss: 0.19725\n",
      "Epoch: 01 [ 5884/20014 ( 29%)], Train Loss: 0.19847\n",
      "Epoch: 01 [ 5924/20014 ( 30%)], Train Loss: 0.20011\n",
      "Epoch: 01 [ 5964/20014 ( 30%)], Train Loss: 0.20033\n",
      "Epoch: 01 [ 6004/20014 ( 30%)], Train Loss: 0.20037\n",
      "Epoch: 01 [ 6044/20014 ( 30%)], Train Loss: 0.19974\n",
      "Epoch: 01 [ 6084/20014 ( 30%)], Train Loss: 0.19913\n",
      "Epoch: 01 [ 6124/20014 ( 31%)], Train Loss: 0.19904\n",
      "Epoch: 01 [ 6164/20014 ( 31%)], Train Loss: 0.19915\n",
      "Epoch: 01 [ 6204/20014 ( 31%)], Train Loss: 0.19944\n",
      "Epoch: 01 [ 6244/20014 ( 31%)], Train Loss: 0.19964\n",
      "Epoch: 01 [ 6284/20014 ( 31%)], Train Loss: 0.19932\n",
      "Epoch: 01 [ 6324/20014 ( 32%)], Train Loss: 0.19889\n",
      "Epoch: 01 [ 6364/20014 ( 32%)], Train Loss: 0.19857\n",
      "Epoch: 01 [ 6404/20014 ( 32%)], Train Loss: 0.19844\n",
      "Epoch: 01 [ 6444/20014 ( 32%)], Train Loss: 0.19795\n",
      "Epoch: 01 [ 6484/20014 ( 32%)], Train Loss: 0.19799\n",
      "Epoch: 01 [ 6524/20014 ( 33%)], Train Loss: 0.19724\n",
      "Epoch: 01 [ 6564/20014 ( 33%)], Train Loss: 0.19673\n",
      "Epoch: 01 [ 6604/20014 ( 33%)], Train Loss: 0.19626\n",
      "Epoch: 01 [ 6644/20014 ( 33%)], Train Loss: 0.19647\n",
      "Epoch: 01 [ 6684/20014 ( 33%)], Train Loss: 0.19656\n",
      "Epoch: 01 [ 6724/20014 ( 34%)], Train Loss: 0.19634\n",
      "Epoch: 01 [ 6764/20014 ( 34%)], Train Loss: 0.19640\n",
      "Epoch: 01 [ 6804/20014 ( 34%)], Train Loss: 0.19665\n",
      "Epoch: 01 [ 6844/20014 ( 34%)], Train Loss: 0.19613\n",
      "Epoch: 01 [ 6884/20014 ( 34%)], Train Loss: 0.19639\n",
      "Epoch: 01 [ 6924/20014 ( 35%)], Train Loss: 0.19590\n",
      "Epoch: 01 [ 6964/20014 ( 35%)], Train Loss: 0.19551\n",
      "Epoch: 01 [ 7004/20014 ( 35%)], Train Loss: 0.19520\n",
      "Epoch: 01 [ 7044/20014 ( 35%)], Train Loss: 0.19496\n",
      "Epoch: 01 [ 7084/20014 ( 35%)], Train Loss: 0.19518\n",
      "Epoch: 01 [ 7124/20014 ( 36%)], Train Loss: 0.19537\n",
      "Epoch: 01 [ 7164/20014 ( 36%)], Train Loss: 0.19506\n",
      "Epoch: 01 [ 7204/20014 ( 36%)], Train Loss: 0.19523\n",
      "Epoch: 01 [ 7244/20014 ( 36%)], Train Loss: 0.19488\n",
      "Epoch: 01 [ 7284/20014 ( 36%)], Train Loss: 0.19477\n",
      "Epoch: 01 [ 7324/20014 ( 37%)], Train Loss: 0.19406\n",
      "Epoch: 01 [ 7364/20014 ( 37%)], Train Loss: 0.19367\n",
      "Epoch: 01 [ 7404/20014 ( 37%)], Train Loss: 0.19361\n",
      "Epoch: 01 [ 7444/20014 ( 37%)], Train Loss: 0.19353\n",
      "Epoch: 01 [ 7484/20014 ( 37%)], Train Loss: 0.19296\n",
      "Epoch: 01 [ 7524/20014 ( 38%)], Train Loss: 0.19272\n",
      "Epoch: 01 [ 7564/20014 ( 38%)], Train Loss: 0.19310\n",
      "Epoch: 01 [ 7604/20014 ( 38%)], Train Loss: 0.19258\n",
      "Epoch: 01 [ 7644/20014 ( 38%)], Train Loss: 0.19219\n",
      "Epoch: 01 [ 7684/20014 ( 38%)], Train Loss: 0.19170\n",
      "Epoch: 01 [ 7724/20014 ( 39%)], Train Loss: 0.19141\n",
      "Epoch: 01 [ 7764/20014 ( 39%)], Train Loss: 0.19109\n",
      "Epoch: 01 [ 7804/20014 ( 39%)], Train Loss: 0.19069\n",
      "Epoch: 01 [ 7844/20014 ( 39%)], Train Loss: 0.19054\n",
      "Epoch: 01 [ 7884/20014 ( 39%)], Train Loss: 0.19092\n",
      "Epoch: 01 [ 7924/20014 ( 40%)], Train Loss: 0.19032\n",
      "Epoch: 01 [ 7964/20014 ( 40%)], Train Loss: 0.18988\n",
      "Epoch: 01 [ 8004/20014 ( 40%)], Train Loss: 0.18952\n",
      "Epoch: 01 [ 8044/20014 ( 40%)], Train Loss: 0.18923\n",
      "Epoch: 01 [ 8084/20014 ( 40%)], Train Loss: 0.18968\n",
      "Epoch: 01 [ 8124/20014 ( 41%)], Train Loss: 0.18896\n",
      "Epoch: 01 [ 8164/20014 ( 41%)], Train Loss: 0.18848\n",
      "Epoch: 01 [ 8204/20014 ( 41%)], Train Loss: 0.18840\n",
      "Epoch: 01 [ 8244/20014 ( 41%)], Train Loss: 0.18786\n",
      "Epoch: 01 [ 8284/20014 ( 41%)], Train Loss: 0.18755\n",
      "Epoch: 01 [ 8324/20014 ( 42%)], Train Loss: 0.18681\n",
      "Epoch: 01 [ 8364/20014 ( 42%)], Train Loss: 0.18639\n",
      "Epoch: 01 [ 8404/20014 ( 42%)], Train Loss: 0.18651\n",
      "Epoch: 01 [ 8444/20014 ( 42%)], Train Loss: 0.18641\n",
      "Epoch: 01 [ 8484/20014 ( 42%)], Train Loss: 0.18742\n",
      "Epoch: 01 [ 8524/20014 ( 43%)], Train Loss: 0.18677\n",
      "Epoch: 01 [ 8564/20014 ( 43%)], Train Loss: 0.18644\n",
      "Epoch: 01 [ 8604/20014 ( 43%)], Train Loss: 0.18625\n",
      "Epoch: 01 [ 8644/20014 ( 43%)], Train Loss: 0.18608\n",
      "Epoch: 01 [ 8684/20014 ( 43%)], Train Loss: 0.18572\n",
      "Epoch: 01 [ 8724/20014 ( 44%)], Train Loss: 0.18593\n",
      "Epoch: 01 [ 8764/20014 ( 44%)], Train Loss: 0.18570\n",
      "Epoch: 01 [ 8804/20014 ( 44%)], Train Loss: 0.18516\n",
      "Epoch: 01 [ 8844/20014 ( 44%)], Train Loss: 0.18509\n",
      "Epoch: 01 [ 8884/20014 ( 44%)], Train Loss: 0.18464\n",
      "Epoch: 01 [ 8924/20014 ( 45%)], Train Loss: 0.18441\n",
      "Epoch: 01 [ 8964/20014 ( 45%)], Train Loss: 0.18403\n",
      "Epoch: 01 [ 9004/20014 ( 45%)], Train Loss: 0.18361\n",
      "Epoch: 01 [ 9044/20014 ( 45%)], Train Loss: 0.18344\n",
      "Epoch: 01 [ 9084/20014 ( 45%)], Train Loss: 0.18364\n",
      "Epoch: 01 [ 9124/20014 ( 46%)], Train Loss: 0.18338\n",
      "Epoch: 01 [ 9164/20014 ( 46%)], Train Loss: 0.18303\n",
      "Epoch: 01 [ 9204/20014 ( 46%)], Train Loss: 0.18269\n",
      "Epoch: 01 [ 9244/20014 ( 46%)], Train Loss: 0.18230\n",
      "Epoch: 01 [ 9284/20014 ( 46%)], Train Loss: 0.18198\n",
      "Epoch: 01 [ 9324/20014 ( 47%)], Train Loss: 0.18151\n",
      "Epoch: 01 [ 9364/20014 ( 47%)], Train Loss: 0.18189\n",
      "Epoch: 01 [ 9404/20014 ( 47%)], Train Loss: 0.18132\n",
      "Epoch: 01 [ 9444/20014 ( 47%)], Train Loss: 0.18075\n",
      "Epoch: 01 [ 9484/20014 ( 47%)], Train Loss: 0.18027\n",
      "Epoch: 01 [ 9524/20014 ( 48%)], Train Loss: 0.18035\n",
      "Epoch: 01 [ 9564/20014 ( 48%)], Train Loss: 0.18002\n",
      "Epoch: 01 [ 9604/20014 ( 48%)], Train Loss: 0.17972\n",
      "Epoch: 01 [ 9644/20014 ( 48%)], Train Loss: 0.17951\n",
      "Epoch: 01 [ 9684/20014 ( 48%)], Train Loss: 0.17913\n",
      "Epoch: 01 [ 9724/20014 ( 49%)], Train Loss: 0.17911\n",
      "Epoch: 01 [ 9764/20014 ( 49%)], Train Loss: 0.17858\n",
      "Epoch: 01 [ 9804/20014 ( 49%)], Train Loss: 0.17847\n",
      "Epoch: 01 [ 9844/20014 ( 49%)], Train Loss: 0.17854\n",
      "Epoch: 01 [ 9884/20014 ( 49%)], Train Loss: 0.17800\n",
      "Epoch: 01 [ 9924/20014 ( 50%)], Train Loss: 0.17782\n",
      "Epoch: 01 [ 9964/20014 ( 50%)], Train Loss: 0.17732\n",
      "Epoch: 01 [10004/20014 ( 50%)], Train Loss: 0.17699\n",
      "Epoch: 01 [10044/20014 ( 50%)], Train Loss: 0.17664\n",
      "Epoch: 01 [10084/20014 ( 50%)], Train Loss: 0.17651\n",
      "Epoch: 01 [10124/20014 ( 51%)], Train Loss: 0.17624\n",
      "Epoch: 01 [10164/20014 ( 51%)], Train Loss: 0.17579\n",
      "Epoch: 01 [10204/20014 ( 51%)], Train Loss: 0.17554\n",
      "Epoch: 01 [10244/20014 ( 51%)], Train Loss: 0.17548\n",
      "Epoch: 01 [10284/20014 ( 51%)], Train Loss: 0.17589\n",
      "Epoch: 01 [10324/20014 ( 52%)], Train Loss: 0.17586\n",
      "Epoch: 01 [10364/20014 ( 52%)], Train Loss: 0.17581\n",
      "Epoch: 01 [10404/20014 ( 52%)], Train Loss: 0.17601\n",
      "Epoch: 01 [10444/20014 ( 52%)], Train Loss: 0.17603\n",
      "Epoch: 01 [10484/20014 ( 52%)], Train Loss: 0.17566\n",
      "Epoch: 01 [10524/20014 ( 53%)], Train Loss: 0.17566\n",
      "Epoch: 01 [10564/20014 ( 53%)], Train Loss: 0.17543\n",
      "Epoch: 01 [10604/20014 ( 53%)], Train Loss: 0.17514\n",
      "Epoch: 01 [10644/20014 ( 53%)], Train Loss: 0.17473\n",
      "Epoch: 01 [10684/20014 ( 53%)], Train Loss: 0.17424\n",
      "Epoch: 01 [10724/20014 ( 54%)], Train Loss: 0.17391\n",
      "Epoch: 01 [10764/20014 ( 54%)], Train Loss: 0.17388\n",
      "Epoch: 01 [10804/20014 ( 54%)], Train Loss: 0.17356\n",
      "Epoch: 01 [10844/20014 ( 54%)], Train Loss: 0.17314\n",
      "Epoch: 01 [10884/20014 ( 54%)], Train Loss: 0.17261\n",
      "Epoch: 01 [10924/20014 ( 55%)], Train Loss: 0.17254\n",
      "Epoch: 01 [10964/20014 ( 55%)], Train Loss: 0.17227\n",
      "Epoch: 01 [11004/20014 ( 55%)], Train Loss: 0.17203\n",
      "Epoch: 01 [11044/20014 ( 55%)], Train Loss: 0.17178\n",
      "Epoch: 01 [11084/20014 ( 55%)], Train Loss: 0.17231\n",
      "Epoch: 01 [11124/20014 ( 56%)], Train Loss: 0.17199\n",
      "Epoch: 01 [11164/20014 ( 56%)], Train Loss: 0.17171\n",
      "Epoch: 01 [11204/20014 ( 56%)], Train Loss: 0.17135\n",
      "Epoch: 01 [11244/20014 ( 56%)], Train Loss: 0.17107\n",
      "Epoch: 01 [11284/20014 ( 56%)], Train Loss: 0.17085\n",
      "Epoch: 01 [11324/20014 ( 57%)], Train Loss: 0.17126\n",
      "Epoch: 01 [11364/20014 ( 57%)], Train Loss: 0.17080\n",
      "Epoch: 01 [11404/20014 ( 57%)], Train Loss: 0.17062\n",
      "Epoch: 01 [11444/20014 ( 57%)], Train Loss: 0.17023\n",
      "Epoch: 01 [11484/20014 ( 57%)], Train Loss: 0.17015\n",
      "Epoch: 01 [11524/20014 ( 58%)], Train Loss: 0.17002\n",
      "Epoch: 01 [11564/20014 ( 58%)], Train Loss: 0.16964\n",
      "Epoch: 01 [11604/20014 ( 58%)], Train Loss: 0.16921\n",
      "Epoch: 01 [11644/20014 ( 58%)], Train Loss: 0.16905\n",
      "Epoch: 01 [11684/20014 ( 58%)], Train Loss: 0.16872\n",
      "Epoch: 01 [11724/20014 ( 59%)], Train Loss: 0.16869\n",
      "Epoch: 01 [11764/20014 ( 59%)], Train Loss: 0.16860\n",
      "Epoch: 01 [11804/20014 ( 59%)], Train Loss: 0.16837\n",
      "Epoch: 01 [11844/20014 ( 59%)], Train Loss: 0.16834\n",
      "Epoch: 01 [11884/20014 ( 59%)], Train Loss: 0.16824\n",
      "Epoch: 01 [11924/20014 ( 60%)], Train Loss: 0.16821\n",
      "Epoch: 01 [11964/20014 ( 60%)], Train Loss: 0.16799\n",
      "Epoch: 01 [12004/20014 ( 60%)], Train Loss: 0.16765\n",
      "Epoch: 01 [12044/20014 ( 60%)], Train Loss: 0.16761\n",
      "Epoch: 01 [12084/20014 ( 60%)], Train Loss: 0.16734\n",
      "Epoch: 01 [12124/20014 ( 61%)], Train Loss: 0.16772\n",
      "Epoch: 01 [12164/20014 ( 61%)], Train Loss: 0.16789\n",
      "Epoch: 01 [12204/20014 ( 61%)], Train Loss: 0.16764\n",
      "Epoch: 01 [12244/20014 ( 61%)], Train Loss: 0.16748\n",
      "Epoch: 01 [12284/20014 ( 61%)], Train Loss: 0.16753\n",
      "Epoch: 01 [12324/20014 ( 62%)], Train Loss: 0.16739\n",
      "Epoch: 01 [12364/20014 ( 62%)], Train Loss: 0.16722\n",
      "Epoch: 01 [12404/20014 ( 62%)], Train Loss: 0.16707\n",
      "Epoch: 01 [12444/20014 ( 62%)], Train Loss: 0.16701\n",
      "Epoch: 01 [12484/20014 ( 62%)], Train Loss: 0.16668\n",
      "Epoch: 01 [12524/20014 ( 63%)], Train Loss: 0.16659\n",
      "Epoch: 01 [12564/20014 ( 63%)], Train Loss: 0.16645\n",
      "Epoch: 01 [12604/20014 ( 63%)], Train Loss: 0.16632\n",
      "Epoch: 01 [12644/20014 ( 63%)], Train Loss: 0.16607\n",
      "Epoch: 01 [12684/20014 ( 63%)], Train Loss: 0.16599\n",
      "Epoch: 01 [12724/20014 ( 64%)], Train Loss: 0.16586\n",
      "Epoch: 01 [12764/20014 ( 64%)], Train Loss: 0.16569\n",
      "Epoch: 01 [12804/20014 ( 64%)], Train Loss: 0.16541\n",
      "Epoch: 01 [12844/20014 ( 64%)], Train Loss: 0.16560\n",
      "Epoch: 01 [12884/20014 ( 64%)], Train Loss: 0.16530\n",
      "Epoch: 01 [12924/20014 ( 65%)], Train Loss: 0.16504\n",
      "Epoch: 01 [12964/20014 ( 65%)], Train Loss: 0.16493\n",
      "Epoch: 01 [13004/20014 ( 65%)], Train Loss: 0.16490\n",
      "Epoch: 01 [13044/20014 ( 65%)], Train Loss: 0.16487\n",
      "Epoch: 01 [13084/20014 ( 65%)], Train Loss: 0.16457\n",
      "Epoch: 01 [13124/20014 ( 66%)], Train Loss: 0.16430\n",
      "Epoch: 01 [13164/20014 ( 66%)], Train Loss: 0.16419\n",
      "Epoch: 01 [13204/20014 ( 66%)], Train Loss: 0.16386\n",
      "Epoch: 01 [13244/20014 ( 66%)], Train Loss: 0.16350\n",
      "Epoch: 01 [13284/20014 ( 66%)], Train Loss: 0.16326\n",
      "Epoch: 01 [13324/20014 ( 67%)], Train Loss: 0.16340\n",
      "Epoch: 01 [13364/20014 ( 67%)], Train Loss: 0.16314\n",
      "Epoch: 01 [13404/20014 ( 67%)], Train Loss: 0.16313\n",
      "Epoch: 01 [13444/20014 ( 67%)], Train Loss: 0.16296\n",
      "Epoch: 01 [13484/20014 ( 67%)], Train Loss: 0.16266\n",
      "Epoch: 01 [13524/20014 ( 68%)], Train Loss: 0.16227\n",
      "Epoch: 01 [13564/20014 ( 68%)], Train Loss: 0.16231\n",
      "Epoch: 01 [13604/20014 ( 68%)], Train Loss: 0.16199\n",
      "Epoch: 01 [13644/20014 ( 68%)], Train Loss: 0.16173\n",
      "Epoch: 01 [13684/20014 ( 68%)], Train Loss: 0.16136\n",
      "Epoch: 01 [13724/20014 ( 69%)], Train Loss: 0.16113\n",
      "Epoch: 01 [13764/20014 ( 69%)], Train Loss: 0.16101\n",
      "Epoch: 01 [13804/20014 ( 69%)], Train Loss: 0.16078\n",
      "Epoch: 01 [13844/20014 ( 69%)], Train Loss: 0.16064\n",
      "Epoch: 01 [13884/20014 ( 69%)], Train Loss: 0.16042\n",
      "Epoch: 01 [13924/20014 ( 70%)], Train Loss: 0.16004\n",
      "Epoch: 01 [13964/20014 ( 70%)], Train Loss: 0.15977\n",
      "Epoch: 01 [14004/20014 ( 70%)], Train Loss: 0.15961\n",
      "Epoch: 01 [14044/20014 ( 70%)], Train Loss: 0.15951\n",
      "Epoch: 01 [14084/20014 ( 70%)], Train Loss: 0.15925\n",
      "Epoch: 01 [14124/20014 ( 71%)], Train Loss: 0.15888\n",
      "Epoch: 01 [14164/20014 ( 71%)], Train Loss: 0.15876\n",
      "Epoch: 01 [14204/20014 ( 71%)], Train Loss: 0.15854\n",
      "Epoch: 01 [14244/20014 ( 71%)], Train Loss: 0.15854\n",
      "Epoch: 01 [14284/20014 ( 71%)], Train Loss: 0.15827\n",
      "Epoch: 01 [14324/20014 ( 72%)], Train Loss: 0.15822\n",
      "Epoch: 01 [14364/20014 ( 72%)], Train Loss: 0.15852\n",
      "Epoch: 01 [14404/20014 ( 72%)], Train Loss: 0.15849\n",
      "Epoch: 01 [14444/20014 ( 72%)], Train Loss: 0.15840\n",
      "Epoch: 01 [14484/20014 ( 72%)], Train Loss: 0.15814\n",
      "Epoch: 01 [14524/20014 ( 73%)], Train Loss: 0.15829\n",
      "Epoch: 01 [14564/20014 ( 73%)], Train Loss: 0.15817\n",
      "Epoch: 01 [14604/20014 ( 73%)], Train Loss: 0.15806\n",
      "Epoch: 01 [14644/20014 ( 73%)], Train Loss: 0.15789\n",
      "Epoch: 01 [14684/20014 ( 73%)], Train Loss: 0.15787\n",
      "Epoch: 01 [14724/20014 ( 74%)], Train Loss: 0.15769\n",
      "Epoch: 01 [14764/20014 ( 74%)], Train Loss: 0.15788\n",
      "Epoch: 01 [14804/20014 ( 74%)], Train Loss: 0.15768\n",
      "Epoch: 01 [14844/20014 ( 74%)], Train Loss: 0.15743\n",
      "Epoch: 01 [14884/20014 ( 74%)], Train Loss: 0.15738\n",
      "Epoch: 01 [14924/20014 ( 75%)], Train Loss: 0.15732\n",
      "Epoch: 01 [14964/20014 ( 75%)], Train Loss: 0.15745\n",
      "Epoch: 01 [15004/20014 ( 75%)], Train Loss: 0.15710\n",
      "Epoch: 01 [15044/20014 ( 75%)], Train Loss: 0.15691\n",
      "Epoch: 01 [15084/20014 ( 75%)], Train Loss: 0.15683\n",
      "Epoch: 01 [15124/20014 ( 76%)], Train Loss: 0.15664\n",
      "Epoch: 01 [15164/20014 ( 76%)], Train Loss: 0.15646\n",
      "Epoch: 01 [15204/20014 ( 76%)], Train Loss: 0.15643\n",
      "Epoch: 01 [15244/20014 ( 76%)], Train Loss: 0.15644\n",
      "Epoch: 01 [15284/20014 ( 76%)], Train Loss: 0.15646\n",
      "Epoch: 01 [15324/20014 ( 77%)], Train Loss: 0.15650\n",
      "Epoch: 01 [15364/20014 ( 77%)], Train Loss: 0.15624\n",
      "Epoch: 01 [15404/20014 ( 77%)], Train Loss: 0.15613\n",
      "Epoch: 01 [15444/20014 ( 77%)], Train Loss: 0.15604\n",
      "Epoch: 01 [15484/20014 ( 77%)], Train Loss: 0.15592\n",
      "Epoch: 01 [15524/20014 ( 78%)], Train Loss: 0.15569\n",
      "Epoch: 01 [15564/20014 ( 78%)], Train Loss: 0.15552\n",
      "Epoch: 01 [15604/20014 ( 78%)], Train Loss: 0.15529\n",
      "Epoch: 01 [15644/20014 ( 78%)], Train Loss: 0.15506\n",
      "Epoch: 01 [15684/20014 ( 78%)], Train Loss: 0.15486\n",
      "Epoch: 01 [15724/20014 ( 79%)], Train Loss: 0.15484\n",
      "Epoch: 01 [15764/20014 ( 79%)], Train Loss: 0.15466\n",
      "Epoch: 01 [15804/20014 ( 79%)], Train Loss: 0.15449\n",
      "Epoch: 01 [15844/20014 ( 79%)], Train Loss: 0.15443\n",
      "Epoch: 01 [15884/20014 ( 79%)], Train Loss: 0.15460\n",
      "Epoch: 01 [15924/20014 ( 80%)], Train Loss: 0.15459\n",
      "Epoch: 01 [15964/20014 ( 80%)], Train Loss: 0.15433\n",
      "Epoch: 01 [16004/20014 ( 80%)], Train Loss: 0.15416\n",
      "Epoch: 01 [16044/20014 ( 80%)], Train Loss: 0.15397\n",
      "Epoch: 01 [16084/20014 ( 80%)], Train Loss: 0.15381\n",
      "Epoch: 01 [16124/20014 ( 81%)], Train Loss: 0.15360\n",
      "Epoch: 01 [16164/20014 ( 81%)], Train Loss: 0.15331\n",
      "Epoch: 01 [16204/20014 ( 81%)], Train Loss: 0.15332\n",
      "Epoch: 01 [16244/20014 ( 81%)], Train Loss: 0.15313\n",
      "Epoch: 01 [16284/20014 ( 81%)], Train Loss: 0.15298\n",
      "Epoch: 01 [16324/20014 ( 82%)], Train Loss: 0.15280\n",
      "Epoch: 01 [16364/20014 ( 82%)], Train Loss: 0.15267\n",
      "Epoch: 01 [16404/20014 ( 82%)], Train Loss: 0.15248\n",
      "Epoch: 01 [16444/20014 ( 82%)], Train Loss: 0.15224\n",
      "Epoch: 01 [16484/20014 ( 82%)], Train Loss: 0.15247\n",
      "Epoch: 01 [16524/20014 ( 83%)], Train Loss: 0.15239\n",
      "Epoch: 01 [16564/20014 ( 83%)], Train Loss: 0.15282\n",
      "Epoch: 01 [16604/20014 ( 83%)], Train Loss: 0.15272\n",
      "Epoch: 01 [16644/20014 ( 83%)], Train Loss: 0.15253\n",
      "Epoch: 01 [16684/20014 ( 83%)], Train Loss: 0.15227\n",
      "Epoch: 01 [16724/20014 ( 84%)], Train Loss: 0.15216\n",
      "Epoch: 01 [16764/20014 ( 84%)], Train Loss: 0.15212\n",
      "Epoch: 01 [16804/20014 ( 84%)], Train Loss: 0.15192\n",
      "Epoch: 01 [16844/20014 ( 84%)], Train Loss: 0.15173\n",
      "Epoch: 01 [16884/20014 ( 84%)], Train Loss: 0.15182\n",
      "Epoch: 01 [16924/20014 ( 85%)], Train Loss: 0.15178\n",
      "Epoch: 01 [16964/20014 ( 85%)], Train Loss: 0.15161\n",
      "Epoch: 01 [17004/20014 ( 85%)], Train Loss: 0.15158\n",
      "Epoch: 01 [17044/20014 ( 85%)], Train Loss: 0.15158\n",
      "Epoch: 01 [17084/20014 ( 85%)], Train Loss: 0.15128\n",
      "Epoch: 01 [17124/20014 ( 86%)], Train Loss: 0.15126\n",
      "Epoch: 01 [17164/20014 ( 86%)], Train Loss: 0.15120\n",
      "Epoch: 01 [17204/20014 ( 86%)], Train Loss: 0.15106\n",
      "Epoch: 01 [17244/20014 ( 86%)], Train Loss: 0.15103\n",
      "Epoch: 01 [17284/20014 ( 86%)], Train Loss: 0.15085\n",
      "Epoch: 01 [17324/20014 ( 87%)], Train Loss: 0.15061\n",
      "Epoch: 01 [17364/20014 ( 87%)], Train Loss: 0.15057\n",
      "Epoch: 01 [17404/20014 ( 87%)], Train Loss: 0.15049\n",
      "Epoch: 01 [17444/20014 ( 87%)], Train Loss: 0.15042\n",
      "Epoch: 01 [17484/20014 ( 87%)], Train Loss: 0.15044\n",
      "Epoch: 01 [17524/20014 ( 88%)], Train Loss: 0.15045\n",
      "Epoch: 01 [17564/20014 ( 88%)], Train Loss: 0.15049\n",
      "Epoch: 01 [17604/20014 ( 88%)], Train Loss: 0.15036\n",
      "Epoch: 01 [17644/20014 ( 88%)], Train Loss: 0.15026\n",
      "Epoch: 01 [17684/20014 ( 88%)], Train Loss: 0.15011\n",
      "Epoch: 01 [17724/20014 ( 89%)], Train Loss: 0.15003\n",
      "Epoch: 01 [17764/20014 ( 89%)], Train Loss: 0.14994\n",
      "Epoch: 01 [17804/20014 ( 89%)], Train Loss: 0.15009\n",
      "Epoch: 01 [17844/20014 ( 89%)], Train Loss: 0.14995\n",
      "Epoch: 01 [17884/20014 ( 89%)], Train Loss: 0.14991\n",
      "Epoch: 01 [17924/20014 ( 90%)], Train Loss: 0.14968\n",
      "Epoch: 01 [17964/20014 ( 90%)], Train Loss: 0.14956\n",
      "Epoch: 01 [18004/20014 ( 90%)], Train Loss: 0.14948\n",
      "Epoch: 01 [18044/20014 ( 90%)], Train Loss: 0.14961\n",
      "Epoch: 01 [18084/20014 ( 90%)], Train Loss: 0.14959\n",
      "Epoch: 01 [18124/20014 ( 91%)], Train Loss: 0.14944\n",
      "Epoch: 01 [18164/20014 ( 91%)], Train Loss: 0.14934\n",
      "Epoch: 01 [18204/20014 ( 91%)], Train Loss: 0.14918\n",
      "Epoch: 01 [18244/20014 ( 91%)], Train Loss: 0.14920\n",
      "Epoch: 01 [18284/20014 ( 91%)], Train Loss: 0.14934\n",
      "Epoch: 01 [18324/20014 ( 92%)], Train Loss: 0.14916\n",
      "Epoch: 01 [18364/20014 ( 92%)], Train Loss: 0.14917\n",
      "Epoch: 01 [18404/20014 ( 92%)], Train Loss: 0.14906\n",
      "Epoch: 01 [18444/20014 ( 92%)], Train Loss: 0.14890\n",
      "Epoch: 01 [18484/20014 ( 92%)], Train Loss: 0.14884\n",
      "Epoch: 01 [18524/20014 ( 93%)], Train Loss: 0.14866\n",
      "Epoch: 01 [18564/20014 ( 93%)], Train Loss: 0.14850\n",
      "Epoch: 01 [18604/20014 ( 93%)], Train Loss: 0.14826\n",
      "Epoch: 01 [18644/20014 ( 93%)], Train Loss: 0.14829\n",
      "Epoch: 01 [18684/20014 ( 93%)], Train Loss: 0.14811\n",
      "Epoch: 01 [18724/20014 ( 94%)], Train Loss: 0.14826\n",
      "Epoch: 01 [18764/20014 ( 94%)], Train Loss: 0.14832\n",
      "Epoch: 01 [18804/20014 ( 94%)], Train Loss: 0.14820\n",
      "Epoch: 01 [18844/20014 ( 94%)], Train Loss: 0.14822\n",
      "Epoch: 01 [18884/20014 ( 94%)], Train Loss: 0.14825\n",
      "Epoch: 01 [18924/20014 ( 95%)], Train Loss: 0.14802\n",
      "Epoch: 01 [18964/20014 ( 95%)], Train Loss: 0.14807\n",
      "Epoch: 01 [19004/20014 ( 95%)], Train Loss: 0.14800\n",
      "Epoch: 01 [19044/20014 ( 95%)], Train Loss: 0.14810\n",
      "Epoch: 01 [19084/20014 ( 95%)], Train Loss: 0.14807\n",
      "Epoch: 01 [19124/20014 ( 96%)], Train Loss: 0.14799\n",
      "Epoch: 01 [19164/20014 ( 96%)], Train Loss: 0.14803\n",
      "Epoch: 01 [19204/20014 ( 96%)], Train Loss: 0.14794\n",
      "Epoch: 01 [19244/20014 ( 96%)], Train Loss: 0.14781\n",
      "Epoch: 01 [19284/20014 ( 96%)], Train Loss: 0.14768\n",
      "Epoch: 01 [19324/20014 ( 97%)], Train Loss: 0.14783\n",
      "Epoch: 01 [19364/20014 ( 97%)], Train Loss: 0.14776\n",
      "Epoch: 01 [19404/20014 ( 97%)], Train Loss: 0.14765\n",
      "Epoch: 01 [19444/20014 ( 97%)], Train Loss: 0.14753\n",
      "Epoch: 01 [19484/20014 ( 97%)], Train Loss: 0.14737\n",
      "Epoch: 01 [19524/20014 ( 98%)], Train Loss: 0.14745\n",
      "Epoch: 01 [19564/20014 ( 98%)], Train Loss: 0.14730\n",
      "Epoch: 01 [19604/20014 ( 98%)], Train Loss: 0.14711\n",
      "Epoch: 01 [19644/20014 ( 98%)], Train Loss: 0.14707\n",
      "Epoch: 01 [19684/20014 ( 98%)], Train Loss: 0.14709\n",
      "Epoch: 01 [19724/20014 ( 99%)], Train Loss: 0.14720\n",
      "Epoch: 01 [19764/20014 ( 99%)], Train Loss: 0.14705\n",
      "Epoch: 01 [19804/20014 ( 99%)], Train Loss: 0.14720\n",
      "Epoch: 01 [19844/20014 ( 99%)], Train Loss: 0.14702\n",
      "Epoch: 01 [19884/20014 ( 99%)], Train Loss: 0.14702\n",
      "Epoch: 01 [19924/20014 (100%)], Train Loss: 0.14683\n",
      "Epoch: 01 [19964/20014 (100%)], Train Loss: 0.14687\n",
      "Epoch: 01 [20004/20014 (100%)], Train Loss: 0.14665\n",
      "Epoch: 01 [20014/20014 (100%)], Train Loss: 0.14662\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.28075\n",
      "\n",
      "Total Training Time: 5654.684762001038secs, Average Training Time per Epoch: 2827.342381000519secs.\n",
      "Total Validation Time: 257.75185799598694secs, Average Validation Time per Epoch: 128.87592899799347secs.\n"
     ]
    }
   ],
   "source": [
    "#example for training second fold\n",
    "\n",
    "for fold in range(1, 2):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d41a826c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T06:37:44.582954Z",
     "iopub.status.busy": "2021-10-03T06:37:44.582429Z",
     "iopub.status.idle": "2021-10-03T08:18:48.854846Z",
     "shell.execute_reply": "2021-10-03T08:18:48.855283Z"
    },
    "papermill": {
     "duration": 6064.817659,
     "end_time": "2021-10-03T08:18:48.855446",
     "exception": false,
     "start_time": "2021-10-03T06:37:44.037787",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 2\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20299, Num examples Valid=2770\n",
      "Total Training Steps: 5076, Total Warmup Steps: 507\n",
      "Epoch: 00 [    4/20299 (  0%)], Train Loss: 3.09636\n",
      "Epoch: 00 [   44/20299 (  0%)], Train Loss: 3.07231\n",
      "Epoch: 00 [   84/20299 (  0%)], Train Loss: 3.05850\n",
      "Epoch: 00 [  124/20299 (  1%)], Train Loss: 3.04404\n",
      "Epoch: 00 [  164/20299 (  1%)], Train Loss: 3.01934\n",
      "Epoch: 00 [  204/20299 (  1%)], Train Loss: 2.99455\n",
      "Epoch: 00 [  244/20299 (  1%)], Train Loss: 2.95727\n",
      "Epoch: 00 [  284/20299 (  1%)], Train Loss: 2.91644\n",
      "Epoch: 00 [  324/20299 (  2%)], Train Loss: 2.86379\n",
      "Epoch: 00 [  364/20299 (  2%)], Train Loss: 2.80762\n",
      "Epoch: 00 [  404/20299 (  2%)], Train Loss: 2.74476\n",
      "Epoch: 00 [  444/20299 (  2%)], Train Loss: 2.66836\n",
      "Epoch: 00 [  484/20299 (  2%)], Train Loss: 2.58437\n",
      "Epoch: 00 [  524/20299 (  3%)], Train Loss: 2.49083\n",
      "Epoch: 00 [  564/20299 (  3%)], Train Loss: 2.41933\n",
      "Epoch: 00 [  604/20299 (  3%)], Train Loss: 2.32318\n",
      "Epoch: 00 [  644/20299 (  3%)], Train Loss: 2.22436\n",
      "Epoch: 00 [  684/20299 (  3%)], Train Loss: 2.13315\n",
      "Epoch: 00 [  724/20299 (  4%)], Train Loss: 2.04674\n",
      "Epoch: 00 [  764/20299 (  4%)], Train Loss: 1.98000\n",
      "Epoch: 00 [  804/20299 (  4%)], Train Loss: 1.90856\n",
      "Epoch: 00 [  844/20299 (  4%)], Train Loss: 1.84568\n",
      "Epoch: 00 [  884/20299 (  4%)], Train Loss: 1.79281\n",
      "Epoch: 00 [  924/20299 (  5%)], Train Loss: 1.74059\n",
      "Epoch: 00 [  964/20299 (  5%)], Train Loss: 1.69022\n",
      "Epoch: 00 [ 1004/20299 (  5%)], Train Loss: 1.64363\n",
      "Epoch: 00 [ 1044/20299 (  5%)], Train Loss: 1.61367\n",
      "Epoch: 00 [ 1084/20299 (  5%)], Train Loss: 1.57150\n",
      "Epoch: 00 [ 1124/20299 (  6%)], Train Loss: 1.53584\n",
      "Epoch: 00 [ 1164/20299 (  6%)], Train Loss: 1.49724\n",
      "Epoch: 00 [ 1204/20299 (  6%)], Train Loss: 1.45865\n",
      "Epoch: 00 [ 1244/20299 (  6%)], Train Loss: 1.43763\n",
      "Epoch: 00 [ 1284/20299 (  6%)], Train Loss: 1.40516\n",
      "Epoch: 00 [ 1324/20299 (  7%)], Train Loss: 1.37928\n",
      "Epoch: 00 [ 1364/20299 (  7%)], Train Loss: 1.35429\n",
      "Epoch: 00 [ 1404/20299 (  7%)], Train Loss: 1.32508\n",
      "Epoch: 00 [ 1444/20299 (  7%)], Train Loss: 1.30461\n",
      "Epoch: 00 [ 1484/20299 (  7%)], Train Loss: 1.28072\n",
      "Epoch: 00 [ 1524/20299 (  8%)], Train Loss: 1.26008\n",
      "Epoch: 00 [ 1564/20299 (  8%)], Train Loss: 1.24154\n",
      "Epoch: 00 [ 1604/20299 (  8%)], Train Loss: 1.22087\n",
      "Epoch: 00 [ 1644/20299 (  8%)], Train Loss: 1.20139\n",
      "Epoch: 00 [ 1684/20299 (  8%)], Train Loss: 1.18249\n",
      "Epoch: 00 [ 1724/20299 (  8%)], Train Loss: 1.16489\n",
      "Epoch: 00 [ 1764/20299 (  9%)], Train Loss: 1.15142\n",
      "Epoch: 00 [ 1804/20299 (  9%)], Train Loss: 1.13689\n",
      "Epoch: 00 [ 1844/20299 (  9%)], Train Loss: 1.11616\n",
      "Epoch: 00 [ 1884/20299 (  9%)], Train Loss: 1.09974\n",
      "Epoch: 00 [ 1924/20299 (  9%)], Train Loss: 1.08723\n",
      "Epoch: 00 [ 1964/20299 ( 10%)], Train Loss: 1.07212\n",
      "Epoch: 00 [ 2004/20299 ( 10%)], Train Loss: 1.05839\n",
      "Epoch: 00 [ 2044/20299 ( 10%)], Train Loss: 1.04414\n",
      "Epoch: 00 [ 2084/20299 ( 10%)], Train Loss: 1.03373\n",
      "Epoch: 00 [ 2124/20299 ( 10%)], Train Loss: 1.02232\n",
      "Epoch: 00 [ 2164/20299 ( 11%)], Train Loss: 1.01040\n",
      "Epoch: 00 [ 2204/20299 ( 11%)], Train Loss: 0.99668\n",
      "Epoch: 00 [ 2244/20299 ( 11%)], Train Loss: 0.98537\n",
      "Epoch: 00 [ 2284/20299 ( 11%)], Train Loss: 0.97362\n",
      "Epoch: 00 [ 2324/20299 ( 11%)], Train Loss: 0.96275\n",
      "Epoch: 00 [ 2364/20299 ( 12%)], Train Loss: 0.95421\n",
      "Epoch: 00 [ 2404/20299 ( 12%)], Train Loss: 0.94660\n",
      "Epoch: 00 [ 2444/20299 ( 12%)], Train Loss: 0.93781\n",
      "Epoch: 00 [ 2484/20299 ( 12%)], Train Loss: 0.92886\n",
      "Epoch: 00 [ 2524/20299 ( 12%)], Train Loss: 0.91794\n",
      "Epoch: 00 [ 2564/20299 ( 13%)], Train Loss: 0.91090\n",
      "Epoch: 00 [ 2604/20299 ( 13%)], Train Loss: 0.90015\n",
      "Epoch: 00 [ 2644/20299 ( 13%)], Train Loss: 0.89553\n",
      "Epoch: 00 [ 2684/20299 ( 13%)], Train Loss: 0.88926\n",
      "Epoch: 00 [ 2724/20299 ( 13%)], Train Loss: 0.88512\n",
      "Epoch: 00 [ 2764/20299 ( 14%)], Train Loss: 0.87582\n",
      "Epoch: 00 [ 2804/20299 ( 14%)], Train Loss: 0.86896\n",
      "Epoch: 00 [ 2844/20299 ( 14%)], Train Loss: 0.86176\n",
      "Epoch: 00 [ 2884/20299 ( 14%)], Train Loss: 0.85411\n",
      "Epoch: 00 [ 2924/20299 ( 14%)], Train Loss: 0.84793\n",
      "Epoch: 00 [ 2964/20299 ( 15%)], Train Loss: 0.84198\n",
      "Epoch: 00 [ 3004/20299 ( 15%)], Train Loss: 0.83551\n",
      "Epoch: 00 [ 3044/20299 ( 15%)], Train Loss: 0.82813\n",
      "Epoch: 00 [ 3084/20299 ( 15%)], Train Loss: 0.82558\n",
      "Epoch: 00 [ 3124/20299 ( 15%)], Train Loss: 0.82009\n",
      "Epoch: 00 [ 3164/20299 ( 16%)], Train Loss: 0.81749\n",
      "Epoch: 00 [ 3204/20299 ( 16%)], Train Loss: 0.81484\n",
      "Epoch: 00 [ 3244/20299 ( 16%)], Train Loss: 0.80945\n",
      "Epoch: 00 [ 3284/20299 ( 16%)], Train Loss: 0.80527\n",
      "Epoch: 00 [ 3324/20299 ( 16%)], Train Loss: 0.80174\n",
      "Epoch: 00 [ 3364/20299 ( 17%)], Train Loss: 0.79679\n",
      "Epoch: 00 [ 3404/20299 ( 17%)], Train Loss: 0.79059\n",
      "Epoch: 00 [ 3444/20299 ( 17%)], Train Loss: 0.78492\n",
      "Epoch: 00 [ 3484/20299 ( 17%)], Train Loss: 0.77925\n",
      "Epoch: 00 [ 3524/20299 ( 17%)], Train Loss: 0.77494\n",
      "Epoch: 00 [ 3564/20299 ( 18%)], Train Loss: 0.77117\n",
      "Epoch: 00 [ 3604/20299 ( 18%)], Train Loss: 0.76694\n",
      "Epoch: 00 [ 3644/20299 ( 18%)], Train Loss: 0.76425\n",
      "Epoch: 00 [ 3684/20299 ( 18%)], Train Loss: 0.76013\n",
      "Epoch: 00 [ 3724/20299 ( 18%)], Train Loss: 0.75577\n",
      "Epoch: 00 [ 3764/20299 ( 19%)], Train Loss: 0.75405\n",
      "Epoch: 00 [ 3804/20299 ( 19%)], Train Loss: 0.75190\n",
      "Epoch: 00 [ 3844/20299 ( 19%)], Train Loss: 0.74877\n",
      "Epoch: 00 [ 3884/20299 ( 19%)], Train Loss: 0.74686\n",
      "Epoch: 00 [ 3924/20299 ( 19%)], Train Loss: 0.74415\n",
      "Epoch: 00 [ 3964/20299 ( 20%)], Train Loss: 0.74268\n",
      "Epoch: 00 [ 4004/20299 ( 20%)], Train Loss: 0.73963\n",
      "Epoch: 00 [ 4044/20299 ( 20%)], Train Loss: 0.73575\n",
      "Epoch: 00 [ 4084/20299 ( 20%)], Train Loss: 0.73265\n",
      "Epoch: 00 [ 4124/20299 ( 20%)], Train Loss: 0.72733\n",
      "Epoch: 00 [ 4164/20299 ( 21%)], Train Loss: 0.72516\n",
      "Epoch: 00 [ 4204/20299 ( 21%)], Train Loss: 0.71993\n",
      "Epoch: 00 [ 4244/20299 ( 21%)], Train Loss: 0.71681\n",
      "Epoch: 00 [ 4284/20299 ( 21%)], Train Loss: 0.71362\n",
      "Epoch: 00 [ 4324/20299 ( 21%)], Train Loss: 0.70971\n",
      "Epoch: 00 [ 4364/20299 ( 21%)], Train Loss: 0.70621\n",
      "Epoch: 00 [ 4404/20299 ( 22%)], Train Loss: 0.70445\n",
      "Epoch: 00 [ 4444/20299 ( 22%)], Train Loss: 0.70383\n",
      "Epoch: 00 [ 4484/20299 ( 22%)], Train Loss: 0.70155\n",
      "Epoch: 00 [ 4524/20299 ( 22%)], Train Loss: 0.70014\n",
      "Epoch: 00 [ 4564/20299 ( 22%)], Train Loss: 0.70036\n",
      "Epoch: 00 [ 4604/20299 ( 23%)], Train Loss: 0.69731\n",
      "Epoch: 00 [ 4644/20299 ( 23%)], Train Loss: 0.69582\n",
      "Epoch: 00 [ 4684/20299 ( 23%)], Train Loss: 0.69464\n",
      "Epoch: 00 [ 4724/20299 ( 23%)], Train Loss: 0.69263\n",
      "Epoch: 00 [ 4764/20299 ( 23%)], Train Loss: 0.68982\n",
      "Epoch: 00 [ 4804/20299 ( 24%)], Train Loss: 0.68891\n",
      "Epoch: 00 [ 4844/20299 ( 24%)], Train Loss: 0.68608\n",
      "Epoch: 00 [ 4884/20299 ( 24%)], Train Loss: 0.68446\n",
      "Epoch: 00 [ 4924/20299 ( 24%)], Train Loss: 0.68328\n",
      "Epoch: 00 [ 4964/20299 ( 24%)], Train Loss: 0.68133\n",
      "Epoch: 00 [ 5004/20299 ( 25%)], Train Loss: 0.68082\n",
      "Epoch: 00 [ 5044/20299 ( 25%)], Train Loss: 0.67815\n",
      "Epoch: 00 [ 5084/20299 ( 25%)], Train Loss: 0.67557\n",
      "Epoch: 00 [ 5124/20299 ( 25%)], Train Loss: 0.67222\n",
      "Epoch: 00 [ 5164/20299 ( 25%)], Train Loss: 0.66908\n",
      "Epoch: 00 [ 5204/20299 ( 26%)], Train Loss: 0.66822\n",
      "Epoch: 00 [ 5244/20299 ( 26%)], Train Loss: 0.66533\n",
      "Epoch: 00 [ 5284/20299 ( 26%)], Train Loss: 0.66363\n",
      "Epoch: 00 [ 5324/20299 ( 26%)], Train Loss: 0.66206\n",
      "Epoch: 00 [ 5364/20299 ( 26%)], Train Loss: 0.65944\n",
      "Epoch: 00 [ 5404/20299 ( 27%)], Train Loss: 0.65816\n",
      "Epoch: 00 [ 5444/20299 ( 27%)], Train Loss: 0.65560\n",
      "Epoch: 00 [ 5484/20299 ( 27%)], Train Loss: 0.65400\n",
      "Epoch: 00 [ 5524/20299 ( 27%)], Train Loss: 0.65228\n",
      "Epoch: 00 [ 5564/20299 ( 27%)], Train Loss: 0.64956\n",
      "Epoch: 00 [ 5604/20299 ( 28%)], Train Loss: 0.64688\n",
      "Epoch: 00 [ 5644/20299 ( 28%)], Train Loss: 0.64571\n",
      "Epoch: 00 [ 5684/20299 ( 28%)], Train Loss: 0.64522\n",
      "Epoch: 00 [ 5724/20299 ( 28%)], Train Loss: 0.64322\n",
      "Epoch: 00 [ 5764/20299 ( 28%)], Train Loss: 0.64209\n",
      "Epoch: 00 [ 5804/20299 ( 29%)], Train Loss: 0.64138\n",
      "Epoch: 00 [ 5844/20299 ( 29%)], Train Loss: 0.64075\n",
      "Epoch: 00 [ 5884/20299 ( 29%)], Train Loss: 0.63949\n",
      "Epoch: 00 [ 5924/20299 ( 29%)], Train Loss: 0.63734\n",
      "Epoch: 00 [ 5964/20299 ( 29%)], Train Loss: 0.63671\n",
      "Epoch: 00 [ 6004/20299 ( 30%)], Train Loss: 0.63549\n",
      "Epoch: 00 [ 6044/20299 ( 30%)], Train Loss: 0.63518\n",
      "Epoch: 00 [ 6084/20299 ( 30%)], Train Loss: 0.63301\n",
      "Epoch: 00 [ 6124/20299 ( 30%)], Train Loss: 0.63036\n",
      "Epoch: 00 [ 6164/20299 ( 30%)], Train Loss: 0.62829\n",
      "Epoch: 00 [ 6204/20299 ( 31%)], Train Loss: 0.62611\n",
      "Epoch: 00 [ 6244/20299 ( 31%)], Train Loss: 0.62540\n",
      "Epoch: 00 [ 6284/20299 ( 31%)], Train Loss: 0.62548\n",
      "Epoch: 00 [ 6324/20299 ( 31%)], Train Loss: 0.62611\n",
      "Epoch: 00 [ 6364/20299 ( 31%)], Train Loss: 0.62453\n",
      "Epoch: 00 [ 6404/20299 ( 32%)], Train Loss: 0.62383\n",
      "Epoch: 00 [ 6444/20299 ( 32%)], Train Loss: 0.62134\n",
      "Epoch: 00 [ 6484/20299 ( 32%)], Train Loss: 0.61993\n",
      "Epoch: 00 [ 6524/20299 ( 32%)], Train Loss: 0.61849\n",
      "Epoch: 00 [ 6564/20299 ( 32%)], Train Loss: 0.61858\n",
      "Epoch: 00 [ 6604/20299 ( 33%)], Train Loss: 0.61767\n",
      "Epoch: 00 [ 6644/20299 ( 33%)], Train Loss: 0.61585\n",
      "Epoch: 00 [ 6684/20299 ( 33%)], Train Loss: 0.61417\n",
      "Epoch: 00 [ 6724/20299 ( 33%)], Train Loss: 0.61300\n",
      "Epoch: 00 [ 6764/20299 ( 33%)], Train Loss: 0.61239\n",
      "Epoch: 00 [ 6804/20299 ( 34%)], Train Loss: 0.61197\n",
      "Epoch: 00 [ 6844/20299 ( 34%)], Train Loss: 0.61004\n",
      "Epoch: 00 [ 6884/20299 ( 34%)], Train Loss: 0.60904\n",
      "Epoch: 00 [ 6924/20299 ( 34%)], Train Loss: 0.60803\n",
      "Epoch: 00 [ 6964/20299 ( 34%)], Train Loss: 0.60672\n",
      "Epoch: 00 [ 7004/20299 ( 35%)], Train Loss: 0.60497\n",
      "Epoch: 00 [ 7044/20299 ( 35%)], Train Loss: 0.60363\n",
      "Epoch: 00 [ 7084/20299 ( 35%)], Train Loss: 0.60188\n",
      "Epoch: 00 [ 7124/20299 ( 35%)], Train Loss: 0.59969\n",
      "Epoch: 00 [ 7164/20299 ( 35%)], Train Loss: 0.59807\n",
      "Epoch: 00 [ 7204/20299 ( 35%)], Train Loss: 0.59718\n",
      "Epoch: 00 [ 7244/20299 ( 36%)], Train Loss: 0.59553\n",
      "Epoch: 00 [ 7284/20299 ( 36%)], Train Loss: 0.59419\n",
      "Epoch: 00 [ 7324/20299 ( 36%)], Train Loss: 0.59299\n",
      "Epoch: 00 [ 7364/20299 ( 36%)], Train Loss: 0.59172\n",
      "Epoch: 00 [ 7404/20299 ( 36%)], Train Loss: 0.59098\n",
      "Epoch: 00 [ 7444/20299 ( 37%)], Train Loss: 0.59123\n",
      "Epoch: 00 [ 7484/20299 ( 37%)], Train Loss: 0.59000\n",
      "Epoch: 00 [ 7524/20299 ( 37%)], Train Loss: 0.58920\n",
      "Epoch: 00 [ 7564/20299 ( 37%)], Train Loss: 0.58863\n",
      "Epoch: 00 [ 7604/20299 ( 37%)], Train Loss: 0.58824\n",
      "Epoch: 00 [ 7644/20299 ( 38%)], Train Loss: 0.58705\n",
      "Epoch: 00 [ 7684/20299 ( 38%)], Train Loss: 0.58665\n",
      "Epoch: 00 [ 7724/20299 ( 38%)], Train Loss: 0.58603\n",
      "Epoch: 00 [ 7764/20299 ( 38%)], Train Loss: 0.58487\n",
      "Epoch: 00 [ 7804/20299 ( 38%)], Train Loss: 0.58424\n",
      "Epoch: 00 [ 7844/20299 ( 39%)], Train Loss: 0.58262\n",
      "Epoch: 00 [ 7884/20299 ( 39%)], Train Loss: 0.58224\n",
      "Epoch: 00 [ 7924/20299 ( 39%)], Train Loss: 0.58121\n",
      "Epoch: 00 [ 7964/20299 ( 39%)], Train Loss: 0.57985\n",
      "Epoch: 00 [ 8004/20299 ( 39%)], Train Loss: 0.57899\n",
      "Epoch: 00 [ 8044/20299 ( 40%)], Train Loss: 0.57831\n",
      "Epoch: 00 [ 8084/20299 ( 40%)], Train Loss: 0.57885\n",
      "Epoch: 00 [ 8124/20299 ( 40%)], Train Loss: 0.57662\n",
      "Epoch: 00 [ 8164/20299 ( 40%)], Train Loss: 0.57601\n",
      "Epoch: 00 [ 8204/20299 ( 40%)], Train Loss: 0.57541\n",
      "Epoch: 00 [ 8244/20299 ( 41%)], Train Loss: 0.57392\n",
      "Epoch: 00 [ 8284/20299 ( 41%)], Train Loss: 0.57311\n",
      "Epoch: 00 [ 8324/20299 ( 41%)], Train Loss: 0.57300\n",
      "Epoch: 00 [ 8364/20299 ( 41%)], Train Loss: 0.57267\n",
      "Epoch: 00 [ 8404/20299 ( 41%)], Train Loss: 0.57195\n",
      "Epoch: 00 [ 8444/20299 ( 42%)], Train Loss: 0.57122\n",
      "Epoch: 00 [ 8484/20299 ( 42%)], Train Loss: 0.57148\n",
      "Epoch: 00 [ 8524/20299 ( 42%)], Train Loss: 0.57031\n",
      "Epoch: 00 [ 8564/20299 ( 42%)], Train Loss: 0.56883\n",
      "Epoch: 00 [ 8604/20299 ( 42%)], Train Loss: 0.56830\n",
      "Epoch: 00 [ 8644/20299 ( 43%)], Train Loss: 0.56762\n",
      "Epoch: 00 [ 8684/20299 ( 43%)], Train Loss: 0.56604\n",
      "Epoch: 00 [ 8724/20299 ( 43%)], Train Loss: 0.56700\n",
      "Epoch: 00 [ 8764/20299 ( 43%)], Train Loss: 0.56724\n",
      "Epoch: 00 [ 8804/20299 ( 43%)], Train Loss: 0.56693\n",
      "Epoch: 00 [ 8844/20299 ( 44%)], Train Loss: 0.56506\n",
      "Epoch: 00 [ 8884/20299 ( 44%)], Train Loss: 0.56460\n",
      "Epoch: 00 [ 8924/20299 ( 44%)], Train Loss: 0.56344\n",
      "Epoch: 00 [ 8964/20299 ( 44%)], Train Loss: 0.56290\n",
      "Epoch: 00 [ 9004/20299 ( 44%)], Train Loss: 0.56393\n",
      "Epoch: 00 [ 9044/20299 ( 45%)], Train Loss: 0.56315\n",
      "Epoch: 00 [ 9084/20299 ( 45%)], Train Loss: 0.56213\n",
      "Epoch: 00 [ 9124/20299 ( 45%)], Train Loss: 0.56143\n",
      "Epoch: 00 [ 9164/20299 ( 45%)], Train Loss: 0.56032\n",
      "Epoch: 00 [ 9204/20299 ( 45%)], Train Loss: 0.55886\n",
      "Epoch: 00 [ 9244/20299 ( 46%)], Train Loss: 0.55735\n",
      "Epoch: 00 [ 9284/20299 ( 46%)], Train Loss: 0.55703\n",
      "Epoch: 00 [ 9324/20299 ( 46%)], Train Loss: 0.55818\n",
      "Epoch: 00 [ 9364/20299 ( 46%)], Train Loss: 0.55699\n",
      "Epoch: 00 [ 9404/20299 ( 46%)], Train Loss: 0.55557\n",
      "Epoch: 00 [ 9444/20299 ( 47%)], Train Loss: 0.55496\n",
      "Epoch: 00 [ 9484/20299 ( 47%)], Train Loss: 0.55440\n",
      "Epoch: 00 [ 9524/20299 ( 47%)], Train Loss: 0.55453\n",
      "Epoch: 00 [ 9564/20299 ( 47%)], Train Loss: 0.55371\n",
      "Epoch: 00 [ 9604/20299 ( 47%)], Train Loss: 0.55280\n",
      "Epoch: 00 [ 9644/20299 ( 48%)], Train Loss: 0.55245\n",
      "Epoch: 00 [ 9684/20299 ( 48%)], Train Loss: 0.55154\n",
      "Epoch: 00 [ 9724/20299 ( 48%)], Train Loss: 0.55174\n",
      "Epoch: 00 [ 9764/20299 ( 48%)], Train Loss: 0.55143\n",
      "Epoch: 00 [ 9804/20299 ( 48%)], Train Loss: 0.55076\n",
      "Epoch: 00 [ 9844/20299 ( 48%)], Train Loss: 0.54984\n",
      "Epoch: 00 [ 9884/20299 ( 49%)], Train Loss: 0.54949\n",
      "Epoch: 00 [ 9924/20299 ( 49%)], Train Loss: 0.54913\n",
      "Epoch: 00 [ 9964/20299 ( 49%)], Train Loss: 0.54828\n",
      "Epoch: 00 [10004/20299 ( 49%)], Train Loss: 0.54748\n",
      "Epoch: 00 [10044/20299 ( 49%)], Train Loss: 0.54666\n",
      "Epoch: 00 [10084/20299 ( 50%)], Train Loss: 0.54675\n",
      "Epoch: 00 [10124/20299 ( 50%)], Train Loss: 0.54565\n",
      "Epoch: 00 [10164/20299 ( 50%)], Train Loss: 0.54481\n",
      "Epoch: 00 [10204/20299 ( 50%)], Train Loss: 0.54416\n",
      "Epoch: 00 [10244/20299 ( 50%)], Train Loss: 0.54408\n",
      "Epoch: 00 [10284/20299 ( 51%)], Train Loss: 0.54326\n",
      "Epoch: 00 [10324/20299 ( 51%)], Train Loss: 0.54238\n",
      "Epoch: 00 [10364/20299 ( 51%)], Train Loss: 0.54135\n",
      "Epoch: 00 [10404/20299 ( 51%)], Train Loss: 0.53989\n",
      "Epoch: 00 [10444/20299 ( 51%)], Train Loss: 0.54015\n",
      "Epoch: 00 [10484/20299 ( 52%)], Train Loss: 0.53896\n",
      "Epoch: 00 [10524/20299 ( 52%)], Train Loss: 0.53874\n",
      "Epoch: 00 [10564/20299 ( 52%)], Train Loss: 0.53830\n",
      "Epoch: 00 [10604/20299 ( 52%)], Train Loss: 0.53801\n",
      "Epoch: 00 [10644/20299 ( 52%)], Train Loss: 0.53752\n",
      "Epoch: 00 [10684/20299 ( 53%)], Train Loss: 0.53679\n",
      "Epoch: 00 [10724/20299 ( 53%)], Train Loss: 0.53763\n",
      "Epoch: 00 [10764/20299 ( 53%)], Train Loss: 0.53707\n",
      "Epoch: 00 [10804/20299 ( 53%)], Train Loss: 0.53672\n",
      "Epoch: 00 [10844/20299 ( 53%)], Train Loss: 0.53646\n",
      "Epoch: 00 [10884/20299 ( 54%)], Train Loss: 0.53592\n",
      "Epoch: 00 [10924/20299 ( 54%)], Train Loss: 0.53446\n",
      "Epoch: 00 [10964/20299 ( 54%)], Train Loss: 0.53376\n",
      "Epoch: 00 [11004/20299 ( 54%)], Train Loss: 0.53328\n",
      "Epoch: 00 [11044/20299 ( 54%)], Train Loss: 0.53372\n",
      "Epoch: 00 [11084/20299 ( 55%)], Train Loss: 0.53286\n",
      "Epoch: 00 [11124/20299 ( 55%)], Train Loss: 0.53279\n",
      "Epoch: 00 [11164/20299 ( 55%)], Train Loss: 0.53278\n",
      "Epoch: 00 [11204/20299 ( 55%)], Train Loss: 0.53175\n",
      "Epoch: 00 [11244/20299 ( 55%)], Train Loss: 0.53080\n",
      "Epoch: 00 [11284/20299 ( 56%)], Train Loss: 0.53083\n",
      "Epoch: 00 [11324/20299 ( 56%)], Train Loss: 0.53091\n",
      "Epoch: 00 [11364/20299 ( 56%)], Train Loss: 0.52981\n",
      "Epoch: 00 [11404/20299 ( 56%)], Train Loss: 0.52898\n",
      "Epoch: 00 [11444/20299 ( 56%)], Train Loss: 0.52826\n",
      "Epoch: 00 [11484/20299 ( 57%)], Train Loss: 0.52838\n",
      "Epoch: 00 [11524/20299 ( 57%)], Train Loss: 0.52785\n",
      "Epoch: 00 [11564/20299 ( 57%)], Train Loss: 0.52690\n",
      "Epoch: 00 [11604/20299 ( 57%)], Train Loss: 0.52653\n",
      "Epoch: 00 [11644/20299 ( 57%)], Train Loss: 0.52551\n",
      "Epoch: 00 [11684/20299 ( 58%)], Train Loss: 0.52539\n",
      "Epoch: 00 [11724/20299 ( 58%)], Train Loss: 0.52470\n",
      "Epoch: 00 [11764/20299 ( 58%)], Train Loss: 0.52386\n",
      "Epoch: 00 [11804/20299 ( 58%)], Train Loss: 0.52368\n",
      "Epoch: 00 [11844/20299 ( 58%)], Train Loss: 0.52350\n",
      "Epoch: 00 [11884/20299 ( 59%)], Train Loss: 0.52309\n",
      "Epoch: 00 [11924/20299 ( 59%)], Train Loss: 0.52334\n",
      "Epoch: 00 [11964/20299 ( 59%)], Train Loss: 0.52301\n",
      "Epoch: 00 [12004/20299 ( 59%)], Train Loss: 0.52285\n",
      "Epoch: 00 [12044/20299 ( 59%)], Train Loss: 0.52261\n",
      "Epoch: 00 [12084/20299 ( 60%)], Train Loss: 0.52159\n",
      "Epoch: 00 [12124/20299 ( 60%)], Train Loss: 0.52108\n",
      "Epoch: 00 [12164/20299 ( 60%)], Train Loss: 0.52083\n",
      "Epoch: 00 [12204/20299 ( 60%)], Train Loss: 0.51992\n",
      "Epoch: 00 [12244/20299 ( 60%)], Train Loss: 0.51997\n",
      "Epoch: 00 [12284/20299 ( 61%)], Train Loss: 0.51950\n",
      "Epoch: 00 [12324/20299 ( 61%)], Train Loss: 0.51883\n",
      "Epoch: 00 [12364/20299 ( 61%)], Train Loss: 0.51819\n",
      "Epoch: 00 [12404/20299 ( 61%)], Train Loss: 0.51758\n",
      "Epoch: 00 [12444/20299 ( 61%)], Train Loss: 0.51674\n",
      "Epoch: 00 [12484/20299 ( 62%)], Train Loss: 0.51629\n",
      "Epoch: 00 [12524/20299 ( 62%)], Train Loss: 0.51605\n",
      "Epoch: 00 [12564/20299 ( 62%)], Train Loss: 0.51537\n",
      "Epoch: 00 [12604/20299 ( 62%)], Train Loss: 0.51530\n",
      "Epoch: 00 [12644/20299 ( 62%)], Train Loss: 0.51507\n",
      "Epoch: 00 [12684/20299 ( 62%)], Train Loss: 0.51445\n",
      "Epoch: 00 [12724/20299 ( 63%)], Train Loss: 0.51401\n",
      "Epoch: 00 [12764/20299 ( 63%)], Train Loss: 0.51343\n",
      "Epoch: 00 [12804/20299 ( 63%)], Train Loss: 0.51305\n",
      "Epoch: 00 [12844/20299 ( 63%)], Train Loss: 0.51271\n",
      "Epoch: 00 [12884/20299 ( 63%)], Train Loss: 0.51212\n",
      "Epoch: 00 [12924/20299 ( 64%)], Train Loss: 0.51209\n",
      "Epoch: 00 [12964/20299 ( 64%)], Train Loss: 0.51229\n",
      "Epoch: 00 [13004/20299 ( 64%)], Train Loss: 0.51166\n",
      "Epoch: 00 [13044/20299 ( 64%)], Train Loss: 0.51103\n",
      "Epoch: 00 [13084/20299 ( 64%)], Train Loss: 0.51036\n",
      "Epoch: 00 [13124/20299 ( 65%)], Train Loss: 0.50964\n",
      "Epoch: 00 [13164/20299 ( 65%)], Train Loss: 0.50897\n",
      "Epoch: 00 [13204/20299 ( 65%)], Train Loss: 0.50816\n",
      "Epoch: 00 [13244/20299 ( 65%)], Train Loss: 0.50848\n",
      "Epoch: 00 [13284/20299 ( 65%)], Train Loss: 0.50763\n",
      "Epoch: 00 [13324/20299 ( 66%)], Train Loss: 0.50739\n",
      "Epoch: 00 [13364/20299 ( 66%)], Train Loss: 0.50682\n",
      "Epoch: 00 [13404/20299 ( 66%)], Train Loss: 0.50647\n",
      "Epoch: 00 [13444/20299 ( 66%)], Train Loss: 0.50620\n",
      "Epoch: 00 [13484/20299 ( 66%)], Train Loss: 0.50547\n",
      "Epoch: 00 [13524/20299 ( 67%)], Train Loss: 0.50486\n",
      "Epoch: 00 [13564/20299 ( 67%)], Train Loss: 0.50461\n",
      "Epoch: 00 [13604/20299 ( 67%)], Train Loss: 0.50368\n",
      "Epoch: 00 [13644/20299 ( 67%)], Train Loss: 0.50298\n",
      "Epoch: 00 [13684/20299 ( 67%)], Train Loss: 0.50257\n",
      "Epoch: 00 [13724/20299 ( 68%)], Train Loss: 0.50175\n",
      "Epoch: 00 [13764/20299 ( 68%)], Train Loss: 0.50138\n",
      "Epoch: 00 [13804/20299 ( 68%)], Train Loss: 0.50050\n",
      "Epoch: 00 [13844/20299 ( 68%)], Train Loss: 0.50019\n",
      "Epoch: 00 [13884/20299 ( 68%)], Train Loss: 0.50036\n",
      "Epoch: 00 [13924/20299 ( 69%)], Train Loss: 0.50012\n",
      "Epoch: 00 [13964/20299 ( 69%)], Train Loss: 0.49987\n",
      "Epoch: 00 [14004/20299 ( 69%)], Train Loss: 0.49940\n",
      "Epoch: 00 [14044/20299 ( 69%)], Train Loss: 0.49914\n",
      "Epoch: 00 [14084/20299 ( 69%)], Train Loss: 0.49839\n",
      "Epoch: 00 [14124/20299 ( 70%)], Train Loss: 0.49817\n",
      "Epoch: 00 [14164/20299 ( 70%)], Train Loss: 0.49752\n",
      "Epoch: 00 [14204/20299 ( 70%)], Train Loss: 0.49732\n",
      "Epoch: 00 [14244/20299 ( 70%)], Train Loss: 0.49723\n",
      "Epoch: 00 [14284/20299 ( 70%)], Train Loss: 0.49645\n",
      "Epoch: 00 [14324/20299 ( 71%)], Train Loss: 0.49675\n",
      "Epoch: 00 [14364/20299 ( 71%)], Train Loss: 0.49643\n",
      "Epoch: 00 [14404/20299 ( 71%)], Train Loss: 0.49596\n",
      "Epoch: 00 [14444/20299 ( 71%)], Train Loss: 0.49605\n",
      "Epoch: 00 [14484/20299 ( 71%)], Train Loss: 0.49555\n",
      "Epoch: 00 [14524/20299 ( 72%)], Train Loss: 0.49476\n",
      "Epoch: 00 [14564/20299 ( 72%)], Train Loss: 0.49482\n",
      "Epoch: 00 [14604/20299 ( 72%)], Train Loss: 0.49450\n",
      "Epoch: 00 [14644/20299 ( 72%)], Train Loss: 0.49386\n",
      "Epoch: 00 [14684/20299 ( 72%)], Train Loss: 0.49394\n",
      "Epoch: 00 [14724/20299 ( 73%)], Train Loss: 0.49337\n",
      "Epoch: 00 [14764/20299 ( 73%)], Train Loss: 0.49291\n",
      "Epoch: 00 [14804/20299 ( 73%)], Train Loss: 0.49235\n",
      "Epoch: 00 [14844/20299 ( 73%)], Train Loss: 0.49240\n",
      "Epoch: 00 [14884/20299 ( 73%)], Train Loss: 0.49156\n",
      "Epoch: 00 [14924/20299 ( 74%)], Train Loss: 0.49090\n",
      "Epoch: 00 [14964/20299 ( 74%)], Train Loss: 0.49036\n",
      "Epoch: 00 [15004/20299 ( 74%)], Train Loss: 0.48968\n",
      "Epoch: 00 [15044/20299 ( 74%)], Train Loss: 0.48935\n",
      "Epoch: 00 [15084/20299 ( 74%)], Train Loss: 0.48895\n",
      "Epoch: 00 [15124/20299 ( 75%)], Train Loss: 0.48835\n",
      "Epoch: 00 [15164/20299 ( 75%)], Train Loss: 0.48798\n",
      "Epoch: 00 [15204/20299 ( 75%)], Train Loss: 0.48769\n",
      "Epoch: 00 [15244/20299 ( 75%)], Train Loss: 0.48747\n",
      "Epoch: 00 [15284/20299 ( 75%)], Train Loss: 0.48712\n",
      "Epoch: 00 [15324/20299 ( 75%)], Train Loss: 0.48682\n",
      "Epoch: 00 [15364/20299 ( 76%)], Train Loss: 0.48644\n",
      "Epoch: 00 [15404/20299 ( 76%)], Train Loss: 0.48585\n",
      "Epoch: 00 [15444/20299 ( 76%)], Train Loss: 0.48561\n",
      "Epoch: 00 [15484/20299 ( 76%)], Train Loss: 0.48500\n",
      "Epoch: 00 [15524/20299 ( 76%)], Train Loss: 0.48490\n",
      "Epoch: 00 [15564/20299 ( 77%)], Train Loss: 0.48473\n",
      "Epoch: 00 [15604/20299 ( 77%)], Train Loss: 0.48451\n",
      "Epoch: 00 [15644/20299 ( 77%)], Train Loss: 0.48444\n",
      "Epoch: 00 [15684/20299 ( 77%)], Train Loss: 0.48441\n",
      "Epoch: 00 [15724/20299 ( 77%)], Train Loss: 0.48403\n",
      "Epoch: 00 [15764/20299 ( 78%)], Train Loss: 0.48358\n",
      "Epoch: 00 [15804/20299 ( 78%)], Train Loss: 0.48358\n",
      "Epoch: 00 [15844/20299 ( 78%)], Train Loss: 0.48301\n",
      "Epoch: 00 [15884/20299 ( 78%)], Train Loss: 0.48292\n",
      "Epoch: 00 [15924/20299 ( 78%)], Train Loss: 0.48261\n",
      "Epoch: 00 [15964/20299 ( 79%)], Train Loss: 0.48234\n",
      "Epoch: 00 [16004/20299 ( 79%)], Train Loss: 0.48223\n",
      "Epoch: 00 [16044/20299 ( 79%)], Train Loss: 0.48183\n",
      "Epoch: 00 [16084/20299 ( 79%)], Train Loss: 0.48107\n",
      "Epoch: 00 [16124/20299 ( 79%)], Train Loss: 0.48067\n",
      "Epoch: 00 [16164/20299 ( 80%)], Train Loss: 0.48058\n",
      "Epoch: 00 [16204/20299 ( 80%)], Train Loss: 0.48009\n",
      "Epoch: 00 [16244/20299 ( 80%)], Train Loss: 0.48009\n",
      "Epoch: 00 [16284/20299 ( 80%)], Train Loss: 0.47989\n",
      "Epoch: 00 [16324/20299 ( 80%)], Train Loss: 0.47985\n",
      "Epoch: 00 [16364/20299 ( 81%)], Train Loss: 0.47973\n",
      "Epoch: 00 [16404/20299 ( 81%)], Train Loss: 0.47928\n",
      "Epoch: 00 [16444/20299 ( 81%)], Train Loss: 0.47865\n",
      "Epoch: 00 [16484/20299 ( 81%)], Train Loss: 0.47809\n",
      "Epoch: 00 [16524/20299 ( 81%)], Train Loss: 0.47863\n",
      "Epoch: 00 [16564/20299 ( 82%)], Train Loss: 0.47892\n",
      "Epoch: 00 [16604/20299 ( 82%)], Train Loss: 0.47865\n",
      "Epoch: 00 [16644/20299 ( 82%)], Train Loss: 0.47841\n",
      "Epoch: 00 [16684/20299 ( 82%)], Train Loss: 0.47824\n",
      "Epoch: 00 [16724/20299 ( 82%)], Train Loss: 0.47783\n",
      "Epoch: 00 [16764/20299 ( 83%)], Train Loss: 0.47763\n",
      "Epoch: 00 [16804/20299 ( 83%)], Train Loss: 0.47735\n",
      "Epoch: 00 [16844/20299 ( 83%)], Train Loss: 0.47654\n",
      "Epoch: 00 [16884/20299 ( 83%)], Train Loss: 0.47621\n",
      "Epoch: 00 [16924/20299 ( 83%)], Train Loss: 0.47563\n",
      "Epoch: 00 [16964/20299 ( 84%)], Train Loss: 0.47510\n",
      "Epoch: 00 [17004/20299 ( 84%)], Train Loss: 0.47490\n",
      "Epoch: 00 [17044/20299 ( 84%)], Train Loss: 0.47472\n",
      "Epoch: 00 [17084/20299 ( 84%)], Train Loss: 0.47458\n",
      "Epoch: 00 [17124/20299 ( 84%)], Train Loss: 0.47401\n",
      "Epoch: 00 [17164/20299 ( 85%)], Train Loss: 0.47403\n",
      "Epoch: 00 [17204/20299 ( 85%)], Train Loss: 0.47401\n",
      "Epoch: 00 [17244/20299 ( 85%)], Train Loss: 0.47355\n",
      "Epoch: 00 [17284/20299 ( 85%)], Train Loss: 0.47288\n",
      "Epoch: 00 [17324/20299 ( 85%)], Train Loss: 0.47256\n",
      "Epoch: 00 [17364/20299 ( 86%)], Train Loss: 0.47190\n",
      "Epoch: 00 [17404/20299 ( 86%)], Train Loss: 0.47166\n",
      "Epoch: 00 [17444/20299 ( 86%)], Train Loss: 0.47164\n",
      "Epoch: 00 [17484/20299 ( 86%)], Train Loss: 0.47127\n",
      "Epoch: 00 [17524/20299 ( 86%)], Train Loss: 0.47133\n",
      "Epoch: 00 [17564/20299 ( 87%)], Train Loss: 0.47082\n",
      "Epoch: 00 [17604/20299 ( 87%)], Train Loss: 0.47029\n",
      "Epoch: 00 [17644/20299 ( 87%)], Train Loss: 0.47083\n",
      "Epoch: 00 [17684/20299 ( 87%)], Train Loss: 0.47033\n",
      "Epoch: 00 [17724/20299 ( 87%)], Train Loss: 0.47030\n",
      "Epoch: 00 [17764/20299 ( 88%)], Train Loss: 0.47006\n",
      "Epoch: 00 [17804/20299 ( 88%)], Train Loss: 0.46991\n",
      "Epoch: 00 [17844/20299 ( 88%)], Train Loss: 0.46933\n",
      "Epoch: 00 [17884/20299 ( 88%)], Train Loss: 0.46880\n",
      "Epoch: 00 [17924/20299 ( 88%)], Train Loss: 0.46834\n",
      "Epoch: 00 [17964/20299 ( 88%)], Train Loss: 0.46788\n",
      "Epoch: 00 [18004/20299 ( 89%)], Train Loss: 0.46849\n",
      "Epoch: 00 [18044/20299 ( 89%)], Train Loss: 0.46794\n",
      "Epoch: 00 [18084/20299 ( 89%)], Train Loss: 0.46768\n",
      "Epoch: 00 [18124/20299 ( 89%)], Train Loss: 0.46755\n",
      "Epoch: 00 [18164/20299 ( 89%)], Train Loss: 0.46749\n",
      "Epoch: 00 [18204/20299 ( 90%)], Train Loss: 0.46720\n",
      "Epoch: 00 [18244/20299 ( 90%)], Train Loss: 0.46716\n",
      "Epoch: 00 [18284/20299 ( 90%)], Train Loss: 0.46655\n",
      "Epoch: 00 [18324/20299 ( 90%)], Train Loss: 0.46617\n",
      "Epoch: 00 [18364/20299 ( 90%)], Train Loss: 0.46583\n",
      "Epoch: 00 [18404/20299 ( 91%)], Train Loss: 0.46542\n",
      "Epoch: 00 [18444/20299 ( 91%)], Train Loss: 0.46520\n",
      "Epoch: 00 [18484/20299 ( 91%)], Train Loss: 0.46527\n",
      "Epoch: 00 [18524/20299 ( 91%)], Train Loss: 0.46493\n",
      "Epoch: 00 [18564/20299 ( 91%)], Train Loss: 0.46457\n",
      "Epoch: 00 [18604/20299 ( 92%)], Train Loss: 0.46399\n",
      "Epoch: 00 [18644/20299 ( 92%)], Train Loss: 0.46389\n",
      "Epoch: 00 [18684/20299 ( 92%)], Train Loss: 0.46358\n",
      "Epoch: 00 [18724/20299 ( 92%)], Train Loss: 0.46324\n",
      "Epoch: 00 [18764/20299 ( 92%)], Train Loss: 0.46271\n",
      "Epoch: 00 [18804/20299 ( 93%)], Train Loss: 0.46329\n",
      "Epoch: 00 [18844/20299 ( 93%)], Train Loss: 0.46324\n",
      "Epoch: 00 [18884/20299 ( 93%)], Train Loss: 0.46275\n",
      "Epoch: 00 [18924/20299 ( 93%)], Train Loss: 0.46246\n",
      "Epoch: 00 [18964/20299 ( 93%)], Train Loss: 0.46215\n",
      "Epoch: 00 [19004/20299 ( 94%)], Train Loss: 0.46193\n",
      "Epoch: 00 [19044/20299 ( 94%)], Train Loss: 0.46175\n",
      "Epoch: 00 [19084/20299 ( 94%)], Train Loss: 0.46169\n",
      "Epoch: 00 [19124/20299 ( 94%)], Train Loss: 0.46137\n",
      "Epoch: 00 [19164/20299 ( 94%)], Train Loss: 0.46112\n",
      "Epoch: 00 [19204/20299 ( 95%)], Train Loss: 0.46065\n",
      "Epoch: 00 [19244/20299 ( 95%)], Train Loss: 0.46055\n",
      "Epoch: 00 [19284/20299 ( 95%)], Train Loss: 0.46029\n",
      "Epoch: 00 [19324/20299 ( 95%)], Train Loss: 0.45996\n",
      "Epoch: 00 [19364/20299 ( 95%)], Train Loss: 0.45962\n",
      "Epoch: 00 [19404/20299 ( 96%)], Train Loss: 0.45932\n",
      "Epoch: 00 [19444/20299 ( 96%)], Train Loss: 0.45906\n",
      "Epoch: 00 [19484/20299 ( 96%)], Train Loss: 0.45859\n",
      "Epoch: 00 [19524/20299 ( 96%)], Train Loss: 0.45839\n",
      "Epoch: 00 [19564/20299 ( 96%)], Train Loss: 0.45773\n",
      "Epoch: 00 [19604/20299 ( 97%)], Train Loss: 0.45789\n",
      "Epoch: 00 [19644/20299 ( 97%)], Train Loss: 0.45763\n",
      "Epoch: 00 [19684/20299 ( 97%)], Train Loss: 0.45759\n",
      "Epoch: 00 [19724/20299 ( 97%)], Train Loss: 0.45718\n",
      "Epoch: 00 [19764/20299 ( 97%)], Train Loss: 0.45671\n",
      "Epoch: 00 [19804/20299 ( 98%)], Train Loss: 0.45652\n",
      "Epoch: 00 [19844/20299 ( 98%)], Train Loss: 0.45617\n",
      "Epoch: 00 [19884/20299 ( 98%)], Train Loss: 0.45626\n",
      "Epoch: 00 [19924/20299 ( 98%)], Train Loss: 0.45652\n",
      "Epoch: 00 [19964/20299 ( 98%)], Train Loss: 0.45601\n",
      "Epoch: 00 [20004/20299 ( 99%)], Train Loss: 0.45579\n",
      "Epoch: 00 [20044/20299 ( 99%)], Train Loss: 0.45565\n",
      "Epoch: 00 [20084/20299 ( 99%)], Train Loss: 0.45554\n",
      "Epoch: 00 [20124/20299 ( 99%)], Train Loss: 0.45547\n",
      "Epoch: 00 [20164/20299 ( 99%)], Train Loss: 0.45508\n",
      "Epoch: 00 [20204/20299 (100%)], Train Loss: 0.45482\n",
      "Epoch: 00 [20244/20299 (100%)], Train Loss: 0.45417\n",
      "Epoch: 00 [20284/20299 (100%)], Train Loss: 0.45404\n",
      "Epoch: 00 [20299/20299 (100%)], Train Loss: 0.45385\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.24408\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.24408\n",
      "Saving model checkpoint to output/checkpoint-fold-2.\n",
      "\n",
      "Epoch: 01 [    4/20299 (  0%)], Train Loss: 0.65599\n",
      "Epoch: 01 [   44/20299 (  0%)], Train Loss: 0.19850\n",
      "Epoch: 01 [   84/20299 (  0%)], Train Loss: 0.29967\n",
      "Epoch: 01 [  124/20299 (  1%)], Train Loss: 0.27471\n",
      "Epoch: 01 [  164/20299 (  1%)], Train Loss: 0.28342\n",
      "Epoch: 01 [  204/20299 (  1%)], Train Loss: 0.30015\n",
      "Epoch: 01 [  244/20299 (  1%)], Train Loss: 0.33620\n",
      "Epoch: 01 [  284/20299 (  1%)], Train Loss: 0.33439\n",
      "Epoch: 01 [  324/20299 (  2%)], Train Loss: 0.34793\n",
      "Epoch: 01 [  364/20299 (  2%)], Train Loss: 0.34470\n",
      "Epoch: 01 [  404/20299 (  2%)], Train Loss: 0.34548\n",
      "Epoch: 01 [  444/20299 (  2%)], Train Loss: 0.33941\n",
      "Epoch: 01 [  484/20299 (  2%)], Train Loss: 0.33270\n",
      "Epoch: 01 [  524/20299 (  3%)], Train Loss: 0.32199\n",
      "Epoch: 01 [  564/20299 (  3%)], Train Loss: 0.33893\n",
      "Epoch: 01 [  604/20299 (  3%)], Train Loss: 0.32806\n",
      "Epoch: 01 [  644/20299 (  3%)], Train Loss: 0.31880\n",
      "Epoch: 01 [  684/20299 (  3%)], Train Loss: 0.31304\n",
      "Epoch: 01 [  724/20299 (  4%)], Train Loss: 0.30638\n",
      "Epoch: 01 [  764/20299 (  4%)], Train Loss: 0.30783\n",
      "Epoch: 01 [  804/20299 (  4%)], Train Loss: 0.30739\n",
      "Epoch: 01 [  844/20299 (  4%)], Train Loss: 0.30469\n",
      "Epoch: 01 [  884/20299 (  4%)], Train Loss: 0.30296\n",
      "Epoch: 01 [  924/20299 (  5%)], Train Loss: 0.30141\n",
      "Epoch: 01 [  964/20299 (  5%)], Train Loss: 0.30005\n",
      "Epoch: 01 [ 1004/20299 (  5%)], Train Loss: 0.29553\n",
      "Epoch: 01 [ 1044/20299 (  5%)], Train Loss: 0.30480\n",
      "Epoch: 01 [ 1084/20299 (  5%)], Train Loss: 0.29997\n",
      "Epoch: 01 [ 1124/20299 (  6%)], Train Loss: 0.29854\n",
      "Epoch: 01 [ 1164/20299 (  6%)], Train Loss: 0.29706\n",
      "Epoch: 01 [ 1204/20299 (  6%)], Train Loss: 0.29390\n",
      "Epoch: 01 [ 1244/20299 (  6%)], Train Loss: 0.29844\n",
      "Epoch: 01 [ 1284/20299 (  6%)], Train Loss: 0.29575\n",
      "Epoch: 01 [ 1324/20299 (  7%)], Train Loss: 0.29606\n",
      "Epoch: 01 [ 1364/20299 (  7%)], Train Loss: 0.29732\n",
      "Epoch: 01 [ 1404/20299 (  7%)], Train Loss: 0.29341\n",
      "Epoch: 01 [ 1444/20299 (  7%)], Train Loss: 0.29245\n",
      "Epoch: 01 [ 1484/20299 (  7%)], Train Loss: 0.29089\n",
      "Epoch: 01 [ 1524/20299 (  8%)], Train Loss: 0.28805\n",
      "Epoch: 01 [ 1564/20299 (  8%)], Train Loss: 0.28806\n",
      "Epoch: 01 [ 1604/20299 (  8%)], Train Loss: 0.28517\n",
      "Epoch: 01 [ 1644/20299 (  8%)], Train Loss: 0.28273\n",
      "Epoch: 01 [ 1684/20299 (  8%)], Train Loss: 0.28163\n",
      "Epoch: 01 [ 1724/20299 (  8%)], Train Loss: 0.28129\n",
      "Epoch: 01 [ 1764/20299 (  9%)], Train Loss: 0.28259\n",
      "Epoch: 01 [ 1804/20299 (  9%)], Train Loss: 0.28192\n",
      "Epoch: 01 [ 1844/20299 (  9%)], Train Loss: 0.27759\n",
      "Epoch: 01 [ 1884/20299 (  9%)], Train Loss: 0.27829\n",
      "Epoch: 01 [ 1924/20299 (  9%)], Train Loss: 0.27724\n",
      "Epoch: 01 [ 1964/20299 ( 10%)], Train Loss: 0.27580\n",
      "Epoch: 01 [ 2004/20299 ( 10%)], Train Loss: 0.27489\n",
      "Epoch: 01 [ 2044/20299 ( 10%)], Train Loss: 0.27349\n",
      "Epoch: 01 [ 2084/20299 ( 10%)], Train Loss: 0.27116\n",
      "Epoch: 01 [ 2124/20299 ( 10%)], Train Loss: 0.26969\n",
      "Epoch: 01 [ 2164/20299 ( 11%)], Train Loss: 0.26800\n",
      "Epoch: 01 [ 2204/20299 ( 11%)], Train Loss: 0.26593\n",
      "Epoch: 01 [ 2244/20299 ( 11%)], Train Loss: 0.26399\n",
      "Epoch: 01 [ 2284/20299 ( 11%)], Train Loss: 0.26131\n",
      "Epoch: 01 [ 2324/20299 ( 11%)], Train Loss: 0.25962\n",
      "Epoch: 01 [ 2364/20299 ( 12%)], Train Loss: 0.25864\n",
      "Epoch: 01 [ 2404/20299 ( 12%)], Train Loss: 0.25874\n",
      "Epoch: 01 [ 2444/20299 ( 12%)], Train Loss: 0.25734\n",
      "Epoch: 01 [ 2484/20299 ( 12%)], Train Loss: 0.25643\n",
      "Epoch: 01 [ 2524/20299 ( 12%)], Train Loss: 0.25493\n",
      "Epoch: 01 [ 2564/20299 ( 13%)], Train Loss: 0.25475\n",
      "Epoch: 01 [ 2604/20299 ( 13%)], Train Loss: 0.25181\n",
      "Epoch: 01 [ 2644/20299 ( 13%)], Train Loss: 0.25208\n",
      "Epoch: 01 [ 2684/20299 ( 13%)], Train Loss: 0.25020\n",
      "Epoch: 01 [ 2724/20299 ( 13%)], Train Loss: 0.25113\n",
      "Epoch: 01 [ 2764/20299 ( 14%)], Train Loss: 0.24860\n",
      "Epoch: 01 [ 2804/20299 ( 14%)], Train Loss: 0.24666\n",
      "Epoch: 01 [ 2844/20299 ( 14%)], Train Loss: 0.24526\n",
      "Epoch: 01 [ 2884/20299 ( 14%)], Train Loss: 0.24412\n",
      "Epoch: 01 [ 2924/20299 ( 14%)], Train Loss: 0.24365\n",
      "Epoch: 01 [ 2964/20299 ( 15%)], Train Loss: 0.24316\n",
      "Epoch: 01 [ 3004/20299 ( 15%)], Train Loss: 0.24111\n",
      "Epoch: 01 [ 3044/20299 ( 15%)], Train Loss: 0.23931\n",
      "Epoch: 01 [ 3084/20299 ( 15%)], Train Loss: 0.23890\n",
      "Epoch: 01 [ 3124/20299 ( 15%)], Train Loss: 0.23631\n",
      "Epoch: 01 [ 3164/20299 ( 16%)], Train Loss: 0.23656\n",
      "Epoch: 01 [ 3204/20299 ( 16%)], Train Loss: 0.23616\n",
      "Epoch: 01 [ 3244/20299 ( 16%)], Train Loss: 0.23482\n",
      "Epoch: 01 [ 3284/20299 ( 16%)], Train Loss: 0.23346\n",
      "Epoch: 01 [ 3324/20299 ( 16%)], Train Loss: 0.23245\n",
      "Epoch: 01 [ 3364/20299 ( 17%)], Train Loss: 0.23206\n",
      "Epoch: 01 [ 3404/20299 ( 17%)], Train Loss: 0.23066\n",
      "Epoch: 01 [ 3444/20299 ( 17%)], Train Loss: 0.22995\n",
      "Epoch: 01 [ 3484/20299 ( 17%)], Train Loss: 0.22824\n",
      "Epoch: 01 [ 3524/20299 ( 17%)], Train Loss: 0.22697\n",
      "Epoch: 01 [ 3564/20299 ( 18%)], Train Loss: 0.22582\n",
      "Epoch: 01 [ 3604/20299 ( 18%)], Train Loss: 0.22569\n",
      "Epoch: 01 [ 3644/20299 ( 18%)], Train Loss: 0.22644\n",
      "Epoch: 01 [ 3684/20299 ( 18%)], Train Loss: 0.22559\n",
      "Epoch: 01 [ 3724/20299 ( 18%)], Train Loss: 0.22483\n",
      "Epoch: 01 [ 3764/20299 ( 19%)], Train Loss: 0.22556\n",
      "Epoch: 01 [ 3804/20299 ( 19%)], Train Loss: 0.22694\n",
      "Epoch: 01 [ 3844/20299 ( 19%)], Train Loss: 0.22606\n",
      "Epoch: 01 [ 3884/20299 ( 19%)], Train Loss: 0.22548\n",
      "Epoch: 01 [ 3924/20299 ( 19%)], Train Loss: 0.22415\n",
      "Epoch: 01 [ 3964/20299 ( 20%)], Train Loss: 0.22442\n",
      "Epoch: 01 [ 4004/20299 ( 20%)], Train Loss: 0.22376\n",
      "Epoch: 01 [ 4044/20299 ( 20%)], Train Loss: 0.22252\n",
      "Epoch: 01 [ 4084/20299 ( 20%)], Train Loss: 0.22219\n",
      "Epoch: 01 [ 4124/20299 ( 20%)], Train Loss: 0.22054\n",
      "Epoch: 01 [ 4164/20299 ( 21%)], Train Loss: 0.21998\n",
      "Epoch: 01 [ 4204/20299 ( 21%)], Train Loss: 0.21849\n",
      "Epoch: 01 [ 4244/20299 ( 21%)], Train Loss: 0.21800\n",
      "Epoch: 01 [ 4284/20299 ( 21%)], Train Loss: 0.21689\n",
      "Epoch: 01 [ 4324/20299 ( 21%)], Train Loss: 0.21641\n",
      "Epoch: 01 [ 4364/20299 ( 21%)], Train Loss: 0.21575\n",
      "Epoch: 01 [ 4404/20299 ( 22%)], Train Loss: 0.21552\n",
      "Epoch: 01 [ 4444/20299 ( 22%)], Train Loss: 0.21525\n",
      "Epoch: 01 [ 4484/20299 ( 22%)], Train Loss: 0.21433\n",
      "Epoch: 01 [ 4524/20299 ( 22%)], Train Loss: 0.21345\n",
      "Epoch: 01 [ 4564/20299 ( 22%)], Train Loss: 0.21332\n",
      "Epoch: 01 [ 4604/20299 ( 23%)], Train Loss: 0.21268\n",
      "Epoch: 01 [ 4644/20299 ( 23%)], Train Loss: 0.21224\n",
      "Epoch: 01 [ 4684/20299 ( 23%)], Train Loss: 0.21249\n",
      "Epoch: 01 [ 4724/20299 ( 23%)], Train Loss: 0.21293\n",
      "Epoch: 01 [ 4764/20299 ( 23%)], Train Loss: 0.21320\n",
      "Epoch: 01 [ 4804/20299 ( 24%)], Train Loss: 0.21409\n",
      "Epoch: 01 [ 4844/20299 ( 24%)], Train Loss: 0.21330\n",
      "Epoch: 01 [ 4884/20299 ( 24%)], Train Loss: 0.21291\n",
      "Epoch: 01 [ 4924/20299 ( 24%)], Train Loss: 0.21406\n",
      "Epoch: 01 [ 4964/20299 ( 24%)], Train Loss: 0.21340\n",
      "Epoch: 01 [ 5004/20299 ( 25%)], Train Loss: 0.21348\n",
      "Epoch: 01 [ 5044/20299 ( 25%)], Train Loss: 0.21253\n",
      "Epoch: 01 [ 5084/20299 ( 25%)], Train Loss: 0.21172\n",
      "Epoch: 01 [ 5124/20299 ( 25%)], Train Loss: 0.21085\n",
      "Epoch: 01 [ 5164/20299 ( 25%)], Train Loss: 0.21015\n",
      "Epoch: 01 [ 5204/20299 ( 26%)], Train Loss: 0.20960\n",
      "Epoch: 01 [ 5244/20299 ( 26%)], Train Loss: 0.20851\n",
      "Epoch: 01 [ 5284/20299 ( 26%)], Train Loss: 0.20778\n",
      "Epoch: 01 [ 5324/20299 ( 26%)], Train Loss: 0.20717\n",
      "Epoch: 01 [ 5364/20299 ( 26%)], Train Loss: 0.20625\n",
      "Epoch: 01 [ 5404/20299 ( 27%)], Train Loss: 0.20617\n",
      "Epoch: 01 [ 5444/20299 ( 27%)], Train Loss: 0.20526\n",
      "Epoch: 01 [ 5484/20299 ( 27%)], Train Loss: 0.20454\n",
      "Epoch: 01 [ 5524/20299 ( 27%)], Train Loss: 0.20360\n",
      "Epoch: 01 [ 5564/20299 ( 27%)], Train Loss: 0.20257\n",
      "Epoch: 01 [ 5604/20299 ( 28%)], Train Loss: 0.20172\n",
      "Epoch: 01 [ 5644/20299 ( 28%)], Train Loss: 0.20146\n",
      "Epoch: 01 [ 5684/20299 ( 28%)], Train Loss: 0.20197\n",
      "Epoch: 01 [ 5724/20299 ( 28%)], Train Loss: 0.20173\n",
      "Epoch: 01 [ 5764/20299 ( 28%)], Train Loss: 0.20103\n",
      "Epoch: 01 [ 5804/20299 ( 29%)], Train Loss: 0.20102\n",
      "Epoch: 01 [ 5844/20299 ( 29%)], Train Loss: 0.20138\n",
      "Epoch: 01 [ 5884/20299 ( 29%)], Train Loss: 0.20107\n",
      "Epoch: 01 [ 5924/20299 ( 29%)], Train Loss: 0.20118\n",
      "Epoch: 01 [ 5964/20299 ( 29%)], Train Loss: 0.20125\n",
      "Epoch: 01 [ 6004/20299 ( 30%)], Train Loss: 0.20119\n",
      "Epoch: 01 [ 6044/20299 ( 30%)], Train Loss: 0.20089\n",
      "Epoch: 01 [ 6084/20299 ( 30%)], Train Loss: 0.20044\n",
      "Epoch: 01 [ 6124/20299 ( 30%)], Train Loss: 0.19970\n",
      "Epoch: 01 [ 6164/20299 ( 30%)], Train Loss: 0.19905\n",
      "Epoch: 01 [ 6204/20299 ( 31%)], Train Loss: 0.19854\n",
      "Epoch: 01 [ 6244/20299 ( 31%)], Train Loss: 0.19846\n",
      "Epoch: 01 [ 6284/20299 ( 31%)], Train Loss: 0.19899\n",
      "Epoch: 01 [ 6324/20299 ( 31%)], Train Loss: 0.20001\n",
      "Epoch: 01 [ 6364/20299 ( 31%)], Train Loss: 0.19946\n",
      "Epoch: 01 [ 6404/20299 ( 32%)], Train Loss: 0.19954\n",
      "Epoch: 01 [ 6444/20299 ( 32%)], Train Loss: 0.19908\n",
      "Epoch: 01 [ 6484/20299 ( 32%)], Train Loss: 0.19882\n",
      "Epoch: 01 [ 6524/20299 ( 32%)], Train Loss: 0.19834\n",
      "Epoch: 01 [ 6564/20299 ( 32%)], Train Loss: 0.19817\n",
      "Epoch: 01 [ 6604/20299 ( 33%)], Train Loss: 0.19786\n",
      "Epoch: 01 [ 6644/20299 ( 33%)], Train Loss: 0.19738\n",
      "Epoch: 01 [ 6684/20299 ( 33%)], Train Loss: 0.19696\n",
      "Epoch: 01 [ 6724/20299 ( 33%)], Train Loss: 0.19665\n",
      "Epoch: 01 [ 6764/20299 ( 33%)], Train Loss: 0.19667\n",
      "Epoch: 01 [ 6804/20299 ( 34%)], Train Loss: 0.19688\n",
      "Epoch: 01 [ 6844/20299 ( 34%)], Train Loss: 0.19598\n",
      "Epoch: 01 [ 6884/20299 ( 34%)], Train Loss: 0.19533\n",
      "Epoch: 01 [ 6924/20299 ( 34%)], Train Loss: 0.19480\n",
      "Epoch: 01 [ 6964/20299 ( 34%)], Train Loss: 0.19421\n",
      "Epoch: 01 [ 7004/20299 ( 35%)], Train Loss: 0.19341\n",
      "Epoch: 01 [ 7044/20299 ( 35%)], Train Loss: 0.19334\n",
      "Epoch: 01 [ 7084/20299 ( 35%)], Train Loss: 0.19310\n",
      "Epoch: 01 [ 7124/20299 ( 35%)], Train Loss: 0.19212\n",
      "Epoch: 01 [ 7164/20299 ( 35%)], Train Loss: 0.19153\n",
      "Epoch: 01 [ 7204/20299 ( 35%)], Train Loss: 0.19114\n",
      "Epoch: 01 [ 7244/20299 ( 36%)], Train Loss: 0.19057\n",
      "Epoch: 01 [ 7284/20299 ( 36%)], Train Loss: 0.19003\n",
      "Epoch: 01 [ 7324/20299 ( 36%)], Train Loss: 0.18935\n",
      "Epoch: 01 [ 7364/20299 ( 36%)], Train Loss: 0.18901\n",
      "Epoch: 01 [ 7404/20299 ( 36%)], Train Loss: 0.18858\n",
      "Epoch: 01 [ 7444/20299 ( 37%)], Train Loss: 0.18849\n",
      "Epoch: 01 [ 7484/20299 ( 37%)], Train Loss: 0.18771\n",
      "Epoch: 01 [ 7524/20299 ( 37%)], Train Loss: 0.18794\n",
      "Epoch: 01 [ 7564/20299 ( 37%)], Train Loss: 0.18775\n",
      "Epoch: 01 [ 7604/20299 ( 37%)], Train Loss: 0.18845\n",
      "Epoch: 01 [ 7644/20299 ( 38%)], Train Loss: 0.18830\n",
      "Epoch: 01 [ 7684/20299 ( 38%)], Train Loss: 0.18851\n",
      "Epoch: 01 [ 7724/20299 ( 38%)], Train Loss: 0.18858\n",
      "Epoch: 01 [ 7764/20299 ( 38%)], Train Loss: 0.18812\n",
      "Epoch: 01 [ 7804/20299 ( 38%)], Train Loss: 0.18831\n",
      "Epoch: 01 [ 7844/20299 ( 39%)], Train Loss: 0.18783\n",
      "Epoch: 01 [ 7884/20299 ( 39%)], Train Loss: 0.18755\n",
      "Epoch: 01 [ 7924/20299 ( 39%)], Train Loss: 0.18754\n",
      "Epoch: 01 [ 7964/20299 ( 39%)], Train Loss: 0.18701\n",
      "Epoch: 01 [ 8004/20299 ( 39%)], Train Loss: 0.18668\n",
      "Epoch: 01 [ 8044/20299 ( 40%)], Train Loss: 0.18671\n",
      "Epoch: 01 [ 8084/20299 ( 40%)], Train Loss: 0.18712\n",
      "Epoch: 01 [ 8124/20299 ( 40%)], Train Loss: 0.18632\n",
      "Epoch: 01 [ 8164/20299 ( 40%)], Train Loss: 0.18606\n",
      "Epoch: 01 [ 8204/20299 ( 40%)], Train Loss: 0.18595\n",
      "Epoch: 01 [ 8244/20299 ( 41%)], Train Loss: 0.18539\n",
      "Epoch: 01 [ 8284/20299 ( 41%)], Train Loss: 0.18536\n",
      "Epoch: 01 [ 8324/20299 ( 41%)], Train Loss: 0.18517\n",
      "Epoch: 01 [ 8364/20299 ( 41%)], Train Loss: 0.18498\n",
      "Epoch: 01 [ 8404/20299 ( 41%)], Train Loss: 0.18452\n",
      "Epoch: 01 [ 8444/20299 ( 42%)], Train Loss: 0.18410\n",
      "Epoch: 01 [ 8484/20299 ( 42%)], Train Loss: 0.18427\n",
      "Epoch: 01 [ 8524/20299 ( 42%)], Train Loss: 0.18383\n",
      "Epoch: 01 [ 8564/20299 ( 42%)], Train Loss: 0.18324\n",
      "Epoch: 01 [ 8604/20299 ( 42%)], Train Loss: 0.18295\n",
      "Epoch: 01 [ 8644/20299 ( 43%)], Train Loss: 0.18288\n",
      "Epoch: 01 [ 8684/20299 ( 43%)], Train Loss: 0.18245\n",
      "Epoch: 01 [ 8724/20299 ( 43%)], Train Loss: 0.18275\n",
      "Epoch: 01 [ 8764/20299 ( 43%)], Train Loss: 0.18260\n",
      "Epoch: 01 [ 8804/20299 ( 43%)], Train Loss: 0.18243\n",
      "Epoch: 01 [ 8844/20299 ( 44%)], Train Loss: 0.18178\n",
      "Epoch: 01 [ 8884/20299 ( 44%)], Train Loss: 0.18231\n",
      "Epoch: 01 [ 8924/20299 ( 44%)], Train Loss: 0.18207\n",
      "Epoch: 01 [ 8964/20299 ( 44%)], Train Loss: 0.18203\n",
      "Epoch: 01 [ 9004/20299 ( 44%)], Train Loss: 0.18266\n",
      "Epoch: 01 [ 9044/20299 ( 45%)], Train Loss: 0.18237\n",
      "Epoch: 01 [ 9084/20299 ( 45%)], Train Loss: 0.18178\n",
      "Epoch: 01 [ 9124/20299 ( 45%)], Train Loss: 0.18158\n",
      "Epoch: 01 [ 9164/20299 ( 45%)], Train Loss: 0.18120\n",
      "Epoch: 01 [ 9204/20299 ( 45%)], Train Loss: 0.18087\n",
      "Epoch: 01 [ 9244/20299 ( 46%)], Train Loss: 0.18027\n",
      "Epoch: 01 [ 9284/20299 ( 46%)], Train Loss: 0.18027\n",
      "Epoch: 01 [ 9324/20299 ( 46%)], Train Loss: 0.18066\n",
      "Epoch: 01 [ 9364/20299 ( 46%)], Train Loss: 0.18035\n",
      "Epoch: 01 [ 9404/20299 ( 46%)], Train Loss: 0.17979\n",
      "Epoch: 01 [ 9444/20299 ( 47%)], Train Loss: 0.17968\n",
      "Epoch: 01 [ 9484/20299 ( 47%)], Train Loss: 0.17972\n",
      "Epoch: 01 [ 9524/20299 ( 47%)], Train Loss: 0.18023\n",
      "Epoch: 01 [ 9564/20299 ( 47%)], Train Loss: 0.17983\n",
      "Epoch: 01 [ 9604/20299 ( 47%)], Train Loss: 0.17963\n",
      "Epoch: 01 [ 9644/20299 ( 48%)], Train Loss: 0.17935\n",
      "Epoch: 01 [ 9684/20299 ( 48%)], Train Loss: 0.17884\n",
      "Epoch: 01 [ 9724/20299 ( 48%)], Train Loss: 0.17874\n",
      "Epoch: 01 [ 9764/20299 ( 48%)], Train Loss: 0.17948\n",
      "Epoch: 01 [ 9804/20299 ( 48%)], Train Loss: 0.17932\n",
      "Epoch: 01 [ 9844/20299 ( 48%)], Train Loss: 0.17918\n",
      "Epoch: 01 [ 9884/20299 ( 49%)], Train Loss: 0.17914\n",
      "Epoch: 01 [ 9924/20299 ( 49%)], Train Loss: 0.17885\n",
      "Epoch: 01 [ 9964/20299 ( 49%)], Train Loss: 0.17853\n",
      "Epoch: 01 [10004/20299 ( 49%)], Train Loss: 0.17814\n",
      "Epoch: 01 [10044/20299 ( 49%)], Train Loss: 0.17792\n",
      "Epoch: 01 [10084/20299 ( 50%)], Train Loss: 0.17856\n",
      "Epoch: 01 [10124/20299 ( 50%)], Train Loss: 0.17811\n",
      "Epoch: 01 [10164/20299 ( 50%)], Train Loss: 0.17797\n",
      "Epoch: 01 [10204/20299 ( 50%)], Train Loss: 0.17768\n",
      "Epoch: 01 [10244/20299 ( 50%)], Train Loss: 0.17742\n",
      "Epoch: 01 [10284/20299 ( 51%)], Train Loss: 0.17709\n",
      "Epoch: 01 [10324/20299 ( 51%)], Train Loss: 0.17667\n",
      "Epoch: 01 [10364/20299 ( 51%)], Train Loss: 0.17635\n",
      "Epoch: 01 [10404/20299 ( 51%)], Train Loss: 0.17581\n",
      "Epoch: 01 [10444/20299 ( 51%)], Train Loss: 0.17563\n",
      "Epoch: 01 [10484/20299 ( 52%)], Train Loss: 0.17521\n",
      "Epoch: 01 [10524/20299 ( 52%)], Train Loss: 0.17504\n",
      "Epoch: 01 [10564/20299 ( 52%)], Train Loss: 0.17494\n",
      "Epoch: 01 [10604/20299 ( 52%)], Train Loss: 0.17464\n",
      "Epoch: 01 [10644/20299 ( 52%)], Train Loss: 0.17452\n",
      "Epoch: 01 [10684/20299 ( 53%)], Train Loss: 0.17408\n",
      "Epoch: 01 [10724/20299 ( 53%)], Train Loss: 0.17536\n",
      "Epoch: 01 [10764/20299 ( 53%)], Train Loss: 0.17516\n",
      "Epoch: 01 [10804/20299 ( 53%)], Train Loss: 0.17494\n",
      "Epoch: 01 [10844/20299 ( 53%)], Train Loss: 0.17477\n",
      "Epoch: 01 [10884/20299 ( 54%)], Train Loss: 0.17470\n",
      "Epoch: 01 [10924/20299 ( 54%)], Train Loss: 0.17417\n",
      "Epoch: 01 [10964/20299 ( 54%)], Train Loss: 0.17371\n",
      "Epoch: 01 [11004/20299 ( 54%)], Train Loss: 0.17339\n",
      "Epoch: 01 [11044/20299 ( 54%)], Train Loss: 0.17374\n",
      "Epoch: 01 [11084/20299 ( 55%)], Train Loss: 0.17334\n",
      "Epoch: 01 [11124/20299 ( 55%)], Train Loss: 0.17344\n",
      "Epoch: 01 [11164/20299 ( 55%)], Train Loss: 0.17339\n",
      "Epoch: 01 [11204/20299 ( 55%)], Train Loss: 0.17342\n",
      "Epoch: 01 [11244/20299 ( 55%)], Train Loss: 0.17313\n",
      "Epoch: 01 [11284/20299 ( 56%)], Train Loss: 0.17329\n",
      "Epoch: 01 [11324/20299 ( 56%)], Train Loss: 0.17362\n",
      "Epoch: 01 [11364/20299 ( 56%)], Train Loss: 0.17319\n",
      "Epoch: 01 [11404/20299 ( 56%)], Train Loss: 0.17290\n",
      "Epoch: 01 [11444/20299 ( 56%)], Train Loss: 0.17261\n",
      "Epoch: 01 [11484/20299 ( 57%)], Train Loss: 0.17281\n",
      "Epoch: 01 [11524/20299 ( 57%)], Train Loss: 0.17300\n",
      "Epoch: 01 [11564/20299 ( 57%)], Train Loss: 0.17285\n",
      "Epoch: 01 [11604/20299 ( 57%)], Train Loss: 0.17283\n",
      "Epoch: 01 [11644/20299 ( 57%)], Train Loss: 0.17250\n",
      "Epoch: 01 [11684/20299 ( 58%)], Train Loss: 0.17262\n",
      "Epoch: 01 [11724/20299 ( 58%)], Train Loss: 0.17222\n",
      "Epoch: 01 [11764/20299 ( 58%)], Train Loss: 0.17201\n",
      "Epoch: 01 [11804/20299 ( 58%)], Train Loss: 0.17200\n",
      "Epoch: 01 [11844/20299 ( 58%)], Train Loss: 0.17183\n",
      "Epoch: 01 [11884/20299 ( 59%)], Train Loss: 0.17173\n",
      "Epoch: 01 [11924/20299 ( 59%)], Train Loss: 0.17200\n",
      "Epoch: 01 [11964/20299 ( 59%)], Train Loss: 0.17199\n",
      "Epoch: 01 [12004/20299 ( 59%)], Train Loss: 0.17197\n",
      "Epoch: 01 [12044/20299 ( 59%)], Train Loss: 0.17186\n",
      "Epoch: 01 [12084/20299 ( 60%)], Train Loss: 0.17155\n",
      "Epoch: 01 [12124/20299 ( 60%)], Train Loss: 0.17147\n",
      "Epoch: 01 [12164/20299 ( 60%)], Train Loss: 0.17165\n",
      "Epoch: 01 [12204/20299 ( 60%)], Train Loss: 0.17124\n",
      "Epoch: 01 [12244/20299 ( 60%)], Train Loss: 0.17141\n",
      "Epoch: 01 [12284/20299 ( 61%)], Train Loss: 0.17150\n",
      "Epoch: 01 [12324/20299 ( 61%)], Train Loss: 0.17112\n",
      "Epoch: 01 [12364/20299 ( 61%)], Train Loss: 0.17085\n",
      "Epoch: 01 [12404/20299 ( 61%)], Train Loss: 0.17061\n",
      "Epoch: 01 [12444/20299 ( 61%)], Train Loss: 0.17023\n",
      "Epoch: 01 [12484/20299 ( 62%)], Train Loss: 0.16997\n",
      "Epoch: 01 [12524/20299 ( 62%)], Train Loss: 0.16981\n",
      "Epoch: 01 [12564/20299 ( 62%)], Train Loss: 0.16945\n",
      "Epoch: 01 [12604/20299 ( 62%)], Train Loss: 0.16938\n",
      "Epoch: 01 [12644/20299 ( 62%)], Train Loss: 0.16946\n",
      "Epoch: 01 [12684/20299 ( 62%)], Train Loss: 0.16926\n",
      "Epoch: 01 [12724/20299 ( 63%)], Train Loss: 0.16914\n",
      "Epoch: 01 [12764/20299 ( 63%)], Train Loss: 0.16897\n",
      "Epoch: 01 [12804/20299 ( 63%)], Train Loss: 0.16869\n",
      "Epoch: 01 [12844/20299 ( 63%)], Train Loss: 0.16863\n",
      "Epoch: 01 [12884/20299 ( 63%)], Train Loss: 0.16868\n",
      "Epoch: 01 [12924/20299 ( 64%)], Train Loss: 0.16860\n",
      "Epoch: 01 [12964/20299 ( 64%)], Train Loss: 0.16863\n",
      "Epoch: 01 [13004/20299 ( 64%)], Train Loss: 0.16830\n",
      "Epoch: 01 [13044/20299 ( 64%)], Train Loss: 0.16805\n",
      "Epoch: 01 [13084/20299 ( 64%)], Train Loss: 0.16788\n",
      "Epoch: 01 [13124/20299 ( 65%)], Train Loss: 0.16762\n",
      "Epoch: 01 [13164/20299 ( 65%)], Train Loss: 0.16736\n",
      "Epoch: 01 [13204/20299 ( 65%)], Train Loss: 0.16708\n",
      "Epoch: 01 [13244/20299 ( 65%)], Train Loss: 0.16702\n",
      "Epoch: 01 [13284/20299 ( 65%)], Train Loss: 0.16673\n",
      "Epoch: 01 [13324/20299 ( 66%)], Train Loss: 0.16676\n",
      "Epoch: 01 [13364/20299 ( 66%)], Train Loss: 0.16656\n",
      "Epoch: 01 [13404/20299 ( 66%)], Train Loss: 0.16648\n",
      "Epoch: 01 [13444/20299 ( 66%)], Train Loss: 0.16659\n",
      "Epoch: 01 [13484/20299 ( 66%)], Train Loss: 0.16624\n",
      "Epoch: 01 [13524/20299 ( 67%)], Train Loss: 0.16603\n",
      "Epoch: 01 [13564/20299 ( 67%)], Train Loss: 0.16577\n",
      "Epoch: 01 [13604/20299 ( 67%)], Train Loss: 0.16538\n",
      "Epoch: 01 [13644/20299 ( 67%)], Train Loss: 0.16511\n",
      "Epoch: 01 [13684/20299 ( 67%)], Train Loss: 0.16520\n",
      "Epoch: 01 [13724/20299 ( 68%)], Train Loss: 0.16499\n",
      "Epoch: 01 [13764/20299 ( 68%)], Train Loss: 0.16498\n",
      "Epoch: 01 [13804/20299 ( 68%)], Train Loss: 0.16464\n",
      "Epoch: 01 [13844/20299 ( 68%)], Train Loss: 0.16449\n",
      "Epoch: 01 [13884/20299 ( 68%)], Train Loss: 0.16454\n",
      "Epoch: 01 [13924/20299 ( 69%)], Train Loss: 0.16452\n",
      "Epoch: 01 [13964/20299 ( 69%)], Train Loss: 0.16454\n",
      "Epoch: 01 [14004/20299 ( 69%)], Train Loss: 0.16439\n",
      "Epoch: 01 [14044/20299 ( 69%)], Train Loss: 0.16426\n",
      "Epoch: 01 [14084/20299 ( 69%)], Train Loss: 0.16404\n",
      "Epoch: 01 [14124/20299 ( 70%)], Train Loss: 0.16379\n",
      "Epoch: 01 [14164/20299 ( 70%)], Train Loss: 0.16359\n",
      "Epoch: 01 [14204/20299 ( 70%)], Train Loss: 0.16354\n",
      "Epoch: 01 [14244/20299 ( 70%)], Train Loss: 0.16344\n",
      "Epoch: 01 [14284/20299 ( 70%)], Train Loss: 0.16309\n",
      "Epoch: 01 [14324/20299 ( 71%)], Train Loss: 0.16322\n",
      "Epoch: 01 [14364/20299 ( 71%)], Train Loss: 0.16295\n",
      "Epoch: 01 [14404/20299 ( 71%)], Train Loss: 0.16265\n",
      "Epoch: 01 [14444/20299 ( 71%)], Train Loss: 0.16255\n",
      "Epoch: 01 [14484/20299 ( 71%)], Train Loss: 0.16235\n",
      "Epoch: 01 [14524/20299 ( 72%)], Train Loss: 0.16212\n",
      "Epoch: 01 [14564/20299 ( 72%)], Train Loss: 0.16223\n",
      "Epoch: 01 [14604/20299 ( 72%)], Train Loss: 0.16209\n",
      "Epoch: 01 [14644/20299 ( 72%)], Train Loss: 0.16172\n",
      "Epoch: 01 [14684/20299 ( 72%)], Train Loss: 0.16150\n",
      "Epoch: 01 [14724/20299 ( 73%)], Train Loss: 0.16133\n",
      "Epoch: 01 [14764/20299 ( 73%)], Train Loss: 0.16110\n",
      "Epoch: 01 [14804/20299 ( 73%)], Train Loss: 0.16080\n",
      "Epoch: 01 [14844/20299 ( 73%)], Train Loss: 0.16090\n",
      "Epoch: 01 [14884/20299 ( 73%)], Train Loss: 0.16057\n",
      "Epoch: 01 [14924/20299 ( 74%)], Train Loss: 0.16026\n",
      "Epoch: 01 [14964/20299 ( 74%)], Train Loss: 0.16005\n",
      "Epoch: 01 [15004/20299 ( 74%)], Train Loss: 0.15995\n",
      "Epoch: 01 [15044/20299 ( 74%)], Train Loss: 0.15966\n",
      "Epoch: 01 [15084/20299 ( 74%)], Train Loss: 0.15960\n",
      "Epoch: 01 [15124/20299 ( 75%)], Train Loss: 0.15930\n",
      "Epoch: 01 [15164/20299 ( 75%)], Train Loss: 0.15922\n",
      "Epoch: 01 [15204/20299 ( 75%)], Train Loss: 0.15893\n",
      "Epoch: 01 [15244/20299 ( 75%)], Train Loss: 0.15880\n",
      "Epoch: 01 [15284/20299 ( 75%)], Train Loss: 0.15876\n",
      "Epoch: 01 [15324/20299 ( 75%)], Train Loss: 0.15872\n",
      "Epoch: 01 [15364/20299 ( 76%)], Train Loss: 0.15847\n",
      "Epoch: 01 [15404/20299 ( 76%)], Train Loss: 0.15837\n",
      "Epoch: 01 [15444/20299 ( 76%)], Train Loss: 0.15819\n",
      "Epoch: 01 [15484/20299 ( 76%)], Train Loss: 0.15810\n",
      "Epoch: 01 [15524/20299 ( 76%)], Train Loss: 0.15798\n",
      "Epoch: 01 [15564/20299 ( 77%)], Train Loss: 0.15819\n",
      "Epoch: 01 [15604/20299 ( 77%)], Train Loss: 0.15799\n",
      "Epoch: 01 [15644/20299 ( 77%)], Train Loss: 0.15796\n",
      "Epoch: 01 [15684/20299 ( 77%)], Train Loss: 0.15811\n",
      "Epoch: 01 [15724/20299 ( 77%)], Train Loss: 0.15791\n",
      "Epoch: 01 [15764/20299 ( 78%)], Train Loss: 0.15767\n",
      "Epoch: 01 [15804/20299 ( 78%)], Train Loss: 0.15794\n",
      "Epoch: 01 [15844/20299 ( 78%)], Train Loss: 0.15763\n",
      "Epoch: 01 [15884/20299 ( 78%)], Train Loss: 0.15769\n",
      "Epoch: 01 [15924/20299 ( 78%)], Train Loss: 0.15759\n",
      "Epoch: 01 [15964/20299 ( 79%)], Train Loss: 0.15754\n",
      "Epoch: 01 [16004/20299 ( 79%)], Train Loss: 0.15762\n",
      "Epoch: 01 [16044/20299 ( 79%)], Train Loss: 0.15738\n",
      "Epoch: 01 [16084/20299 ( 79%)], Train Loss: 0.15725\n",
      "Epoch: 01 [16124/20299 ( 79%)], Train Loss: 0.15699\n",
      "Epoch: 01 [16164/20299 ( 80%)], Train Loss: 0.15700\n",
      "Epoch: 01 [16204/20299 ( 80%)], Train Loss: 0.15674\n",
      "Epoch: 01 [16244/20299 ( 80%)], Train Loss: 0.15671\n",
      "Epoch: 01 [16284/20299 ( 80%)], Train Loss: 0.15668\n",
      "Epoch: 01 [16324/20299 ( 80%)], Train Loss: 0.15663\n",
      "Epoch: 01 [16364/20299 ( 81%)], Train Loss: 0.15663\n",
      "Epoch: 01 [16404/20299 ( 81%)], Train Loss: 0.15639\n",
      "Epoch: 01 [16444/20299 ( 81%)], Train Loss: 0.15612\n",
      "Epoch: 01 [16484/20299 ( 81%)], Train Loss: 0.15585\n",
      "Epoch: 01 [16524/20299 ( 81%)], Train Loss: 0.15602\n",
      "Epoch: 01 [16564/20299 ( 82%)], Train Loss: 0.15625\n",
      "Epoch: 01 [16604/20299 ( 82%)], Train Loss: 0.15606\n",
      "Epoch: 01 [16644/20299 ( 82%)], Train Loss: 0.15593\n",
      "Epoch: 01 [16684/20299 ( 82%)], Train Loss: 0.15576\n",
      "Epoch: 01 [16724/20299 ( 82%)], Train Loss: 0.15547\n",
      "Epoch: 01 [16764/20299 ( 83%)], Train Loss: 0.15530\n",
      "Epoch: 01 [16804/20299 ( 83%)], Train Loss: 0.15515\n",
      "Epoch: 01 [16844/20299 ( 83%)], Train Loss: 0.15485\n",
      "Epoch: 01 [16884/20299 ( 83%)], Train Loss: 0.15464\n",
      "Epoch: 01 [16924/20299 ( 83%)], Train Loss: 0.15445\n",
      "Epoch: 01 [16964/20299 ( 84%)], Train Loss: 0.15430\n",
      "Epoch: 01 [17004/20299 ( 84%)], Train Loss: 0.15415\n",
      "Epoch: 01 [17044/20299 ( 84%)], Train Loss: 0.15391\n",
      "Epoch: 01 [17084/20299 ( 84%)], Train Loss: 0.15380\n",
      "Epoch: 01 [17124/20299 ( 84%)], Train Loss: 0.15360\n",
      "Epoch: 01 [17164/20299 ( 85%)], Train Loss: 0.15361\n",
      "Epoch: 01 [17204/20299 ( 85%)], Train Loss: 0.15397\n",
      "Epoch: 01 [17244/20299 ( 85%)], Train Loss: 0.15371\n",
      "Epoch: 01 [17284/20299 ( 85%)], Train Loss: 0.15353\n",
      "Epoch: 01 [17324/20299 ( 85%)], Train Loss: 0.15330\n",
      "Epoch: 01 [17364/20299 ( 86%)], Train Loss: 0.15316\n",
      "Epoch: 01 [17404/20299 ( 86%)], Train Loss: 0.15305\n",
      "Epoch: 01 [17444/20299 ( 86%)], Train Loss: 0.15304\n",
      "Epoch: 01 [17484/20299 ( 86%)], Train Loss: 0.15282\n",
      "Epoch: 01 [17524/20299 ( 86%)], Train Loss: 0.15275\n",
      "Epoch: 01 [17564/20299 ( 87%)], Train Loss: 0.15264\n",
      "Epoch: 01 [17604/20299 ( 87%)], Train Loss: 0.15236\n",
      "Epoch: 01 [17644/20299 ( 87%)], Train Loss: 0.15271\n",
      "Epoch: 01 [17684/20299 ( 87%)], Train Loss: 0.15252\n",
      "Epoch: 01 [17724/20299 ( 87%)], Train Loss: 0.15247\n",
      "Epoch: 01 [17764/20299 ( 88%)], Train Loss: 0.15243\n",
      "Epoch: 01 [17804/20299 ( 88%)], Train Loss: 0.15255\n",
      "Epoch: 01 [17844/20299 ( 88%)], Train Loss: 0.15243\n",
      "Epoch: 01 [17884/20299 ( 88%)], Train Loss: 0.15226\n",
      "Epoch: 01 [17924/20299 ( 88%)], Train Loss: 0.15216\n",
      "Epoch: 01 [17964/20299 ( 88%)], Train Loss: 0.15205\n",
      "Epoch: 01 [18004/20299 ( 89%)], Train Loss: 0.15243\n",
      "Epoch: 01 [18044/20299 ( 89%)], Train Loss: 0.15220\n",
      "Epoch: 01 [18084/20299 ( 89%)], Train Loss: 0.15209\n",
      "Epoch: 01 [18124/20299 ( 89%)], Train Loss: 0.15209\n",
      "Epoch: 01 [18164/20299 ( 89%)], Train Loss: 0.15238\n",
      "Epoch: 01 [18204/20299 ( 90%)], Train Loss: 0.15228\n",
      "Epoch: 01 [18244/20299 ( 90%)], Train Loss: 0.15241\n",
      "Epoch: 01 [18284/20299 ( 90%)], Train Loss: 0.15217\n",
      "Epoch: 01 [18324/20299 ( 90%)], Train Loss: 0.15201\n",
      "Epoch: 01 [18364/20299 ( 90%)], Train Loss: 0.15194\n",
      "Epoch: 01 [18404/20299 ( 91%)], Train Loss: 0.15180\n",
      "Epoch: 01 [18444/20299 ( 91%)], Train Loss: 0.15166\n",
      "Epoch: 01 [18484/20299 ( 91%)], Train Loss: 0.15155\n",
      "Epoch: 01 [18524/20299 ( 91%)], Train Loss: 0.15146\n",
      "Epoch: 01 [18564/20299 ( 91%)], Train Loss: 0.15131\n",
      "Epoch: 01 [18604/20299 ( 92%)], Train Loss: 0.15113\n",
      "Epoch: 01 [18644/20299 ( 92%)], Train Loss: 0.15129\n",
      "Epoch: 01 [18684/20299 ( 92%)], Train Loss: 0.15122\n",
      "Epoch: 01 [18724/20299 ( 92%)], Train Loss: 0.15109\n",
      "Epoch: 01 [18764/20299 ( 92%)], Train Loss: 0.15085\n",
      "Epoch: 01 [18804/20299 ( 93%)], Train Loss: 0.15088\n",
      "Epoch: 01 [18844/20299 ( 93%)], Train Loss: 0.15085\n",
      "Epoch: 01 [18884/20299 ( 93%)], Train Loss: 0.15074\n",
      "Epoch: 01 [18924/20299 ( 93%)], Train Loss: 0.15073\n",
      "Epoch: 01 [18964/20299 ( 93%)], Train Loss: 0.15061\n",
      "Epoch: 01 [19004/20299 ( 94%)], Train Loss: 0.15051\n",
      "Epoch: 01 [19044/20299 ( 94%)], Train Loss: 0.15048\n",
      "Epoch: 01 [19084/20299 ( 94%)], Train Loss: 0.15044\n",
      "Epoch: 01 [19124/20299 ( 94%)], Train Loss: 0.15036\n",
      "Epoch: 01 [19164/20299 ( 94%)], Train Loss: 0.15021\n",
      "Epoch: 01 [19204/20299 ( 95%)], Train Loss: 0.15006\n",
      "Epoch: 01 [19244/20299 ( 95%)], Train Loss: 0.14985\n",
      "Epoch: 01 [19284/20299 ( 95%)], Train Loss: 0.14988\n",
      "Epoch: 01 [19324/20299 ( 95%)], Train Loss: 0.14971\n",
      "Epoch: 01 [19364/20299 ( 95%)], Train Loss: 0.14959\n",
      "Epoch: 01 [19404/20299 ( 96%)], Train Loss: 0.14949\n",
      "Epoch: 01 [19444/20299 ( 96%)], Train Loss: 0.14933\n",
      "Epoch: 01 [19484/20299 ( 96%)], Train Loss: 0.14915\n",
      "Epoch: 01 [19524/20299 ( 96%)], Train Loss: 0.14905\n",
      "Epoch: 01 [19564/20299 ( 96%)], Train Loss: 0.14879\n",
      "Epoch: 01 [19604/20299 ( 97%)], Train Loss: 0.14888\n",
      "Epoch: 01 [19644/20299 ( 97%)], Train Loss: 0.14881\n",
      "Epoch: 01 [19684/20299 ( 97%)], Train Loss: 0.14903\n",
      "Epoch: 01 [19724/20299 ( 97%)], Train Loss: 0.14888\n",
      "Epoch: 01 [19764/20299 ( 97%)], Train Loss: 0.14876\n",
      "Epoch: 01 [19804/20299 ( 98%)], Train Loss: 0.14864\n",
      "Epoch: 01 [19844/20299 ( 98%)], Train Loss: 0.14852\n",
      "Epoch: 01 [19884/20299 ( 98%)], Train Loss: 0.14866\n",
      "Epoch: 01 [19924/20299 ( 98%)], Train Loss: 0.14878\n",
      "Epoch: 01 [19964/20299 ( 98%)], Train Loss: 0.14856\n",
      "Epoch: 01 [20004/20299 ( 99%)], Train Loss: 0.14851\n",
      "Epoch: 01 [20044/20299 ( 99%)], Train Loss: 0.14830\n",
      "Epoch: 01 [20084/20299 ( 99%)], Train Loss: 0.14825\n",
      "Epoch: 01 [20124/20299 ( 99%)], Train Loss: 0.14827\n",
      "Epoch: 01 [20164/20299 ( 99%)], Train Loss: 0.14816\n",
      "Epoch: 01 [20204/20299 (100%)], Train Loss: 0.14810\n",
      "Epoch: 01 [20244/20299 (100%)], Train Loss: 0.14792\n",
      "Epoch: 01 [20284/20299 (100%)], Train Loss: 0.14795\n",
      "Epoch: 01 [20299/20299 (100%)], Train Loss: 0.14787\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.27376\n",
      "\n",
      "Total Training Time: 5736.796823501587secs, Average Training Time per Epoch: 2868.3984117507935secs.\n",
      "Total Validation Time: 233.81421780586243secs, Average Validation Time per Epoch: 116.90710890293121secs.\n"
     ]
    }
   ],
   "source": [
    "for fold in range(2, 3):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c1d5a5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T08:18:50.438401Z",
     "iopub.status.busy": "2021-10-03T08:18:50.437634Z",
     "iopub.status.idle": "2021-10-03T09:59:14.121749Z",
     "shell.execute_reply": "2021-10-03T09:59:14.121215Z"
    },
    "papermill": {
     "duration": 6024.479978,
     "end_time": "2021-10-03T09:59:14.121893",
     "exception": false,
     "start_time": "2021-10-03T08:18:49.641915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 3\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20101, Num examples Valid=2968\n",
      "Total Training Steps: 5026, Total Warmup Steps: 502\n",
      "Epoch: 00 [    4/20101 (  0%)], Train Loss: 3.03077\n",
      "Epoch: 00 [   44/20101 (  0%)], Train Loss: 3.06658\n",
      "Epoch: 00 [   84/20101 (  0%)], Train Loss: 3.06563\n",
      "Epoch: 00 [  124/20101 (  1%)], Train Loss: 3.05761\n",
      "Epoch: 00 [  164/20101 (  1%)], Train Loss: 3.03374\n",
      "Epoch: 00 [  204/20101 (  1%)], Train Loss: 3.00115\n",
      "Epoch: 00 [  244/20101 (  1%)], Train Loss: 2.96456\n",
      "Epoch: 00 [  284/20101 (  1%)], Train Loss: 2.91843\n",
      "Epoch: 00 [  324/20101 (  2%)], Train Loss: 2.86900\n",
      "Epoch: 00 [  364/20101 (  2%)], Train Loss: 2.81030\n",
      "Epoch: 00 [  404/20101 (  2%)], Train Loss: 2.74215\n",
      "Epoch: 00 [  444/20101 (  2%)], Train Loss: 2.67682\n",
      "Epoch: 00 [  484/20101 (  2%)], Train Loss: 2.60581\n",
      "Epoch: 00 [  524/20101 (  3%)], Train Loss: 2.51771\n",
      "Epoch: 00 [  564/20101 (  3%)], Train Loss: 2.41797\n",
      "Epoch: 00 [  604/20101 (  3%)], Train Loss: 2.32006\n",
      "Epoch: 00 [  644/20101 (  3%)], Train Loss: 2.24068\n",
      "Epoch: 00 [  684/20101 (  3%)], Train Loss: 2.14882\n",
      "Epoch: 00 [  724/20101 (  4%)], Train Loss: 2.08198\n",
      "Epoch: 00 [  764/20101 (  4%)], Train Loss: 2.01936\n",
      "Epoch: 00 [  804/20101 (  4%)], Train Loss: 1.95550\n",
      "Epoch: 00 [  844/20101 (  4%)], Train Loss: 1.88766\n",
      "Epoch: 00 [  884/20101 (  4%)], Train Loss: 1.82782\n",
      "Epoch: 00 [  924/20101 (  5%)], Train Loss: 1.77424\n",
      "Epoch: 00 [  964/20101 (  5%)], Train Loss: 1.72775\n",
      "Epoch: 00 [ 1004/20101 (  5%)], Train Loss: 1.68436\n",
      "Epoch: 00 [ 1044/20101 (  5%)], Train Loss: 1.63560\n",
      "Epoch: 00 [ 1084/20101 (  5%)], Train Loss: 1.60361\n",
      "Epoch: 00 [ 1124/20101 (  6%)], Train Loss: 1.56619\n",
      "Epoch: 00 [ 1164/20101 (  6%)], Train Loss: 1.52351\n",
      "Epoch: 00 [ 1204/20101 (  6%)], Train Loss: 1.48840\n",
      "Epoch: 00 [ 1244/20101 (  6%)], Train Loss: 1.45818\n",
      "Epoch: 00 [ 1284/20101 (  6%)], Train Loss: 1.43625\n",
      "Epoch: 00 [ 1324/20101 (  7%)], Train Loss: 1.40888\n",
      "Epoch: 00 [ 1364/20101 (  7%)], Train Loss: 1.37918\n",
      "Epoch: 00 [ 1404/20101 (  7%)], Train Loss: 1.35477\n",
      "Epoch: 00 [ 1444/20101 (  7%)], Train Loss: 1.32760\n",
      "Epoch: 00 [ 1484/20101 (  7%)], Train Loss: 1.30557\n",
      "Epoch: 00 [ 1524/20101 (  8%)], Train Loss: 1.27728\n",
      "Epoch: 00 [ 1564/20101 (  8%)], Train Loss: 1.25124\n",
      "Epoch: 00 [ 1604/20101 (  8%)], Train Loss: 1.23187\n",
      "Epoch: 00 [ 1644/20101 (  8%)], Train Loss: 1.21210\n",
      "Epoch: 00 [ 1684/20101 (  8%)], Train Loss: 1.19178\n",
      "Epoch: 00 [ 1724/20101 (  9%)], Train Loss: 1.17558\n",
      "Epoch: 00 [ 1764/20101 (  9%)], Train Loss: 1.15838\n",
      "Epoch: 00 [ 1804/20101 (  9%)], Train Loss: 1.13997\n",
      "Epoch: 00 [ 1844/20101 (  9%)], Train Loss: 1.12251\n",
      "Epoch: 00 [ 1884/20101 (  9%)], Train Loss: 1.10492\n",
      "Epoch: 00 [ 1924/20101 ( 10%)], Train Loss: 1.08686\n",
      "Epoch: 00 [ 1964/20101 ( 10%)], Train Loss: 1.07267\n",
      "Epoch: 00 [ 2004/20101 ( 10%)], Train Loss: 1.06250\n",
      "Epoch: 00 [ 2044/20101 ( 10%)], Train Loss: 1.05032\n",
      "Epoch: 00 [ 2084/20101 ( 10%)], Train Loss: 1.03924\n",
      "Epoch: 00 [ 2124/20101 ( 11%)], Train Loss: 1.03023\n",
      "Epoch: 00 [ 2164/20101 ( 11%)], Train Loss: 1.02056\n",
      "Epoch: 00 [ 2204/20101 ( 11%)], Train Loss: 1.01181\n",
      "Epoch: 00 [ 2244/20101 ( 11%)], Train Loss: 0.99851\n",
      "Epoch: 00 [ 2284/20101 ( 11%)], Train Loss: 0.99132\n",
      "Epoch: 00 [ 2324/20101 ( 12%)], Train Loss: 0.98149\n",
      "Epoch: 00 [ 2364/20101 ( 12%)], Train Loss: 0.96903\n",
      "Epoch: 00 [ 2404/20101 ( 12%)], Train Loss: 0.95794\n",
      "Epoch: 00 [ 2444/20101 ( 12%)], Train Loss: 0.94717\n",
      "Epoch: 00 [ 2484/20101 ( 12%)], Train Loss: 0.93880\n",
      "Epoch: 00 [ 2524/20101 ( 13%)], Train Loss: 0.92976\n",
      "Epoch: 00 [ 2564/20101 ( 13%)], Train Loss: 0.92310\n",
      "Epoch: 00 [ 2604/20101 ( 13%)], Train Loss: 0.91482\n",
      "Epoch: 00 [ 2644/20101 ( 13%)], Train Loss: 0.90503\n",
      "Epoch: 00 [ 2684/20101 ( 13%)], Train Loss: 0.89728\n",
      "Epoch: 00 [ 2724/20101 ( 14%)], Train Loss: 0.88984\n",
      "Epoch: 00 [ 2764/20101 ( 14%)], Train Loss: 0.88123\n",
      "Epoch: 00 [ 2804/20101 ( 14%)], Train Loss: 0.87585\n",
      "Epoch: 00 [ 2844/20101 ( 14%)], Train Loss: 0.87020\n",
      "Epoch: 00 [ 2884/20101 ( 14%)], Train Loss: 0.86282\n",
      "Epoch: 00 [ 2924/20101 ( 15%)], Train Loss: 0.85587\n",
      "Epoch: 00 [ 2964/20101 ( 15%)], Train Loss: 0.85151\n",
      "Epoch: 00 [ 3004/20101 ( 15%)], Train Loss: 0.84589\n",
      "Epoch: 00 [ 3044/20101 ( 15%)], Train Loss: 0.84323\n",
      "Epoch: 00 [ 3084/20101 ( 15%)], Train Loss: 0.83838\n",
      "Epoch: 00 [ 3124/20101 ( 16%)], Train Loss: 0.83308\n",
      "Epoch: 00 [ 3164/20101 ( 16%)], Train Loss: 0.82958\n",
      "Epoch: 00 [ 3204/20101 ( 16%)], Train Loss: 0.82580\n",
      "Epoch: 00 [ 3244/20101 ( 16%)], Train Loss: 0.82209\n",
      "Epoch: 00 [ 3284/20101 ( 16%)], Train Loss: 0.81749\n",
      "Epoch: 00 [ 3324/20101 ( 17%)], Train Loss: 0.81172\n",
      "Epoch: 00 [ 3364/20101 ( 17%)], Train Loss: 0.80548\n",
      "Epoch: 00 [ 3404/20101 ( 17%)], Train Loss: 0.79977\n",
      "Epoch: 00 [ 3444/20101 ( 17%)], Train Loss: 0.79579\n",
      "Epoch: 00 [ 3484/20101 ( 17%)], Train Loss: 0.78999\n",
      "Epoch: 00 [ 3524/20101 ( 18%)], Train Loss: 0.78543\n",
      "Epoch: 00 [ 3564/20101 ( 18%)], Train Loss: 0.77990\n",
      "Epoch: 00 [ 3604/20101 ( 18%)], Train Loss: 0.77953\n",
      "Epoch: 00 [ 3644/20101 ( 18%)], Train Loss: 0.77649\n",
      "Epoch: 00 [ 3684/20101 ( 18%)], Train Loss: 0.77231\n",
      "Epoch: 00 [ 3724/20101 ( 19%)], Train Loss: 0.76829\n",
      "Epoch: 00 [ 3764/20101 ( 19%)], Train Loss: 0.76578\n",
      "Epoch: 00 [ 3804/20101 ( 19%)], Train Loss: 0.76297\n",
      "Epoch: 00 [ 3844/20101 ( 19%)], Train Loss: 0.75845\n",
      "Epoch: 00 [ 3884/20101 ( 19%)], Train Loss: 0.75591\n",
      "Epoch: 00 [ 3924/20101 ( 20%)], Train Loss: 0.75303\n",
      "Epoch: 00 [ 3964/20101 ( 20%)], Train Loss: 0.75034\n",
      "Epoch: 00 [ 4004/20101 ( 20%)], Train Loss: 0.74485\n",
      "Epoch: 00 [ 4044/20101 ( 20%)], Train Loss: 0.74253\n",
      "Epoch: 00 [ 4084/20101 ( 20%)], Train Loss: 0.73864\n",
      "Epoch: 00 [ 4124/20101 ( 21%)], Train Loss: 0.73597\n",
      "Epoch: 00 [ 4164/20101 ( 21%)], Train Loss: 0.73242\n",
      "Epoch: 00 [ 4204/20101 ( 21%)], Train Loss: 0.72863\n",
      "Epoch: 00 [ 4244/20101 ( 21%)], Train Loss: 0.72569\n",
      "Epoch: 00 [ 4284/20101 ( 21%)], Train Loss: 0.72284\n",
      "Epoch: 00 [ 4324/20101 ( 22%)], Train Loss: 0.71872\n",
      "Epoch: 00 [ 4364/20101 ( 22%)], Train Loss: 0.71611\n",
      "Epoch: 00 [ 4404/20101 ( 22%)], Train Loss: 0.71461\n",
      "Epoch: 00 [ 4444/20101 ( 22%)], Train Loss: 0.71379\n",
      "Epoch: 00 [ 4484/20101 ( 22%)], Train Loss: 0.71274\n",
      "Epoch: 00 [ 4524/20101 ( 23%)], Train Loss: 0.70913\n",
      "Epoch: 00 [ 4564/20101 ( 23%)], Train Loss: 0.70780\n",
      "Epoch: 00 [ 4604/20101 ( 23%)], Train Loss: 0.70706\n",
      "Epoch: 00 [ 4644/20101 ( 23%)], Train Loss: 0.70421\n",
      "Epoch: 00 [ 4684/20101 ( 23%)], Train Loss: 0.70219\n",
      "Epoch: 00 [ 4724/20101 ( 24%)], Train Loss: 0.69857\n",
      "Epoch: 00 [ 4764/20101 ( 24%)], Train Loss: 0.69664\n",
      "Epoch: 00 [ 4804/20101 ( 24%)], Train Loss: 0.69391\n",
      "Epoch: 00 [ 4844/20101 ( 24%)], Train Loss: 0.68987\n",
      "Epoch: 00 [ 4884/20101 ( 24%)], Train Loss: 0.68806\n",
      "Epoch: 00 [ 4924/20101 ( 24%)], Train Loss: 0.68550\n",
      "Epoch: 00 [ 4964/20101 ( 25%)], Train Loss: 0.68481\n",
      "Epoch: 00 [ 5004/20101 ( 25%)], Train Loss: 0.68269\n",
      "Epoch: 00 [ 5044/20101 ( 25%)], Train Loss: 0.68017\n",
      "Epoch: 00 [ 5084/20101 ( 25%)], Train Loss: 0.67901\n",
      "Epoch: 00 [ 5124/20101 ( 25%)], Train Loss: 0.67612\n",
      "Epoch: 00 [ 5164/20101 ( 26%)], Train Loss: 0.67492\n",
      "Epoch: 00 [ 5204/20101 ( 26%)], Train Loss: 0.67372\n",
      "Epoch: 00 [ 5244/20101 ( 26%)], Train Loss: 0.66980\n",
      "Epoch: 00 [ 5284/20101 ( 26%)], Train Loss: 0.66920\n",
      "Epoch: 00 [ 5324/20101 ( 26%)], Train Loss: 0.66779\n",
      "Epoch: 00 [ 5364/20101 ( 27%)], Train Loss: 0.66507\n",
      "Epoch: 00 [ 5404/20101 ( 27%)], Train Loss: 0.66164\n",
      "Epoch: 00 [ 5444/20101 ( 27%)], Train Loss: 0.65986\n",
      "Epoch: 00 [ 5484/20101 ( 27%)], Train Loss: 0.65925\n",
      "Epoch: 00 [ 5524/20101 ( 27%)], Train Loss: 0.65847\n",
      "Epoch: 00 [ 5564/20101 ( 28%)], Train Loss: 0.65634\n",
      "Epoch: 00 [ 5604/20101 ( 28%)], Train Loss: 0.65627\n",
      "Epoch: 00 [ 5644/20101 ( 28%)], Train Loss: 0.65422\n",
      "Epoch: 00 [ 5684/20101 ( 28%)], Train Loss: 0.65152\n",
      "Epoch: 00 [ 5724/20101 ( 28%)], Train Loss: 0.65023\n",
      "Epoch: 00 [ 5764/20101 ( 29%)], Train Loss: 0.64795\n",
      "Epoch: 00 [ 5804/20101 ( 29%)], Train Loss: 0.64715\n",
      "Epoch: 00 [ 5844/20101 ( 29%)], Train Loss: 0.64470\n",
      "Epoch: 00 [ 5884/20101 ( 29%)], Train Loss: 0.64206\n",
      "Epoch: 00 [ 5924/20101 ( 29%)], Train Loss: 0.63982\n",
      "Epoch: 00 [ 5964/20101 ( 30%)], Train Loss: 0.63669\n",
      "Epoch: 00 [ 6004/20101 ( 30%)], Train Loss: 0.63618\n",
      "Epoch: 00 [ 6044/20101 ( 30%)], Train Loss: 0.63455\n",
      "Epoch: 00 [ 6084/20101 ( 30%)], Train Loss: 0.63299\n",
      "Epoch: 00 [ 6124/20101 ( 30%)], Train Loss: 0.63050\n",
      "Epoch: 00 [ 6164/20101 ( 31%)], Train Loss: 0.63110\n",
      "Epoch: 00 [ 6204/20101 ( 31%)], Train Loss: 0.62925\n",
      "Epoch: 00 [ 6244/20101 ( 31%)], Train Loss: 0.62892\n",
      "Epoch: 00 [ 6284/20101 ( 31%)], Train Loss: 0.62881\n",
      "Epoch: 00 [ 6324/20101 ( 31%)], Train Loss: 0.62736\n",
      "Epoch: 00 [ 6364/20101 ( 32%)], Train Loss: 0.62714\n",
      "Epoch: 00 [ 6404/20101 ( 32%)], Train Loss: 0.62675\n",
      "Epoch: 00 [ 6444/20101 ( 32%)], Train Loss: 0.62561\n",
      "Epoch: 00 [ 6484/20101 ( 32%)], Train Loss: 0.62354\n",
      "Epoch: 00 [ 6524/20101 ( 32%)], Train Loss: 0.62132\n",
      "Epoch: 00 [ 6564/20101 ( 33%)], Train Loss: 0.61928\n",
      "Epoch: 00 [ 6604/20101 ( 33%)], Train Loss: 0.61651\n",
      "Epoch: 00 [ 6644/20101 ( 33%)], Train Loss: 0.61539\n",
      "Epoch: 00 [ 6684/20101 ( 33%)], Train Loss: 0.61376\n",
      "Epoch: 00 [ 6724/20101 ( 33%)], Train Loss: 0.61193\n",
      "Epoch: 00 [ 6764/20101 ( 34%)], Train Loss: 0.61078\n",
      "Epoch: 00 [ 6804/20101 ( 34%)], Train Loss: 0.60958\n",
      "Epoch: 00 [ 6844/20101 ( 34%)], Train Loss: 0.60874\n",
      "Epoch: 00 [ 6884/20101 ( 34%)], Train Loss: 0.60734\n",
      "Epoch: 00 [ 6924/20101 ( 34%)], Train Loss: 0.60546\n",
      "Epoch: 00 [ 6964/20101 ( 35%)], Train Loss: 0.60403\n",
      "Epoch: 00 [ 7004/20101 ( 35%)], Train Loss: 0.60160\n",
      "Epoch: 00 [ 7044/20101 ( 35%)], Train Loss: 0.60090\n",
      "Epoch: 00 [ 7084/20101 ( 35%)], Train Loss: 0.59881\n",
      "Epoch: 00 [ 7124/20101 ( 35%)], Train Loss: 0.59913\n",
      "Epoch: 00 [ 7164/20101 ( 36%)], Train Loss: 0.59663\n",
      "Epoch: 00 [ 7204/20101 ( 36%)], Train Loss: 0.59520\n",
      "Epoch: 00 [ 7244/20101 ( 36%)], Train Loss: 0.59390\n",
      "Epoch: 00 [ 7284/20101 ( 36%)], Train Loss: 0.59278\n",
      "Epoch: 00 [ 7324/20101 ( 36%)], Train Loss: 0.59189\n",
      "Epoch: 00 [ 7364/20101 ( 37%)], Train Loss: 0.59040\n",
      "Epoch: 00 [ 7404/20101 ( 37%)], Train Loss: 0.58866\n",
      "Epoch: 00 [ 7444/20101 ( 37%)], Train Loss: 0.58854\n",
      "Epoch: 00 [ 7484/20101 ( 37%)], Train Loss: 0.58779\n",
      "Epoch: 00 [ 7524/20101 ( 37%)], Train Loss: 0.58701\n",
      "Epoch: 00 [ 7564/20101 ( 38%)], Train Loss: 0.58578\n",
      "Epoch: 00 [ 7604/20101 ( 38%)], Train Loss: 0.58348\n",
      "Epoch: 00 [ 7644/20101 ( 38%)], Train Loss: 0.58272\n",
      "Epoch: 00 [ 7684/20101 ( 38%)], Train Loss: 0.58168\n",
      "Epoch: 00 [ 7724/20101 ( 38%)], Train Loss: 0.58060\n",
      "Epoch: 00 [ 7764/20101 ( 39%)], Train Loss: 0.58041\n",
      "Epoch: 00 [ 7804/20101 ( 39%)], Train Loss: 0.57948\n",
      "Epoch: 00 [ 7844/20101 ( 39%)], Train Loss: 0.57843\n",
      "Epoch: 00 [ 7884/20101 ( 39%)], Train Loss: 0.57700\n",
      "Epoch: 00 [ 7924/20101 ( 39%)], Train Loss: 0.57608\n",
      "Epoch: 00 [ 7964/20101 ( 40%)], Train Loss: 0.57588\n",
      "Epoch: 00 [ 8004/20101 ( 40%)], Train Loss: 0.57499\n",
      "Epoch: 00 [ 8044/20101 ( 40%)], Train Loss: 0.57405\n",
      "Epoch: 00 [ 8084/20101 ( 40%)], Train Loss: 0.57283\n",
      "Epoch: 00 [ 8124/20101 ( 40%)], Train Loss: 0.57213\n",
      "Epoch: 00 [ 8164/20101 ( 41%)], Train Loss: 0.57117\n",
      "Epoch: 00 [ 8204/20101 ( 41%)], Train Loss: 0.57038\n",
      "Epoch: 00 [ 8244/20101 ( 41%)], Train Loss: 0.57043\n",
      "Epoch: 00 [ 8284/20101 ( 41%)], Train Loss: 0.56997\n",
      "Epoch: 00 [ 8324/20101 ( 41%)], Train Loss: 0.56845\n",
      "Epoch: 00 [ 8364/20101 ( 42%)], Train Loss: 0.56801\n",
      "Epoch: 00 [ 8404/20101 ( 42%)], Train Loss: 0.56736\n",
      "Epoch: 00 [ 8444/20101 ( 42%)], Train Loss: 0.56791\n",
      "Epoch: 00 [ 8484/20101 ( 42%)], Train Loss: 0.56743\n",
      "Epoch: 00 [ 8524/20101 ( 42%)], Train Loss: 0.56599\n",
      "Epoch: 00 [ 8564/20101 ( 43%)], Train Loss: 0.56537\n",
      "Epoch: 00 [ 8604/20101 ( 43%)], Train Loss: 0.56518\n",
      "Epoch: 00 [ 8644/20101 ( 43%)], Train Loss: 0.56441\n",
      "Epoch: 00 [ 8684/20101 ( 43%)], Train Loss: 0.56424\n",
      "Epoch: 00 [ 8724/20101 ( 43%)], Train Loss: 0.56338\n",
      "Epoch: 00 [ 8764/20101 ( 44%)], Train Loss: 0.56295\n",
      "Epoch: 00 [ 8804/20101 ( 44%)], Train Loss: 0.56281\n",
      "Epoch: 00 [ 8844/20101 ( 44%)], Train Loss: 0.56256\n",
      "Epoch: 00 [ 8884/20101 ( 44%)], Train Loss: 0.56123\n",
      "Epoch: 00 [ 8924/20101 ( 44%)], Train Loss: 0.56095\n",
      "Epoch: 00 [ 8964/20101 ( 45%)], Train Loss: 0.55977\n",
      "Epoch: 00 [ 9004/20101 ( 45%)], Train Loss: 0.55891\n",
      "Epoch: 00 [ 9044/20101 ( 45%)], Train Loss: 0.55813\n",
      "Epoch: 00 [ 9084/20101 ( 45%)], Train Loss: 0.55686\n",
      "Epoch: 00 [ 9124/20101 ( 45%)], Train Loss: 0.55620\n",
      "Epoch: 00 [ 9164/20101 ( 46%)], Train Loss: 0.55529\n",
      "Epoch: 00 [ 9204/20101 ( 46%)], Train Loss: 0.55516\n",
      "Epoch: 00 [ 9244/20101 ( 46%)], Train Loss: 0.55409\n",
      "Epoch: 00 [ 9284/20101 ( 46%)], Train Loss: 0.55285\n",
      "Epoch: 00 [ 9324/20101 ( 46%)], Train Loss: 0.55276\n",
      "Epoch: 00 [ 9364/20101 ( 47%)], Train Loss: 0.55286\n",
      "Epoch: 00 [ 9404/20101 ( 47%)], Train Loss: 0.55255\n",
      "Epoch: 00 [ 9444/20101 ( 47%)], Train Loss: 0.55183\n",
      "Epoch: 00 [ 9484/20101 ( 47%)], Train Loss: 0.55162\n",
      "Epoch: 00 [ 9524/20101 ( 47%)], Train Loss: 0.55030\n",
      "Epoch: 00 [ 9564/20101 ( 48%)], Train Loss: 0.54982\n",
      "Epoch: 00 [ 9604/20101 ( 48%)], Train Loss: 0.54917\n",
      "Epoch: 00 [ 9644/20101 ( 48%)], Train Loss: 0.54909\n",
      "Epoch: 00 [ 9684/20101 ( 48%)], Train Loss: 0.54833\n",
      "Epoch: 00 [ 9724/20101 ( 48%)], Train Loss: 0.54733\n",
      "Epoch: 00 [ 9764/20101 ( 49%)], Train Loss: 0.54632\n",
      "Epoch: 00 [ 9804/20101 ( 49%)], Train Loss: 0.54533\n",
      "Epoch: 00 [ 9844/20101 ( 49%)], Train Loss: 0.54470\n",
      "Epoch: 00 [ 9884/20101 ( 49%)], Train Loss: 0.54442\n",
      "Epoch: 00 [ 9924/20101 ( 49%)], Train Loss: 0.54311\n",
      "Epoch: 00 [ 9964/20101 ( 50%)], Train Loss: 0.54233\n",
      "Epoch: 00 [10004/20101 ( 50%)], Train Loss: 0.54147\n",
      "Epoch: 00 [10044/20101 ( 50%)], Train Loss: 0.54160\n",
      "Epoch: 00 [10084/20101 ( 50%)], Train Loss: 0.54125\n",
      "Epoch: 00 [10124/20101 ( 50%)], Train Loss: 0.53987\n",
      "Epoch: 00 [10164/20101 ( 51%)], Train Loss: 0.53860\n",
      "Epoch: 00 [10204/20101 ( 51%)], Train Loss: 0.53738\n",
      "Epoch: 00 [10244/20101 ( 51%)], Train Loss: 0.53646\n",
      "Epoch: 00 [10284/20101 ( 51%)], Train Loss: 0.53703\n",
      "Epoch: 00 [10324/20101 ( 51%)], Train Loss: 0.53603\n",
      "Epoch: 00 [10364/20101 ( 52%)], Train Loss: 0.53549\n",
      "Epoch: 00 [10404/20101 ( 52%)], Train Loss: 0.53466\n",
      "Epoch: 00 [10444/20101 ( 52%)], Train Loss: 0.53464\n",
      "Epoch: 00 [10484/20101 ( 52%)], Train Loss: 0.53462\n",
      "Epoch: 00 [10524/20101 ( 52%)], Train Loss: 0.53380\n",
      "Epoch: 00 [10564/20101 ( 53%)], Train Loss: 0.53347\n",
      "Epoch: 00 [10604/20101 ( 53%)], Train Loss: 0.53279\n",
      "Epoch: 00 [10644/20101 ( 53%)], Train Loss: 0.53147\n",
      "Epoch: 00 [10684/20101 ( 53%)], Train Loss: 0.53147\n",
      "Epoch: 00 [10724/20101 ( 53%)], Train Loss: 0.53066\n",
      "Epoch: 00 [10764/20101 ( 54%)], Train Loss: 0.53062\n",
      "Epoch: 00 [10804/20101 ( 54%)], Train Loss: 0.53010\n",
      "Epoch: 00 [10844/20101 ( 54%)], Train Loss: 0.52958\n",
      "Epoch: 00 [10884/20101 ( 54%)], Train Loss: 0.52863\n",
      "Epoch: 00 [10924/20101 ( 54%)], Train Loss: 0.52768\n",
      "Epoch: 00 [10964/20101 ( 55%)], Train Loss: 0.52747\n",
      "Epoch: 00 [11004/20101 ( 55%)], Train Loss: 0.52692\n",
      "Epoch: 00 [11044/20101 ( 55%)], Train Loss: 0.52681\n",
      "Epoch: 00 [11084/20101 ( 55%)], Train Loss: 0.52733\n",
      "Epoch: 00 [11124/20101 ( 55%)], Train Loss: 0.52710\n",
      "Epoch: 00 [11164/20101 ( 56%)], Train Loss: 0.52673\n",
      "Epoch: 00 [11204/20101 ( 56%)], Train Loss: 0.52620\n",
      "Epoch: 00 [11244/20101 ( 56%)], Train Loss: 0.52582\n",
      "Epoch: 00 [11284/20101 ( 56%)], Train Loss: 0.52515\n",
      "Epoch: 00 [11324/20101 ( 56%)], Train Loss: 0.52465\n",
      "Epoch: 00 [11364/20101 ( 57%)], Train Loss: 0.52407\n",
      "Epoch: 00 [11404/20101 ( 57%)], Train Loss: 0.52377\n",
      "Epoch: 00 [11444/20101 ( 57%)], Train Loss: 0.52302\n",
      "Epoch: 00 [11484/20101 ( 57%)], Train Loss: 0.52284\n",
      "Epoch: 00 [11524/20101 ( 57%)], Train Loss: 0.52228\n",
      "Epoch: 00 [11564/20101 ( 58%)], Train Loss: 0.52145\n",
      "Epoch: 00 [11604/20101 ( 58%)], Train Loss: 0.52075\n",
      "Epoch: 00 [11644/20101 ( 58%)], Train Loss: 0.52013\n",
      "Epoch: 00 [11684/20101 ( 58%)], Train Loss: 0.51934\n",
      "Epoch: 00 [11724/20101 ( 58%)], Train Loss: 0.51911\n",
      "Epoch: 00 [11764/20101 ( 59%)], Train Loss: 0.51888\n",
      "Epoch: 00 [11804/20101 ( 59%)], Train Loss: 0.51814\n",
      "Epoch: 00 [11844/20101 ( 59%)], Train Loss: 0.51765\n",
      "Epoch: 00 [11884/20101 ( 59%)], Train Loss: 0.51707\n",
      "Epoch: 00 [11924/20101 ( 59%)], Train Loss: 0.51622\n",
      "Epoch: 00 [11964/20101 ( 60%)], Train Loss: 0.51544\n",
      "Epoch: 00 [12004/20101 ( 60%)], Train Loss: 0.51528\n",
      "Epoch: 00 [12044/20101 ( 60%)], Train Loss: 0.51457\n",
      "Epoch: 00 [12084/20101 ( 60%)], Train Loss: 0.51365\n",
      "Epoch: 00 [12124/20101 ( 60%)], Train Loss: 0.51343\n",
      "Epoch: 00 [12164/20101 ( 61%)], Train Loss: 0.51249\n",
      "Epoch: 00 [12204/20101 ( 61%)], Train Loss: 0.51141\n",
      "Epoch: 00 [12244/20101 ( 61%)], Train Loss: 0.51081\n",
      "Epoch: 00 [12284/20101 ( 61%)], Train Loss: 0.51021\n",
      "Epoch: 00 [12324/20101 ( 61%)], Train Loss: 0.50989\n",
      "Epoch: 00 [12364/20101 ( 62%)], Train Loss: 0.50951\n",
      "Epoch: 00 [12404/20101 ( 62%)], Train Loss: 0.50907\n",
      "Epoch: 00 [12444/20101 ( 62%)], Train Loss: 0.50849\n",
      "Epoch: 00 [12484/20101 ( 62%)], Train Loss: 0.50787\n",
      "Epoch: 00 [12524/20101 ( 62%)], Train Loss: 0.50726\n",
      "Epoch: 00 [12564/20101 ( 63%)], Train Loss: 0.50623\n",
      "Epoch: 00 [12604/20101 ( 63%)], Train Loss: 0.50565\n",
      "Epoch: 00 [12644/20101 ( 63%)], Train Loss: 0.50547\n",
      "Epoch: 00 [12684/20101 ( 63%)], Train Loss: 0.50507\n",
      "Epoch: 00 [12724/20101 ( 63%)], Train Loss: 0.50524\n",
      "Epoch: 00 [12764/20101 ( 63%)], Train Loss: 0.50469\n",
      "Epoch: 00 [12804/20101 ( 64%)], Train Loss: 0.50376\n",
      "Epoch: 00 [12844/20101 ( 64%)], Train Loss: 0.50324\n",
      "Epoch: 00 [12884/20101 ( 64%)], Train Loss: 0.50269\n",
      "Epoch: 00 [12924/20101 ( 64%)], Train Loss: 0.50239\n",
      "Epoch: 00 [12964/20101 ( 64%)], Train Loss: 0.50271\n",
      "Epoch: 00 [13004/20101 ( 65%)], Train Loss: 0.50166\n",
      "Epoch: 00 [13044/20101 ( 65%)], Train Loss: 0.50136\n",
      "Epoch: 00 [13084/20101 ( 65%)], Train Loss: 0.50063\n",
      "Epoch: 00 [13124/20101 ( 65%)], Train Loss: 0.50013\n",
      "Epoch: 00 [13164/20101 ( 65%)], Train Loss: 0.50040\n",
      "Epoch: 00 [13204/20101 ( 66%)], Train Loss: 0.50056\n",
      "Epoch: 00 [13244/20101 ( 66%)], Train Loss: 0.49973\n",
      "Epoch: 00 [13284/20101 ( 66%)], Train Loss: 0.49894\n",
      "Epoch: 00 [13324/20101 ( 66%)], Train Loss: 0.49923\n",
      "Epoch: 00 [13364/20101 ( 66%)], Train Loss: 0.49872\n",
      "Epoch: 00 [13404/20101 ( 67%)], Train Loss: 0.49818\n",
      "Epoch: 00 [13444/20101 ( 67%)], Train Loss: 0.49811\n",
      "Epoch: 00 [13484/20101 ( 67%)], Train Loss: 0.49783\n",
      "Epoch: 00 [13524/20101 ( 67%)], Train Loss: 0.49681\n",
      "Epoch: 00 [13564/20101 ( 67%)], Train Loss: 0.49705\n",
      "Epoch: 00 [13604/20101 ( 68%)], Train Loss: 0.49675\n",
      "Epoch: 00 [13644/20101 ( 68%)], Train Loss: 0.49658\n",
      "Epoch: 00 [13684/20101 ( 68%)], Train Loss: 0.49611\n",
      "Epoch: 00 [13724/20101 ( 68%)], Train Loss: 0.49571\n",
      "Epoch: 00 [13764/20101 ( 68%)], Train Loss: 0.49555\n",
      "Epoch: 00 [13804/20101 ( 69%)], Train Loss: 0.49544\n",
      "Epoch: 00 [13844/20101 ( 69%)], Train Loss: 0.49579\n",
      "Epoch: 00 [13884/20101 ( 69%)], Train Loss: 0.49545\n",
      "Epoch: 00 [13924/20101 ( 69%)], Train Loss: 0.49492\n",
      "Epoch: 00 [13964/20101 ( 69%)], Train Loss: 0.49451\n",
      "Epoch: 00 [14004/20101 ( 70%)], Train Loss: 0.49344\n",
      "Epoch: 00 [14044/20101 ( 70%)], Train Loss: 0.49341\n",
      "Epoch: 00 [14084/20101 ( 70%)], Train Loss: 0.49299\n",
      "Epoch: 00 [14124/20101 ( 70%)], Train Loss: 0.49247\n",
      "Epoch: 00 [14164/20101 ( 70%)], Train Loss: 0.49159\n",
      "Epoch: 00 [14204/20101 ( 71%)], Train Loss: 0.49127\n",
      "Epoch: 00 [14244/20101 ( 71%)], Train Loss: 0.49111\n",
      "Epoch: 00 [14284/20101 ( 71%)], Train Loss: 0.49053\n",
      "Epoch: 00 [14324/20101 ( 71%)], Train Loss: 0.49023\n",
      "Epoch: 00 [14364/20101 ( 71%)], Train Loss: 0.48998\n",
      "Epoch: 00 [14404/20101 ( 72%)], Train Loss: 0.48969\n",
      "Epoch: 00 [14444/20101 ( 72%)], Train Loss: 0.48987\n",
      "Epoch: 00 [14484/20101 ( 72%)], Train Loss: 0.48936\n",
      "Epoch: 00 [14524/20101 ( 72%)], Train Loss: 0.48883\n",
      "Epoch: 00 [14564/20101 ( 72%)], Train Loss: 0.48838\n",
      "Epoch: 00 [14604/20101 ( 73%)], Train Loss: 0.48824\n",
      "Epoch: 00 [14644/20101 ( 73%)], Train Loss: 0.48842\n",
      "Epoch: 00 [14684/20101 ( 73%)], Train Loss: 0.48788\n",
      "Epoch: 00 [14724/20101 ( 73%)], Train Loss: 0.48761\n",
      "Epoch: 00 [14764/20101 ( 73%)], Train Loss: 0.48757\n",
      "Epoch: 00 [14804/20101 ( 74%)], Train Loss: 0.48675\n",
      "Epoch: 00 [14844/20101 ( 74%)], Train Loss: 0.48602\n",
      "Epoch: 00 [14884/20101 ( 74%)], Train Loss: 0.48535\n",
      "Epoch: 00 [14924/20101 ( 74%)], Train Loss: 0.48567\n",
      "Epoch: 00 [14964/20101 ( 74%)], Train Loss: 0.48549\n",
      "Epoch: 00 [15004/20101 ( 75%)], Train Loss: 0.48480\n",
      "Epoch: 00 [15044/20101 ( 75%)], Train Loss: 0.48481\n",
      "Epoch: 00 [15084/20101 ( 75%)], Train Loss: 0.48434\n",
      "Epoch: 00 [15124/20101 ( 75%)], Train Loss: 0.48443\n",
      "Epoch: 00 [15164/20101 ( 75%)], Train Loss: 0.48436\n",
      "Epoch: 00 [15204/20101 ( 76%)], Train Loss: 0.48471\n",
      "Epoch: 00 [15244/20101 ( 76%)], Train Loss: 0.48456\n",
      "Epoch: 00 [15284/20101 ( 76%)], Train Loss: 0.48418\n",
      "Epoch: 00 [15324/20101 ( 76%)], Train Loss: 0.48362\n",
      "Epoch: 00 [15364/20101 ( 76%)], Train Loss: 0.48374\n",
      "Epoch: 00 [15404/20101 ( 77%)], Train Loss: 0.48326\n",
      "Epoch: 00 [15444/20101 ( 77%)], Train Loss: 0.48302\n",
      "Epoch: 00 [15484/20101 ( 77%)], Train Loss: 0.48266\n",
      "Epoch: 00 [15524/20101 ( 77%)], Train Loss: 0.48216\n",
      "Epoch: 00 [15564/20101 ( 77%)], Train Loss: 0.48116\n",
      "Epoch: 00 [15604/20101 ( 78%)], Train Loss: 0.48079\n",
      "Epoch: 00 [15644/20101 ( 78%)], Train Loss: 0.48009\n",
      "Epoch: 00 [15684/20101 ( 78%)], Train Loss: 0.47980\n",
      "Epoch: 00 [15724/20101 ( 78%)], Train Loss: 0.47972\n",
      "Epoch: 00 [15764/20101 ( 78%)], Train Loss: 0.47956\n",
      "Epoch: 00 [15804/20101 ( 79%)], Train Loss: 0.47923\n",
      "Epoch: 00 [15844/20101 ( 79%)], Train Loss: 0.47869\n",
      "Epoch: 00 [15884/20101 ( 79%)], Train Loss: 0.47833\n",
      "Epoch: 00 [15924/20101 ( 79%)], Train Loss: 0.47774\n",
      "Epoch: 00 [15964/20101 ( 79%)], Train Loss: 0.47783\n",
      "Epoch: 00 [16004/20101 ( 80%)], Train Loss: 0.47743\n",
      "Epoch: 00 [16044/20101 ( 80%)], Train Loss: 0.47684\n",
      "Epoch: 00 [16084/20101 ( 80%)], Train Loss: 0.47680\n",
      "Epoch: 00 [16124/20101 ( 80%)], Train Loss: 0.47617\n",
      "Epoch: 00 [16164/20101 ( 80%)], Train Loss: 0.47592\n",
      "Epoch: 00 [16204/20101 ( 81%)], Train Loss: 0.47522\n",
      "Epoch: 00 [16244/20101 ( 81%)], Train Loss: 0.47536\n",
      "Epoch: 00 [16284/20101 ( 81%)], Train Loss: 0.47515\n",
      "Epoch: 00 [16324/20101 ( 81%)], Train Loss: 0.47484\n",
      "Epoch: 00 [16364/20101 ( 81%)], Train Loss: 0.47502\n",
      "Epoch: 00 [16404/20101 ( 82%)], Train Loss: 0.47560\n",
      "Epoch: 00 [16444/20101 ( 82%)], Train Loss: 0.47536\n",
      "Epoch: 00 [16484/20101 ( 82%)], Train Loss: 0.47481\n",
      "Epoch: 00 [16524/20101 ( 82%)], Train Loss: 0.47455\n",
      "Epoch: 00 [16564/20101 ( 82%)], Train Loss: 0.47447\n",
      "Epoch: 00 [16604/20101 ( 83%)], Train Loss: 0.47460\n",
      "Epoch: 00 [16644/20101 ( 83%)], Train Loss: 0.47441\n",
      "Epoch: 00 [16684/20101 ( 83%)], Train Loss: 0.47402\n",
      "Epoch: 00 [16724/20101 ( 83%)], Train Loss: 0.47330\n",
      "Epoch: 00 [16764/20101 ( 83%)], Train Loss: 0.47299\n",
      "Epoch: 00 [16804/20101 ( 84%)], Train Loss: 0.47299\n",
      "Epoch: 00 [16844/20101 ( 84%)], Train Loss: 0.47265\n",
      "Epoch: 00 [16884/20101 ( 84%)], Train Loss: 0.47223\n",
      "Epoch: 00 [16924/20101 ( 84%)], Train Loss: 0.47215\n",
      "Epoch: 00 [16964/20101 ( 84%)], Train Loss: 0.47157\n",
      "Epoch: 00 [17004/20101 ( 85%)], Train Loss: 0.47170\n",
      "Epoch: 00 [17044/20101 ( 85%)], Train Loss: 0.47167\n",
      "Epoch: 00 [17084/20101 ( 85%)], Train Loss: 0.47117\n",
      "Epoch: 00 [17124/20101 ( 85%)], Train Loss: 0.47058\n",
      "Epoch: 00 [17164/20101 ( 85%)], Train Loss: 0.47014\n",
      "Epoch: 00 [17204/20101 ( 86%)], Train Loss: 0.46977\n",
      "Epoch: 00 [17244/20101 ( 86%)], Train Loss: 0.46964\n",
      "Epoch: 00 [17284/20101 ( 86%)], Train Loss: 0.46935\n",
      "Epoch: 00 [17324/20101 ( 86%)], Train Loss: 0.46882\n",
      "Epoch: 00 [17364/20101 ( 86%)], Train Loss: 0.46841\n",
      "Epoch: 00 [17404/20101 ( 87%)], Train Loss: 0.46812\n",
      "Epoch: 00 [17444/20101 ( 87%)], Train Loss: 0.46751\n",
      "Epoch: 00 [17484/20101 ( 87%)], Train Loss: 0.46692\n",
      "Epoch: 00 [17524/20101 ( 87%)], Train Loss: 0.46668\n",
      "Epoch: 00 [17564/20101 ( 87%)], Train Loss: 0.46636\n",
      "Epoch: 00 [17604/20101 ( 88%)], Train Loss: 0.46639\n",
      "Epoch: 00 [17644/20101 ( 88%)], Train Loss: 0.46610\n",
      "Epoch: 00 [17684/20101 ( 88%)], Train Loss: 0.46633\n",
      "Epoch: 00 [17724/20101 ( 88%)], Train Loss: 0.46599\n",
      "Epoch: 00 [17764/20101 ( 88%)], Train Loss: 0.46573\n",
      "Epoch: 00 [17804/20101 ( 89%)], Train Loss: 0.46551\n",
      "Epoch: 00 [17844/20101 ( 89%)], Train Loss: 0.46492\n",
      "Epoch: 00 [17884/20101 ( 89%)], Train Loss: 0.46497\n",
      "Epoch: 00 [17924/20101 ( 89%)], Train Loss: 0.46433\n",
      "Epoch: 00 [17964/20101 ( 89%)], Train Loss: 0.46414\n",
      "Epoch: 00 [18004/20101 ( 90%)], Train Loss: 0.46382\n",
      "Epoch: 00 [18044/20101 ( 90%)], Train Loss: 0.46353\n",
      "Epoch: 00 [18084/20101 ( 90%)], Train Loss: 0.46347\n",
      "Epoch: 00 [18124/20101 ( 90%)], Train Loss: 0.46327\n",
      "Epoch: 00 [18164/20101 ( 90%)], Train Loss: 0.46301\n",
      "Epoch: 00 [18204/20101 ( 91%)], Train Loss: 0.46248\n",
      "Epoch: 00 [18244/20101 ( 91%)], Train Loss: 0.46222\n",
      "Epoch: 00 [18284/20101 ( 91%)], Train Loss: 0.46243\n",
      "Epoch: 00 [18324/20101 ( 91%)], Train Loss: 0.46205\n",
      "Epoch: 00 [18364/20101 ( 91%)], Train Loss: 0.46186\n",
      "Epoch: 00 [18404/20101 ( 92%)], Train Loss: 0.46192\n",
      "Epoch: 00 [18444/20101 ( 92%)], Train Loss: 0.46155\n",
      "Epoch: 00 [18484/20101 ( 92%)], Train Loss: 0.46094\n",
      "Epoch: 00 [18524/20101 ( 92%)], Train Loss: 0.46095\n",
      "Epoch: 00 [18564/20101 ( 92%)], Train Loss: 0.46073\n",
      "Epoch: 00 [18604/20101 ( 93%)], Train Loss: 0.46045\n",
      "Epoch: 00 [18644/20101 ( 93%)], Train Loss: 0.46019\n",
      "Epoch: 00 [18684/20101 ( 93%)], Train Loss: 0.45981\n",
      "Epoch: 00 [18724/20101 ( 93%)], Train Loss: 0.45952\n",
      "Epoch: 00 [18764/20101 ( 93%)], Train Loss: 0.45968\n",
      "Epoch: 00 [18804/20101 ( 94%)], Train Loss: 0.46013\n",
      "Epoch: 00 [18844/20101 ( 94%)], Train Loss: 0.45997\n",
      "Epoch: 00 [18884/20101 ( 94%)], Train Loss: 0.45974\n",
      "Epoch: 00 [18924/20101 ( 94%)], Train Loss: 0.45960\n",
      "Epoch: 00 [18964/20101 ( 94%)], Train Loss: 0.45936\n",
      "Epoch: 00 [19004/20101 ( 95%)], Train Loss: 0.45893\n",
      "Epoch: 00 [19044/20101 ( 95%)], Train Loss: 0.45867\n",
      "Epoch: 00 [19084/20101 ( 95%)], Train Loss: 0.45828\n",
      "Epoch: 00 [19124/20101 ( 95%)], Train Loss: 0.45805\n",
      "Epoch: 00 [19164/20101 ( 95%)], Train Loss: 0.45752\n",
      "Epoch: 00 [19204/20101 ( 96%)], Train Loss: 0.45719\n",
      "Epoch: 00 [19244/20101 ( 96%)], Train Loss: 0.45700\n",
      "Epoch: 00 [19284/20101 ( 96%)], Train Loss: 0.45639\n",
      "Epoch: 00 [19324/20101 ( 96%)], Train Loss: 0.45652\n",
      "Epoch: 00 [19364/20101 ( 96%)], Train Loss: 0.45638\n",
      "Epoch: 00 [19404/20101 ( 97%)], Train Loss: 0.45622\n",
      "Epoch: 00 [19444/20101 ( 97%)], Train Loss: 0.45618\n",
      "Epoch: 00 [19484/20101 ( 97%)], Train Loss: 0.45626\n",
      "Epoch: 00 [19524/20101 ( 97%)], Train Loss: 0.45678\n",
      "Epoch: 00 [19564/20101 ( 97%)], Train Loss: 0.45656\n",
      "Epoch: 00 [19604/20101 ( 98%)], Train Loss: 0.45624\n",
      "Epoch: 00 [19644/20101 ( 98%)], Train Loss: 0.45586\n",
      "Epoch: 00 [19684/20101 ( 98%)], Train Loss: 0.45548\n",
      "Epoch: 00 [19724/20101 ( 98%)], Train Loss: 0.45535\n",
      "Epoch: 00 [19764/20101 ( 98%)], Train Loss: 0.45483\n",
      "Epoch: 00 [19804/20101 ( 99%)], Train Loss: 0.45471\n",
      "Epoch: 00 [19844/20101 ( 99%)], Train Loss: 0.45462\n",
      "Epoch: 00 [19884/20101 ( 99%)], Train Loss: 0.45442\n",
      "Epoch: 00 [19924/20101 ( 99%)], Train Loss: 0.45421\n",
      "Epoch: 00 [19964/20101 ( 99%)], Train Loss: 0.45385\n",
      "Epoch: 00 [20004/20101 (100%)], Train Loss: 0.45346\n",
      "Epoch: 00 [20044/20101 (100%)], Train Loss: 0.45337\n",
      "Epoch: 00 [20084/20101 (100%)], Train Loss: 0.45318\n",
      "Epoch: 00 [20101/20101 (100%)], Train Loss: 0.45292\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.21587\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.21587\n",
      "Saving model checkpoint to output/checkpoint-fold-3.\n",
      "\n",
      "Epoch: 01 [    4/20101 (  0%)], Train Loss: 0.31047\n",
      "Epoch: 01 [   44/20101 (  0%)], Train Loss: 0.30172\n",
      "Epoch: 01 [   84/20101 (  0%)], Train Loss: 0.46895\n",
      "Epoch: 01 [  124/20101 (  1%)], Train Loss: 0.48135\n",
      "Epoch: 01 [  164/20101 (  1%)], Train Loss: 0.43275\n",
      "Epoch: 01 [  204/20101 (  1%)], Train Loss: 0.39057\n",
      "Epoch: 01 [  244/20101 (  1%)], Train Loss: 0.39441\n",
      "Epoch: 01 [  284/20101 (  1%)], Train Loss: 0.40352\n",
      "Epoch: 01 [  324/20101 (  2%)], Train Loss: 0.39028\n",
      "Epoch: 01 [  364/20101 (  2%)], Train Loss: 0.38736\n",
      "Epoch: 01 [  404/20101 (  2%)], Train Loss: 0.38366\n",
      "Epoch: 01 [  444/20101 (  2%)], Train Loss: 0.39074\n",
      "Epoch: 01 [  484/20101 (  2%)], Train Loss: 0.40046\n",
      "Epoch: 01 [  524/20101 (  3%)], Train Loss: 0.40365\n",
      "Epoch: 01 [  564/20101 (  3%)], Train Loss: 0.39397\n",
      "Epoch: 01 [  604/20101 (  3%)], Train Loss: 0.38697\n",
      "Epoch: 01 [  644/20101 (  3%)], Train Loss: 0.38481\n",
      "Epoch: 01 [  684/20101 (  3%)], Train Loss: 0.38182\n",
      "Epoch: 01 [  724/20101 (  4%)], Train Loss: 0.37763\n",
      "Epoch: 01 [  764/20101 (  4%)], Train Loss: 0.37286\n",
      "Epoch: 01 [  804/20101 (  4%)], Train Loss: 0.36669\n",
      "Epoch: 01 [  844/20101 (  4%)], Train Loss: 0.36167\n",
      "Epoch: 01 [  884/20101 (  4%)], Train Loss: 0.35739\n",
      "Epoch: 01 [  924/20101 (  5%)], Train Loss: 0.35131\n",
      "Epoch: 01 [  964/20101 (  5%)], Train Loss: 0.34396\n",
      "Epoch: 01 [ 1004/20101 (  5%)], Train Loss: 0.33904\n",
      "Epoch: 01 [ 1044/20101 (  5%)], Train Loss: 0.33770\n",
      "Epoch: 01 [ 1084/20101 (  5%)], Train Loss: 0.33604\n",
      "Epoch: 01 [ 1124/20101 (  6%)], Train Loss: 0.33160\n",
      "Epoch: 01 [ 1164/20101 (  6%)], Train Loss: 0.32483\n",
      "Epoch: 01 [ 1204/20101 (  6%)], Train Loss: 0.31992\n",
      "Epoch: 01 [ 1244/20101 (  6%)], Train Loss: 0.32075\n",
      "Epoch: 01 [ 1284/20101 (  6%)], Train Loss: 0.32357\n",
      "Epoch: 01 [ 1324/20101 (  7%)], Train Loss: 0.32186\n",
      "Epoch: 01 [ 1364/20101 (  7%)], Train Loss: 0.31856\n",
      "Epoch: 01 [ 1404/20101 (  7%)], Train Loss: 0.31775\n",
      "Epoch: 01 [ 1444/20101 (  7%)], Train Loss: 0.31662\n",
      "Epoch: 01 [ 1484/20101 (  7%)], Train Loss: 0.31644\n",
      "Epoch: 01 [ 1524/20101 (  8%)], Train Loss: 0.31005\n",
      "Epoch: 01 [ 1564/20101 (  8%)], Train Loss: 0.30477\n",
      "Epoch: 01 [ 1604/20101 (  8%)], Train Loss: 0.30330\n",
      "Epoch: 01 [ 1644/20101 (  8%)], Train Loss: 0.30074\n",
      "Epoch: 01 [ 1684/20101 (  8%)], Train Loss: 0.29794\n",
      "Epoch: 01 [ 1724/20101 (  9%)], Train Loss: 0.29783\n",
      "Epoch: 01 [ 1764/20101 (  9%)], Train Loss: 0.29564\n",
      "Epoch: 01 [ 1804/20101 (  9%)], Train Loss: 0.29173\n",
      "Epoch: 01 [ 1844/20101 (  9%)], Train Loss: 0.28968\n",
      "Epoch: 01 [ 1884/20101 (  9%)], Train Loss: 0.28749\n",
      "Epoch: 01 [ 1924/20101 ( 10%)], Train Loss: 0.28373\n",
      "Epoch: 01 [ 1964/20101 ( 10%)], Train Loss: 0.28228\n",
      "Epoch: 01 [ 2004/20101 ( 10%)], Train Loss: 0.28220\n",
      "Epoch: 01 [ 2044/20101 ( 10%)], Train Loss: 0.28194\n",
      "Epoch: 01 [ 2084/20101 ( 10%)], Train Loss: 0.28282\n",
      "Epoch: 01 [ 2124/20101 ( 11%)], Train Loss: 0.28276\n",
      "Epoch: 01 [ 2164/20101 ( 11%)], Train Loss: 0.28309\n",
      "Epoch: 01 [ 2204/20101 ( 11%)], Train Loss: 0.28282\n",
      "Epoch: 01 [ 2244/20101 ( 11%)], Train Loss: 0.28040\n",
      "Epoch: 01 [ 2284/20101 ( 11%)], Train Loss: 0.27997\n",
      "Epoch: 01 [ 2324/20101 ( 12%)], Train Loss: 0.28097\n",
      "Epoch: 01 [ 2364/20101 ( 12%)], Train Loss: 0.27854\n",
      "Epoch: 01 [ 2404/20101 ( 12%)], Train Loss: 0.27608\n",
      "Epoch: 01 [ 2444/20101 ( 12%)], Train Loss: 0.27470\n",
      "Epoch: 01 [ 2484/20101 ( 12%)], Train Loss: 0.27353\n",
      "Epoch: 01 [ 2524/20101 ( 13%)], Train Loss: 0.27287\n",
      "Epoch: 01 [ 2564/20101 ( 13%)], Train Loss: 0.27162\n",
      "Epoch: 01 [ 2604/20101 ( 13%)], Train Loss: 0.26947\n",
      "Epoch: 01 [ 2644/20101 ( 13%)], Train Loss: 0.26827\n",
      "Epoch: 01 [ 2684/20101 ( 13%)], Train Loss: 0.26682\n",
      "Epoch: 01 [ 2724/20101 ( 14%)], Train Loss: 0.26526\n",
      "Epoch: 01 [ 2764/20101 ( 14%)], Train Loss: 0.26281\n",
      "Epoch: 01 [ 2804/20101 ( 14%)], Train Loss: 0.26226\n",
      "Epoch: 01 [ 2844/20101 ( 14%)], Train Loss: 0.26232\n",
      "Epoch: 01 [ 2884/20101 ( 14%)], Train Loss: 0.26064\n",
      "Epoch: 01 [ 2924/20101 ( 15%)], Train Loss: 0.25819\n",
      "Epoch: 01 [ 2964/20101 ( 15%)], Train Loss: 0.25802\n",
      "Epoch: 01 [ 3004/20101 ( 15%)], Train Loss: 0.25664\n",
      "Epoch: 01 [ 3044/20101 ( 15%)], Train Loss: 0.25672\n",
      "Epoch: 01 [ 3084/20101 ( 15%)], Train Loss: 0.25615\n",
      "Epoch: 01 [ 3124/20101 ( 16%)], Train Loss: 0.25496\n",
      "Epoch: 01 [ 3164/20101 ( 16%)], Train Loss: 0.25529\n",
      "Epoch: 01 [ 3204/20101 ( 16%)], Train Loss: 0.25470\n",
      "Epoch: 01 [ 3244/20101 ( 16%)], Train Loss: 0.25482\n",
      "Epoch: 01 [ 3284/20101 ( 16%)], Train Loss: 0.25340\n",
      "Epoch: 01 [ 3324/20101 ( 17%)], Train Loss: 0.25183\n",
      "Epoch: 01 [ 3364/20101 ( 17%)], Train Loss: 0.25015\n",
      "Epoch: 01 [ 3404/20101 ( 17%)], Train Loss: 0.24846\n",
      "Epoch: 01 [ 3444/20101 ( 17%)], Train Loss: 0.24822\n",
      "Epoch: 01 [ 3484/20101 ( 17%)], Train Loss: 0.24664\n",
      "Epoch: 01 [ 3524/20101 ( 18%)], Train Loss: 0.24616\n",
      "Epoch: 01 [ 3564/20101 ( 18%)], Train Loss: 0.24462\n",
      "Epoch: 01 [ 3604/20101 ( 18%)], Train Loss: 0.24532\n",
      "Epoch: 01 [ 3644/20101 ( 18%)], Train Loss: 0.24506\n",
      "Epoch: 01 [ 3684/20101 ( 18%)], Train Loss: 0.24465\n",
      "Epoch: 01 [ 3724/20101 ( 19%)], Train Loss: 0.24376\n",
      "Epoch: 01 [ 3764/20101 ( 19%)], Train Loss: 0.24361\n",
      "Epoch: 01 [ 3804/20101 ( 19%)], Train Loss: 0.24282\n",
      "Epoch: 01 [ 3844/20101 ( 19%)], Train Loss: 0.24105\n",
      "Epoch: 01 [ 3884/20101 ( 19%)], Train Loss: 0.24006\n",
      "Epoch: 01 [ 3924/20101 ( 20%)], Train Loss: 0.23941\n",
      "Epoch: 01 [ 3964/20101 ( 20%)], Train Loss: 0.23839\n",
      "Epoch: 01 [ 4004/20101 ( 20%)], Train Loss: 0.23661\n",
      "Epoch: 01 [ 4044/20101 ( 20%)], Train Loss: 0.23635\n",
      "Epoch: 01 [ 4084/20101 ( 20%)], Train Loss: 0.23522\n",
      "Epoch: 01 [ 4124/20101 ( 21%)], Train Loss: 0.23586\n",
      "Epoch: 01 [ 4164/20101 ( 21%)], Train Loss: 0.23502\n",
      "Epoch: 01 [ 4204/20101 ( 21%)], Train Loss: 0.23393\n",
      "Epoch: 01 [ 4244/20101 ( 21%)], Train Loss: 0.23284\n",
      "Epoch: 01 [ 4284/20101 ( 21%)], Train Loss: 0.23174\n",
      "Epoch: 01 [ 4324/20101 ( 22%)], Train Loss: 0.23014\n",
      "Epoch: 01 [ 4364/20101 ( 22%)], Train Loss: 0.22912\n",
      "Epoch: 01 [ 4404/20101 ( 22%)], Train Loss: 0.22936\n",
      "Epoch: 01 [ 4444/20101 ( 22%)], Train Loss: 0.22986\n",
      "Epoch: 01 [ 4484/20101 ( 22%)], Train Loss: 0.23048\n",
      "Epoch: 01 [ 4524/20101 ( 23%)], Train Loss: 0.22919\n",
      "Epoch: 01 [ 4564/20101 ( 23%)], Train Loss: 0.22883\n",
      "Epoch: 01 [ 4604/20101 ( 23%)], Train Loss: 0.22859\n",
      "Epoch: 01 [ 4644/20101 ( 23%)], Train Loss: 0.22805\n",
      "Epoch: 01 [ 4684/20101 ( 23%)], Train Loss: 0.22748\n",
      "Epoch: 01 [ 4724/20101 ( 24%)], Train Loss: 0.22635\n",
      "Epoch: 01 [ 4764/20101 ( 24%)], Train Loss: 0.22593\n",
      "Epoch: 01 [ 4804/20101 ( 24%)], Train Loss: 0.22510\n",
      "Epoch: 01 [ 4844/20101 ( 24%)], Train Loss: 0.22371\n",
      "Epoch: 01 [ 4884/20101 ( 24%)], Train Loss: 0.22302\n",
      "Epoch: 01 [ 4924/20101 ( 24%)], Train Loss: 0.22206\n",
      "Epoch: 01 [ 4964/20101 ( 25%)], Train Loss: 0.22168\n",
      "Epoch: 01 [ 5004/20101 ( 25%)], Train Loss: 0.22120\n",
      "Epoch: 01 [ 5044/20101 ( 25%)], Train Loss: 0.22023\n",
      "Epoch: 01 [ 5084/20101 ( 25%)], Train Loss: 0.22057\n",
      "Epoch: 01 [ 5124/20101 ( 25%)], Train Loss: 0.21966\n",
      "Epoch: 01 [ 5164/20101 ( 26%)], Train Loss: 0.21955\n",
      "Epoch: 01 [ 5204/20101 ( 26%)], Train Loss: 0.22000\n",
      "Epoch: 01 [ 5244/20101 ( 26%)], Train Loss: 0.21860\n",
      "Epoch: 01 [ 5284/20101 ( 26%)], Train Loss: 0.21874\n",
      "Epoch: 01 [ 5324/20101 ( 26%)], Train Loss: 0.21869\n",
      "Epoch: 01 [ 5364/20101 ( 27%)], Train Loss: 0.21783\n",
      "Epoch: 01 [ 5404/20101 ( 27%)], Train Loss: 0.21709\n",
      "Epoch: 01 [ 5444/20101 ( 27%)], Train Loss: 0.21653\n",
      "Epoch: 01 [ 5484/20101 ( 27%)], Train Loss: 0.21635\n",
      "Epoch: 01 [ 5524/20101 ( 27%)], Train Loss: 0.21644\n",
      "Epoch: 01 [ 5564/20101 ( 28%)], Train Loss: 0.21544\n",
      "Epoch: 01 [ 5604/20101 ( 28%)], Train Loss: 0.21608\n",
      "Epoch: 01 [ 5644/20101 ( 28%)], Train Loss: 0.21550\n",
      "Epoch: 01 [ 5684/20101 ( 28%)], Train Loss: 0.21455\n",
      "Epoch: 01 [ 5724/20101 ( 28%)], Train Loss: 0.21449\n",
      "Epoch: 01 [ 5764/20101 ( 29%)], Train Loss: 0.21354\n",
      "Epoch: 01 [ 5804/20101 ( 29%)], Train Loss: 0.21371\n",
      "Epoch: 01 [ 5844/20101 ( 29%)], Train Loss: 0.21322\n",
      "Epoch: 01 [ 5884/20101 ( 29%)], Train Loss: 0.21243\n",
      "Epoch: 01 [ 5924/20101 ( 29%)], Train Loss: 0.21154\n",
      "Epoch: 01 [ 5964/20101 ( 30%)], Train Loss: 0.21026\n",
      "Epoch: 01 [ 6004/20101 ( 30%)], Train Loss: 0.20979\n",
      "Epoch: 01 [ 6044/20101 ( 30%)], Train Loss: 0.20906\n",
      "Epoch: 01 [ 6084/20101 ( 30%)], Train Loss: 0.20838\n",
      "Epoch: 01 [ 6124/20101 ( 30%)], Train Loss: 0.20735\n",
      "Epoch: 01 [ 6164/20101 ( 31%)], Train Loss: 0.20719\n",
      "Epoch: 01 [ 6204/20101 ( 31%)], Train Loss: 0.20678\n",
      "Epoch: 01 [ 6244/20101 ( 31%)], Train Loss: 0.20664\n",
      "Epoch: 01 [ 6284/20101 ( 31%)], Train Loss: 0.20634\n",
      "Epoch: 01 [ 6324/20101 ( 31%)], Train Loss: 0.20596\n",
      "Epoch: 01 [ 6364/20101 ( 32%)], Train Loss: 0.20579\n",
      "Epoch: 01 [ 6404/20101 ( 32%)], Train Loss: 0.20619\n",
      "Epoch: 01 [ 6444/20101 ( 32%)], Train Loss: 0.20599\n",
      "Epoch: 01 [ 6484/20101 ( 32%)], Train Loss: 0.20546\n",
      "Epoch: 01 [ 6524/20101 ( 32%)], Train Loss: 0.20445\n",
      "Epoch: 01 [ 6564/20101 ( 33%)], Train Loss: 0.20371\n",
      "Epoch: 01 [ 6604/20101 ( 33%)], Train Loss: 0.20280\n",
      "Epoch: 01 [ 6644/20101 ( 33%)], Train Loss: 0.20272\n",
      "Epoch: 01 [ 6684/20101 ( 33%)], Train Loss: 0.20202\n",
      "Epoch: 01 [ 6724/20101 ( 33%)], Train Loss: 0.20144\n",
      "Epoch: 01 [ 6764/20101 ( 34%)], Train Loss: 0.20125\n",
      "Epoch: 01 [ 6804/20101 ( 34%)], Train Loss: 0.20125\n",
      "Epoch: 01 [ 6844/20101 ( 34%)], Train Loss: 0.20097\n",
      "Epoch: 01 [ 6884/20101 ( 34%)], Train Loss: 0.20010\n",
      "Epoch: 01 [ 6924/20101 ( 34%)], Train Loss: 0.19953\n",
      "Epoch: 01 [ 6964/20101 ( 35%)], Train Loss: 0.19884\n",
      "Epoch: 01 [ 7004/20101 ( 35%)], Train Loss: 0.19810\n",
      "Epoch: 01 [ 7044/20101 ( 35%)], Train Loss: 0.19842\n",
      "Epoch: 01 [ 7084/20101 ( 35%)], Train Loss: 0.19761\n",
      "Epoch: 01 [ 7124/20101 ( 35%)], Train Loss: 0.19799\n",
      "Epoch: 01 [ 7164/20101 ( 36%)], Train Loss: 0.19702\n",
      "Epoch: 01 [ 7204/20101 ( 36%)], Train Loss: 0.19630\n",
      "Epoch: 01 [ 7244/20101 ( 36%)], Train Loss: 0.19560\n",
      "Epoch: 01 [ 7284/20101 ( 36%)], Train Loss: 0.19481\n",
      "Epoch: 01 [ 7324/20101 ( 36%)], Train Loss: 0.19414\n",
      "Epoch: 01 [ 7364/20101 ( 37%)], Train Loss: 0.19349\n",
      "Epoch: 01 [ 7404/20101 ( 37%)], Train Loss: 0.19287\n",
      "Epoch: 01 [ 7444/20101 ( 37%)], Train Loss: 0.19281\n",
      "Epoch: 01 [ 7484/20101 ( 37%)], Train Loss: 0.19273\n",
      "Epoch: 01 [ 7524/20101 ( 37%)], Train Loss: 0.19230\n",
      "Epoch: 01 [ 7564/20101 ( 38%)], Train Loss: 0.19191\n",
      "Epoch: 01 [ 7604/20101 ( 38%)], Train Loss: 0.19104\n",
      "Epoch: 01 [ 7644/20101 ( 38%)], Train Loss: 0.19064\n",
      "Epoch: 01 [ 7684/20101 ( 38%)], Train Loss: 0.19017\n",
      "Epoch: 01 [ 7724/20101 ( 38%)], Train Loss: 0.18961\n",
      "Epoch: 01 [ 7764/20101 ( 39%)], Train Loss: 0.18942\n",
      "Epoch: 01 [ 7804/20101 ( 39%)], Train Loss: 0.18888\n",
      "Epoch: 01 [ 7844/20101 ( 39%)], Train Loss: 0.18824\n",
      "Epoch: 01 [ 7884/20101 ( 39%)], Train Loss: 0.18760\n",
      "Epoch: 01 [ 7924/20101 ( 39%)], Train Loss: 0.18699\n",
      "Epoch: 01 [ 7964/20101 ( 40%)], Train Loss: 0.18675\n",
      "Epoch: 01 [ 8004/20101 ( 40%)], Train Loss: 0.18632\n",
      "Epoch: 01 [ 8044/20101 ( 40%)], Train Loss: 0.18605\n",
      "Epoch: 01 [ 8084/20101 ( 40%)], Train Loss: 0.18579\n",
      "Epoch: 01 [ 8124/20101 ( 40%)], Train Loss: 0.18526\n",
      "Epoch: 01 [ 8164/20101 ( 41%)], Train Loss: 0.18523\n",
      "Epoch: 01 [ 8204/20101 ( 41%)], Train Loss: 0.18483\n",
      "Epoch: 01 [ 8244/20101 ( 41%)], Train Loss: 0.18531\n",
      "Epoch: 01 [ 8284/20101 ( 41%)], Train Loss: 0.18522\n",
      "Epoch: 01 [ 8324/20101 ( 41%)], Train Loss: 0.18476\n",
      "Epoch: 01 [ 8364/20101 ( 42%)], Train Loss: 0.18473\n",
      "Epoch: 01 [ 8404/20101 ( 42%)], Train Loss: 0.18431\n",
      "Epoch: 01 [ 8444/20101 ( 42%)], Train Loss: 0.18445\n",
      "Epoch: 01 [ 8484/20101 ( 42%)], Train Loss: 0.18493\n",
      "Epoch: 01 [ 8524/20101 ( 42%)], Train Loss: 0.18441\n",
      "Epoch: 01 [ 8564/20101 ( 43%)], Train Loss: 0.18427\n",
      "Epoch: 01 [ 8604/20101 ( 43%)], Train Loss: 0.18474\n",
      "Epoch: 01 [ 8644/20101 ( 43%)], Train Loss: 0.18427\n",
      "Epoch: 01 [ 8684/20101 ( 43%)], Train Loss: 0.18459\n",
      "Epoch: 01 [ 8724/20101 ( 43%)], Train Loss: 0.18427\n",
      "Epoch: 01 [ 8764/20101 ( 44%)], Train Loss: 0.18431\n",
      "Epoch: 01 [ 8804/20101 ( 44%)], Train Loss: 0.18454\n",
      "Epoch: 01 [ 8844/20101 ( 44%)], Train Loss: 0.18453\n",
      "Epoch: 01 [ 8884/20101 ( 44%)], Train Loss: 0.18407\n",
      "Epoch: 01 [ 8924/20101 ( 44%)], Train Loss: 0.18414\n",
      "Epoch: 01 [ 8964/20101 ( 45%)], Train Loss: 0.18366\n",
      "Epoch: 01 [ 9004/20101 ( 45%)], Train Loss: 0.18350\n",
      "Epoch: 01 [ 9044/20101 ( 45%)], Train Loss: 0.18322\n",
      "Epoch: 01 [ 9084/20101 ( 45%)], Train Loss: 0.18277\n",
      "Epoch: 01 [ 9124/20101 ( 45%)], Train Loss: 0.18242\n",
      "Epoch: 01 [ 9164/20101 ( 46%)], Train Loss: 0.18190\n",
      "Epoch: 01 [ 9204/20101 ( 46%)], Train Loss: 0.18208\n",
      "Epoch: 01 [ 9244/20101 ( 46%)], Train Loss: 0.18160\n",
      "Epoch: 01 [ 9284/20101 ( 46%)], Train Loss: 0.18136\n",
      "Epoch: 01 [ 9324/20101 ( 46%)], Train Loss: 0.18154\n",
      "Epoch: 01 [ 9364/20101 ( 47%)], Train Loss: 0.18217\n",
      "Epoch: 01 [ 9404/20101 ( 47%)], Train Loss: 0.18226\n",
      "Epoch: 01 [ 9444/20101 ( 47%)], Train Loss: 0.18215\n",
      "Epoch: 01 [ 9484/20101 ( 47%)], Train Loss: 0.18235\n",
      "Epoch: 01 [ 9524/20101 ( 47%)], Train Loss: 0.18194\n",
      "Epoch: 01 [ 9564/20101 ( 48%)], Train Loss: 0.18171\n",
      "Epoch: 01 [ 9604/20101 ( 48%)], Train Loss: 0.18198\n",
      "Epoch: 01 [ 9644/20101 ( 48%)], Train Loss: 0.18204\n",
      "Epoch: 01 [ 9684/20101 ( 48%)], Train Loss: 0.18192\n",
      "Epoch: 01 [ 9724/20101 ( 48%)], Train Loss: 0.18164\n",
      "Epoch: 01 [ 9764/20101 ( 49%)], Train Loss: 0.18130\n",
      "Epoch: 01 [ 9804/20101 ( 49%)], Train Loss: 0.18105\n",
      "Epoch: 01 [ 9844/20101 ( 49%)], Train Loss: 0.18094\n",
      "Epoch: 01 [ 9884/20101 ( 49%)], Train Loss: 0.18081\n",
      "Epoch: 01 [ 9924/20101 ( 49%)], Train Loss: 0.18042\n",
      "Epoch: 01 [ 9964/20101 ( 50%)], Train Loss: 0.18001\n",
      "Epoch: 01 [10004/20101 ( 50%)], Train Loss: 0.17965\n",
      "Epoch: 01 [10044/20101 ( 50%)], Train Loss: 0.18018\n",
      "Epoch: 01 [10084/20101 ( 50%)], Train Loss: 0.18011\n",
      "Epoch: 01 [10124/20101 ( 50%)], Train Loss: 0.17960\n",
      "Epoch: 01 [10164/20101 ( 51%)], Train Loss: 0.17938\n",
      "Epoch: 01 [10204/20101 ( 51%)], Train Loss: 0.17898\n",
      "Epoch: 01 [10244/20101 ( 51%)], Train Loss: 0.17857\n",
      "Epoch: 01 [10284/20101 ( 51%)], Train Loss: 0.17892\n",
      "Epoch: 01 [10324/20101 ( 51%)], Train Loss: 0.17871\n",
      "Epoch: 01 [10364/20101 ( 52%)], Train Loss: 0.17850\n",
      "Epoch: 01 [10404/20101 ( 52%)], Train Loss: 0.17816\n",
      "Epoch: 01 [10444/20101 ( 52%)], Train Loss: 0.17833\n",
      "Epoch: 01 [10484/20101 ( 52%)], Train Loss: 0.17807\n",
      "Epoch: 01 [10524/20101 ( 52%)], Train Loss: 0.17783\n",
      "Epoch: 01 [10564/20101 ( 53%)], Train Loss: 0.17794\n",
      "Epoch: 01 [10604/20101 ( 53%)], Train Loss: 0.17784\n",
      "Epoch: 01 [10644/20101 ( 53%)], Train Loss: 0.17748\n",
      "Epoch: 01 [10684/20101 ( 53%)], Train Loss: 0.17744\n",
      "Epoch: 01 [10724/20101 ( 53%)], Train Loss: 0.17709\n",
      "Epoch: 01 [10764/20101 ( 54%)], Train Loss: 0.17710\n",
      "Epoch: 01 [10804/20101 ( 54%)], Train Loss: 0.17701\n",
      "Epoch: 01 [10844/20101 ( 54%)], Train Loss: 0.17705\n",
      "Epoch: 01 [10884/20101 ( 54%)], Train Loss: 0.17669\n",
      "Epoch: 01 [10924/20101 ( 54%)], Train Loss: 0.17653\n",
      "Epoch: 01 [10964/20101 ( 55%)], Train Loss: 0.17651\n",
      "Epoch: 01 [11004/20101 ( 55%)], Train Loss: 0.17634\n",
      "Epoch: 01 [11044/20101 ( 55%)], Train Loss: 0.17635\n",
      "Epoch: 01 [11084/20101 ( 55%)], Train Loss: 0.17699\n",
      "Epoch: 01 [11124/20101 ( 55%)], Train Loss: 0.17697\n",
      "Epoch: 01 [11164/20101 ( 56%)], Train Loss: 0.17686\n",
      "Epoch: 01 [11204/20101 ( 56%)], Train Loss: 0.17651\n",
      "Epoch: 01 [11244/20101 ( 56%)], Train Loss: 0.17635\n",
      "Epoch: 01 [11284/20101 ( 56%)], Train Loss: 0.17608\n",
      "Epoch: 01 [11324/20101 ( 56%)], Train Loss: 0.17592\n",
      "Epoch: 01 [11364/20101 ( 57%)], Train Loss: 0.17593\n",
      "Epoch: 01 [11404/20101 ( 57%)], Train Loss: 0.17613\n",
      "Epoch: 01 [11444/20101 ( 57%)], Train Loss: 0.17584\n",
      "Epoch: 01 [11484/20101 ( 57%)], Train Loss: 0.17607\n",
      "Epoch: 01 [11524/20101 ( 57%)], Train Loss: 0.17597\n",
      "Epoch: 01 [11564/20101 ( 58%)], Train Loss: 0.17586\n",
      "Epoch: 01 [11604/20101 ( 58%)], Train Loss: 0.17575\n",
      "Epoch: 01 [11644/20101 ( 58%)], Train Loss: 0.17558\n",
      "Epoch: 01 [11684/20101 ( 58%)], Train Loss: 0.17525\n",
      "Epoch: 01 [11724/20101 ( 58%)], Train Loss: 0.17503\n",
      "Epoch: 01 [11764/20101 ( 59%)], Train Loss: 0.17515\n",
      "Epoch: 01 [11804/20101 ( 59%)], Train Loss: 0.17488\n",
      "Epoch: 01 [11844/20101 ( 59%)], Train Loss: 0.17473\n",
      "Epoch: 01 [11884/20101 ( 59%)], Train Loss: 0.17443\n",
      "Epoch: 01 [11924/20101 ( 59%)], Train Loss: 0.17415\n",
      "Epoch: 01 [11964/20101 ( 60%)], Train Loss: 0.17398\n",
      "Epoch: 01 [12004/20101 ( 60%)], Train Loss: 0.17396\n",
      "Epoch: 01 [12044/20101 ( 60%)], Train Loss: 0.17355\n",
      "Epoch: 01 [12084/20101 ( 60%)], Train Loss: 0.17312\n",
      "Epoch: 01 [12124/20101 ( 60%)], Train Loss: 0.17273\n",
      "Epoch: 01 [12164/20101 ( 61%)], Train Loss: 0.17225\n",
      "Epoch: 01 [12204/20101 ( 61%)], Train Loss: 0.17219\n",
      "Epoch: 01 [12244/20101 ( 61%)], Train Loss: 0.17177\n",
      "Epoch: 01 [12284/20101 ( 61%)], Train Loss: 0.17148\n",
      "Epoch: 01 [12324/20101 ( 61%)], Train Loss: 0.17134\n",
      "Epoch: 01 [12364/20101 ( 62%)], Train Loss: 0.17124\n",
      "Epoch: 01 [12404/20101 ( 62%)], Train Loss: 0.17112\n",
      "Epoch: 01 [12444/20101 ( 62%)], Train Loss: 0.17085\n",
      "Epoch: 01 [12484/20101 ( 62%)], Train Loss: 0.17050\n",
      "Epoch: 01 [12524/20101 ( 62%)], Train Loss: 0.17014\n",
      "Epoch: 01 [12564/20101 ( 63%)], Train Loss: 0.16985\n",
      "Epoch: 01 [12604/20101 ( 63%)], Train Loss: 0.16957\n",
      "Epoch: 01 [12644/20101 ( 63%)], Train Loss: 0.16944\n",
      "Epoch: 01 [12684/20101 ( 63%)], Train Loss: 0.16916\n",
      "Epoch: 01 [12724/20101 ( 63%)], Train Loss: 0.16924\n",
      "Epoch: 01 [12764/20101 ( 63%)], Train Loss: 0.16890\n",
      "Epoch: 01 [12804/20101 ( 64%)], Train Loss: 0.16864\n",
      "Epoch: 01 [12844/20101 ( 64%)], Train Loss: 0.16826\n",
      "Epoch: 01 [12884/20101 ( 64%)], Train Loss: 0.16790\n",
      "Epoch: 01 [12924/20101 ( 64%)], Train Loss: 0.16785\n",
      "Epoch: 01 [12964/20101 ( 64%)], Train Loss: 0.16807\n",
      "Epoch: 01 [13004/20101 ( 65%)], Train Loss: 0.16765\n",
      "Epoch: 01 [13044/20101 ( 65%)], Train Loss: 0.16766\n",
      "Epoch: 01 [13084/20101 ( 65%)], Train Loss: 0.16755\n",
      "Epoch: 01 [13124/20101 ( 65%)], Train Loss: 0.16731\n",
      "Epoch: 01 [13164/20101 ( 65%)], Train Loss: 0.16724\n",
      "Epoch: 01 [13204/20101 ( 66%)], Train Loss: 0.16740\n",
      "Epoch: 01 [13244/20101 ( 66%)], Train Loss: 0.16724\n",
      "Epoch: 01 [13284/20101 ( 66%)], Train Loss: 0.16712\n",
      "Epoch: 01 [13324/20101 ( 66%)], Train Loss: 0.16739\n",
      "Epoch: 01 [13364/20101 ( 66%)], Train Loss: 0.16717\n",
      "Epoch: 01 [13404/20101 ( 67%)], Train Loss: 0.16687\n",
      "Epoch: 01 [13444/20101 ( 67%)], Train Loss: 0.16677\n",
      "Epoch: 01 [13484/20101 ( 67%)], Train Loss: 0.16677\n",
      "Epoch: 01 [13524/20101 ( 67%)], Train Loss: 0.16635\n",
      "Epoch: 01 [13564/20101 ( 67%)], Train Loss: 0.16679\n",
      "Epoch: 01 [13604/20101 ( 68%)], Train Loss: 0.16679\n",
      "Epoch: 01 [13644/20101 ( 68%)], Train Loss: 0.16681\n",
      "Epoch: 01 [13684/20101 ( 68%)], Train Loss: 0.16652\n",
      "Epoch: 01 [13724/20101 ( 68%)], Train Loss: 0.16620\n",
      "Epoch: 01 [13764/20101 ( 68%)], Train Loss: 0.16645\n",
      "Epoch: 01 [13804/20101 ( 69%)], Train Loss: 0.16643\n",
      "Epoch: 01 [13844/20101 ( 69%)], Train Loss: 0.16648\n",
      "Epoch: 01 [13884/20101 ( 69%)], Train Loss: 0.16622\n",
      "Epoch: 01 [13924/20101 ( 69%)], Train Loss: 0.16604\n",
      "Epoch: 01 [13964/20101 ( 69%)], Train Loss: 0.16584\n",
      "Epoch: 01 [14004/20101 ( 70%)], Train Loss: 0.16543\n",
      "Epoch: 01 [14044/20101 ( 70%)], Train Loss: 0.16546\n",
      "Epoch: 01 [14084/20101 ( 70%)], Train Loss: 0.16532\n",
      "Epoch: 01 [14124/20101 ( 70%)], Train Loss: 0.16499\n",
      "Epoch: 01 [14164/20101 ( 70%)], Train Loss: 0.16461\n",
      "Epoch: 01 [14204/20101 ( 71%)], Train Loss: 0.16433\n",
      "Epoch: 01 [14244/20101 ( 71%)], Train Loss: 0.16417\n",
      "Epoch: 01 [14284/20101 ( 71%)], Train Loss: 0.16389\n",
      "Epoch: 01 [14324/20101 ( 71%)], Train Loss: 0.16368\n",
      "Epoch: 01 [14364/20101 ( 71%)], Train Loss: 0.16354\n",
      "Epoch: 01 [14404/20101 ( 72%)], Train Loss: 0.16334\n",
      "Epoch: 01 [14444/20101 ( 72%)], Train Loss: 0.16366\n",
      "Epoch: 01 [14484/20101 ( 72%)], Train Loss: 0.16345\n",
      "Epoch: 01 [14524/20101 ( 72%)], Train Loss: 0.16328\n",
      "Epoch: 01 [14564/20101 ( 72%)], Train Loss: 0.16315\n",
      "Epoch: 01 [14604/20101 ( 73%)], Train Loss: 0.16303\n",
      "Epoch: 01 [14644/20101 ( 73%)], Train Loss: 0.16308\n",
      "Epoch: 01 [14684/20101 ( 73%)], Train Loss: 0.16273\n",
      "Epoch: 01 [14724/20101 ( 73%)], Train Loss: 0.16262\n",
      "Epoch: 01 [14764/20101 ( 73%)], Train Loss: 0.16248\n",
      "Epoch: 01 [14804/20101 ( 74%)], Train Loss: 0.16219\n",
      "Epoch: 01 [14844/20101 ( 74%)], Train Loss: 0.16195\n",
      "Epoch: 01 [14884/20101 ( 74%)], Train Loss: 0.16168\n",
      "Epoch: 01 [14924/20101 ( 74%)], Train Loss: 0.16168\n",
      "Epoch: 01 [14964/20101 ( 74%)], Train Loss: 0.16151\n",
      "Epoch: 01 [15004/20101 ( 75%)], Train Loss: 0.16122\n",
      "Epoch: 01 [15044/20101 ( 75%)], Train Loss: 0.16119\n",
      "Epoch: 01 [15084/20101 ( 75%)], Train Loss: 0.16090\n",
      "Epoch: 01 [15124/20101 ( 75%)], Train Loss: 0.16094\n",
      "Epoch: 01 [15164/20101 ( 75%)], Train Loss: 0.16096\n",
      "Epoch: 01 [15204/20101 ( 76%)], Train Loss: 0.16094\n",
      "Epoch: 01 [15244/20101 ( 76%)], Train Loss: 0.16087\n",
      "Epoch: 01 [15284/20101 ( 76%)], Train Loss: 0.16069\n",
      "Epoch: 01 [15324/20101 ( 76%)], Train Loss: 0.16044\n",
      "Epoch: 01 [15364/20101 ( 76%)], Train Loss: 0.16044\n",
      "Epoch: 01 [15404/20101 ( 77%)], Train Loss: 0.16037\n",
      "Epoch: 01 [15444/20101 ( 77%)], Train Loss: 0.16030\n",
      "Epoch: 01 [15484/20101 ( 77%)], Train Loss: 0.16025\n",
      "Epoch: 01 [15524/20101 ( 77%)], Train Loss: 0.16004\n",
      "Epoch: 01 [15564/20101 ( 77%)], Train Loss: 0.15972\n",
      "Epoch: 01 [15604/20101 ( 78%)], Train Loss: 0.15955\n",
      "Epoch: 01 [15644/20101 ( 78%)], Train Loss: 0.15926\n",
      "Epoch: 01 [15684/20101 ( 78%)], Train Loss: 0.15924\n",
      "Epoch: 01 [15724/20101 ( 78%)], Train Loss: 0.15919\n",
      "Epoch: 01 [15764/20101 ( 78%)], Train Loss: 0.15911\n",
      "Epoch: 01 [15804/20101 ( 79%)], Train Loss: 0.15893\n",
      "Epoch: 01 [15844/20101 ( 79%)], Train Loss: 0.15871\n",
      "Epoch: 01 [15884/20101 ( 79%)], Train Loss: 0.15854\n",
      "Epoch: 01 [15924/20101 ( 79%)], Train Loss: 0.15822\n",
      "Epoch: 01 [15964/20101 ( 79%)], Train Loss: 0.15819\n",
      "Epoch: 01 [16004/20101 ( 80%)], Train Loss: 0.15789\n",
      "Epoch: 01 [16044/20101 ( 80%)], Train Loss: 0.15769\n",
      "Epoch: 01 [16084/20101 ( 80%)], Train Loss: 0.15764\n",
      "Epoch: 01 [16124/20101 ( 80%)], Train Loss: 0.15736\n",
      "Epoch: 01 [16164/20101 ( 80%)], Train Loss: 0.15751\n",
      "Epoch: 01 [16204/20101 ( 81%)], Train Loss: 0.15723\n",
      "Epoch: 01 [16244/20101 ( 81%)], Train Loss: 0.15703\n",
      "Epoch: 01 [16284/20101 ( 81%)], Train Loss: 0.15700\n",
      "Epoch: 01 [16324/20101 ( 81%)], Train Loss: 0.15692\n",
      "Epoch: 01 [16364/20101 ( 81%)], Train Loss: 0.15700\n",
      "Epoch: 01 [16404/20101 ( 82%)], Train Loss: 0.15745\n",
      "Epoch: 01 [16444/20101 ( 82%)], Train Loss: 0.15758\n",
      "Epoch: 01 [16484/20101 ( 82%)], Train Loss: 0.15738\n",
      "Epoch: 01 [16524/20101 ( 82%)], Train Loss: 0.15734\n",
      "Epoch: 01 [16564/20101 ( 82%)], Train Loss: 0.15736\n",
      "Epoch: 01 [16604/20101 ( 83%)], Train Loss: 0.15744\n",
      "Epoch: 01 [16644/20101 ( 83%)], Train Loss: 0.15737\n",
      "Epoch: 01 [16684/20101 ( 83%)], Train Loss: 0.15731\n",
      "Epoch: 01 [16724/20101 ( 83%)], Train Loss: 0.15704\n",
      "Epoch: 01 [16764/20101 ( 83%)], Train Loss: 0.15686\n",
      "Epoch: 01 [16804/20101 ( 84%)], Train Loss: 0.15683\n",
      "Epoch: 01 [16844/20101 ( 84%)], Train Loss: 0.15665\n",
      "Epoch: 01 [16884/20101 ( 84%)], Train Loss: 0.15641\n",
      "Epoch: 01 [16924/20101 ( 84%)], Train Loss: 0.15622\n",
      "Epoch: 01 [16964/20101 ( 84%)], Train Loss: 0.15602\n",
      "Epoch: 01 [17004/20101 ( 85%)], Train Loss: 0.15615\n",
      "Epoch: 01 [17044/20101 ( 85%)], Train Loss: 0.15627\n",
      "Epoch: 01 [17084/20101 ( 85%)], Train Loss: 0.15610\n",
      "Epoch: 01 [17124/20101 ( 85%)], Train Loss: 0.15591\n",
      "Epoch: 01 [17164/20101 ( 85%)], Train Loss: 0.15579\n",
      "Epoch: 01 [17204/20101 ( 86%)], Train Loss: 0.15558\n",
      "Epoch: 01 [17244/20101 ( 86%)], Train Loss: 0.15545\n",
      "Epoch: 01 [17284/20101 ( 86%)], Train Loss: 0.15528\n",
      "Epoch: 01 [17324/20101 ( 86%)], Train Loss: 0.15505\n",
      "Epoch: 01 [17364/20101 ( 86%)], Train Loss: 0.15495\n",
      "Epoch: 01 [17404/20101 ( 87%)], Train Loss: 0.15486\n",
      "Epoch: 01 [17444/20101 ( 87%)], Train Loss: 0.15459\n",
      "Epoch: 01 [17484/20101 ( 87%)], Train Loss: 0.15445\n",
      "Epoch: 01 [17524/20101 ( 87%)], Train Loss: 0.15430\n",
      "Epoch: 01 [17564/20101 ( 87%)], Train Loss: 0.15426\n",
      "Epoch: 01 [17604/20101 ( 88%)], Train Loss: 0.15424\n",
      "Epoch: 01 [17644/20101 ( 88%)], Train Loss: 0.15420\n",
      "Epoch: 01 [17684/20101 ( 88%)], Train Loss: 0.15445\n",
      "Epoch: 01 [17724/20101 ( 88%)], Train Loss: 0.15439\n",
      "Epoch: 01 [17764/20101 ( 88%)], Train Loss: 0.15426\n",
      "Epoch: 01 [17804/20101 ( 89%)], Train Loss: 0.15424\n",
      "Epoch: 01 [17844/20101 ( 89%)], Train Loss: 0.15407\n",
      "Epoch: 01 [17884/20101 ( 89%)], Train Loss: 0.15394\n",
      "Epoch: 01 [17924/20101 ( 89%)], Train Loss: 0.15368\n",
      "Epoch: 01 [17964/20101 ( 89%)], Train Loss: 0.15348\n",
      "Epoch: 01 [18004/20101 ( 90%)], Train Loss: 0.15337\n",
      "Epoch: 01 [18044/20101 ( 90%)], Train Loss: 0.15319\n",
      "Epoch: 01 [18084/20101 ( 90%)], Train Loss: 0.15316\n",
      "Epoch: 01 [18124/20101 ( 90%)], Train Loss: 0.15312\n",
      "Epoch: 01 [18164/20101 ( 90%)], Train Loss: 0.15309\n",
      "Epoch: 01 [18204/20101 ( 91%)], Train Loss: 0.15301\n",
      "Epoch: 01 [18244/20101 ( 91%)], Train Loss: 0.15289\n",
      "Epoch: 01 [18284/20101 ( 91%)], Train Loss: 0.15315\n",
      "Epoch: 01 [18324/20101 ( 91%)], Train Loss: 0.15291\n",
      "Epoch: 01 [18364/20101 ( 91%)], Train Loss: 0.15287\n",
      "Epoch: 01 [18404/20101 ( 92%)], Train Loss: 0.15305\n",
      "Epoch: 01 [18444/20101 ( 92%)], Train Loss: 0.15291\n",
      "Epoch: 01 [18484/20101 ( 92%)], Train Loss: 0.15272\n",
      "Epoch: 01 [18524/20101 ( 92%)], Train Loss: 0.15282\n",
      "Epoch: 01 [18564/20101 ( 92%)], Train Loss: 0.15274\n",
      "Epoch: 01 [18604/20101 ( 93%)], Train Loss: 0.15277\n",
      "Epoch: 01 [18644/20101 ( 93%)], Train Loss: 0.15272\n",
      "Epoch: 01 [18684/20101 ( 93%)], Train Loss: 0.15271\n",
      "Epoch: 01 [18724/20101 ( 93%)], Train Loss: 0.15267\n",
      "Epoch: 01 [18764/20101 ( 93%)], Train Loss: 0.15263\n",
      "Epoch: 01 [18804/20101 ( 94%)], Train Loss: 0.15311\n",
      "Epoch: 01 [18844/20101 ( 94%)], Train Loss: 0.15299\n",
      "Epoch: 01 [18884/20101 ( 94%)], Train Loss: 0.15287\n",
      "Epoch: 01 [18924/20101 ( 94%)], Train Loss: 0.15275\n",
      "Epoch: 01 [18964/20101 ( 94%)], Train Loss: 0.15266\n",
      "Epoch: 01 [19004/20101 ( 95%)], Train Loss: 0.15248\n",
      "Epoch: 01 [19044/20101 ( 95%)], Train Loss: 0.15231\n",
      "Epoch: 01 [19084/20101 ( 95%)], Train Loss: 0.15217\n",
      "Epoch: 01 [19124/20101 ( 95%)], Train Loss: 0.15204\n",
      "Epoch: 01 [19164/20101 ( 95%)], Train Loss: 0.15178\n",
      "Epoch: 01 [19204/20101 ( 96%)], Train Loss: 0.15158\n",
      "Epoch: 01 [19244/20101 ( 96%)], Train Loss: 0.15138\n",
      "Epoch: 01 [19284/20101 ( 96%)], Train Loss: 0.15110\n",
      "Epoch: 01 [19324/20101 ( 96%)], Train Loss: 0.15124\n",
      "Epoch: 01 [19364/20101 ( 96%)], Train Loss: 0.15112\n",
      "Epoch: 01 [19404/20101 ( 97%)], Train Loss: 0.15106\n",
      "Epoch: 01 [19444/20101 ( 97%)], Train Loss: 0.15123\n",
      "Epoch: 01 [19484/20101 ( 97%)], Train Loss: 0.15126\n",
      "Epoch: 01 [19524/20101 ( 97%)], Train Loss: 0.15166\n",
      "Epoch: 01 [19564/20101 ( 97%)], Train Loss: 0.15163\n",
      "Epoch: 01 [19604/20101 ( 98%)], Train Loss: 0.15151\n",
      "Epoch: 01 [19644/20101 ( 98%)], Train Loss: 0.15138\n",
      "Epoch: 01 [19684/20101 ( 98%)], Train Loss: 0.15131\n",
      "Epoch: 01 [19724/20101 ( 98%)], Train Loss: 0.15118\n",
      "Epoch: 01 [19764/20101 ( 98%)], Train Loss: 0.15097\n",
      "Epoch: 01 [19804/20101 ( 99%)], Train Loss: 0.15088\n",
      "Epoch: 01 [19844/20101 ( 99%)], Train Loss: 0.15075\n",
      "Epoch: 01 [19884/20101 ( 99%)], Train Loss: 0.15060\n",
      "Epoch: 01 [19924/20101 ( 99%)], Train Loss: 0.15058\n",
      "Epoch: 01 [19964/20101 ( 99%)], Train Loss: 0.15049\n",
      "Epoch: 01 [20004/20101 (100%)], Train Loss: 0.15046\n",
      "Epoch: 01 [20044/20101 (100%)], Train Loss: 0.15043\n",
      "Epoch: 01 [20084/20101 (100%)], Train Loss: 0.15036\n",
      "Epoch: 01 [20101/20101 (100%)], Train Loss: 0.15026\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.27639\n",
      "\n",
      "Total Training Time: 5682.662042856216secs, Average Training Time per Epoch: 2841.331021428108secs.\n",
      "Total Validation Time: 250.71279048919678secs, Average Validation Time per Epoch: 125.35639524459839secs.\n"
     ]
    }
   ],
   "source": [
    "for fold in range(3, 4):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99af92a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-03T09:59:16.214245Z",
     "iopub.status.busy": "2021-10-03T09:59:16.213536Z",
     "iopub.status.idle": "2021-10-03T11:39:46.278541Z",
     "shell.execute_reply": "2021-10-03T11:39:46.279009Z"
    },
    "papermill": {
     "duration": 6031.116862,
     "end_time": "2021-10-03T11:39:46.279158",
     "exception": false,
     "start_time": "2021-10-03T09:59:15.162296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 4\n",
      "--------------------------------------------------\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Num examples Train= 20164, Num examples Valid=2905\n",
      "Total Training Steps: 5042, Total Warmup Steps: 504\n",
      "Epoch: 00 [    4/20164 (  0%)], Train Loss: 3.01919\n",
      "Epoch: 00 [   44/20164 (  0%)], Train Loss: 3.05275\n",
      "Epoch: 00 [   84/20164 (  0%)], Train Loss: 3.04864\n",
      "Epoch: 00 [  124/20164 (  1%)], Train Loss: 3.04047\n",
      "Epoch: 00 [  164/20164 (  1%)], Train Loss: 3.01353\n",
      "Epoch: 00 [  204/20164 (  1%)], Train Loss: 2.98465\n",
      "Epoch: 00 [  244/20164 (  1%)], Train Loss: 2.94505\n",
      "Epoch: 00 [  284/20164 (  1%)], Train Loss: 2.89779\n",
      "Epoch: 00 [  324/20164 (  2%)], Train Loss: 2.84743\n",
      "Epoch: 00 [  364/20164 (  2%)], Train Loss: 2.79283\n",
      "Epoch: 00 [  404/20164 (  2%)], Train Loss: 2.73218\n",
      "Epoch: 00 [  444/20164 (  2%)], Train Loss: 2.65341\n",
      "Epoch: 00 [  484/20164 (  2%)], Train Loss: 2.57215\n",
      "Epoch: 00 [  524/20164 (  3%)], Train Loss: 2.49281\n",
      "Epoch: 00 [  564/20164 (  3%)], Train Loss: 2.39441\n",
      "Epoch: 00 [  604/20164 (  3%)], Train Loss: 2.31064\n",
      "Epoch: 00 [  644/20164 (  3%)], Train Loss: 2.21221\n",
      "Epoch: 00 [  684/20164 (  3%)], Train Loss: 2.12468\n",
      "Epoch: 00 [  724/20164 (  4%)], Train Loss: 2.04263\n",
      "Epoch: 00 [  764/20164 (  4%)], Train Loss: 1.96835\n",
      "Epoch: 00 [  804/20164 (  4%)], Train Loss: 1.90697\n",
      "Epoch: 00 [  844/20164 (  4%)], Train Loss: 1.84274\n",
      "Epoch: 00 [  884/20164 (  4%)], Train Loss: 1.78855\n",
      "Epoch: 00 [  924/20164 (  5%)], Train Loss: 1.73376\n",
      "Epoch: 00 [  964/20164 (  5%)], Train Loss: 1.67797\n",
      "Epoch: 00 [ 1004/20164 (  5%)], Train Loss: 1.63595\n",
      "Epoch: 00 [ 1044/20164 (  5%)], Train Loss: 1.59037\n",
      "Epoch: 00 [ 1084/20164 (  5%)], Train Loss: 1.55335\n",
      "Epoch: 00 [ 1124/20164 (  6%)], Train Loss: 1.51593\n",
      "Epoch: 00 [ 1164/20164 (  6%)], Train Loss: 1.47602\n",
      "Epoch: 00 [ 1204/20164 (  6%)], Train Loss: 1.44676\n",
      "Epoch: 00 [ 1244/20164 (  6%)], Train Loss: 1.41232\n",
      "Epoch: 00 [ 1284/20164 (  6%)], Train Loss: 1.38503\n",
      "Epoch: 00 [ 1324/20164 (  7%)], Train Loss: 1.36591\n",
      "Epoch: 00 [ 1364/20164 (  7%)], Train Loss: 1.34528\n",
      "Epoch: 00 [ 1404/20164 (  7%)], Train Loss: 1.31999\n",
      "Epoch: 00 [ 1444/20164 (  7%)], Train Loss: 1.29706\n",
      "Epoch: 00 [ 1484/20164 (  7%)], Train Loss: 1.27316\n",
      "Epoch: 00 [ 1524/20164 (  8%)], Train Loss: 1.25659\n",
      "Epoch: 00 [ 1564/20164 (  8%)], Train Loss: 1.23836\n",
      "Epoch: 00 [ 1604/20164 (  8%)], Train Loss: 1.21716\n",
      "Epoch: 00 [ 1644/20164 (  8%)], Train Loss: 1.19494\n",
      "Epoch: 00 [ 1684/20164 (  8%)], Train Loss: 1.17542\n",
      "Epoch: 00 [ 1724/20164 (  9%)], Train Loss: 1.15827\n",
      "Epoch: 00 [ 1764/20164 (  9%)], Train Loss: 1.14229\n",
      "Epoch: 00 [ 1804/20164 (  9%)], Train Loss: 1.12404\n",
      "Epoch: 00 [ 1844/20164 (  9%)], Train Loss: 1.10965\n",
      "Epoch: 00 [ 1884/20164 (  9%)], Train Loss: 1.09144\n",
      "Epoch: 00 [ 1924/20164 ( 10%)], Train Loss: 1.08405\n",
      "Epoch: 00 [ 1964/20164 ( 10%)], Train Loss: 1.07026\n",
      "Epoch: 00 [ 2004/20164 ( 10%)], Train Loss: 1.06272\n",
      "Epoch: 00 [ 2044/20164 ( 10%)], Train Loss: 1.04971\n",
      "Epoch: 00 [ 2084/20164 ( 10%)], Train Loss: 1.03662\n",
      "Epoch: 00 [ 2124/20164 ( 11%)], Train Loss: 1.02707\n",
      "Epoch: 00 [ 2164/20164 ( 11%)], Train Loss: 1.01623\n",
      "Epoch: 00 [ 2204/20164 ( 11%)], Train Loss: 1.00734\n",
      "Epoch: 00 [ 2244/20164 ( 11%)], Train Loss: 0.99917\n",
      "Epoch: 00 [ 2284/20164 ( 11%)], Train Loss: 0.98817\n",
      "Epoch: 00 [ 2324/20164 ( 12%)], Train Loss: 0.98056\n",
      "Epoch: 00 [ 2364/20164 ( 12%)], Train Loss: 0.96904\n",
      "Epoch: 00 [ 2404/20164 ( 12%)], Train Loss: 0.96142\n",
      "Epoch: 00 [ 2444/20164 ( 12%)], Train Loss: 0.95299\n",
      "Epoch: 00 [ 2484/20164 ( 12%)], Train Loss: 0.94864\n",
      "Epoch: 00 [ 2524/20164 ( 13%)], Train Loss: 0.94619\n",
      "Epoch: 00 [ 2564/20164 ( 13%)], Train Loss: 0.93840\n",
      "Epoch: 00 [ 2604/20164 ( 13%)], Train Loss: 0.93025\n",
      "Epoch: 00 [ 2644/20164 ( 13%)], Train Loss: 0.92395\n",
      "Epoch: 00 [ 2684/20164 ( 13%)], Train Loss: 0.91667\n",
      "Epoch: 00 [ 2724/20164 ( 14%)], Train Loss: 0.90948\n",
      "Epoch: 00 [ 2764/20164 ( 14%)], Train Loss: 0.89987\n",
      "Epoch: 00 [ 2804/20164 ( 14%)], Train Loss: 0.89349\n",
      "Epoch: 00 [ 2844/20164 ( 14%)], Train Loss: 0.88636\n",
      "Epoch: 00 [ 2884/20164 ( 14%)], Train Loss: 0.87931\n",
      "Epoch: 00 [ 2924/20164 ( 15%)], Train Loss: 0.87688\n",
      "Epoch: 00 [ 2964/20164 ( 15%)], Train Loss: 0.87299\n",
      "Epoch: 00 [ 3004/20164 ( 15%)], Train Loss: 0.86679\n",
      "Epoch: 00 [ 3044/20164 ( 15%)], Train Loss: 0.86356\n",
      "Epoch: 00 [ 3084/20164 ( 15%)], Train Loss: 0.85606\n",
      "Epoch: 00 [ 3124/20164 ( 15%)], Train Loss: 0.85157\n",
      "Epoch: 00 [ 3164/20164 ( 16%)], Train Loss: 0.84651\n",
      "Epoch: 00 [ 3204/20164 ( 16%)], Train Loss: 0.84020\n",
      "Epoch: 00 [ 3244/20164 ( 16%)], Train Loss: 0.83740\n",
      "Epoch: 00 [ 3284/20164 ( 16%)], Train Loss: 0.83282\n",
      "Epoch: 00 [ 3324/20164 ( 16%)], Train Loss: 0.82945\n",
      "Epoch: 00 [ 3364/20164 ( 17%)], Train Loss: 0.82469\n",
      "Epoch: 00 [ 3404/20164 ( 17%)], Train Loss: 0.81862\n",
      "Epoch: 00 [ 3444/20164 ( 17%)], Train Loss: 0.81357\n",
      "Epoch: 00 [ 3484/20164 ( 17%)], Train Loss: 0.80789\n",
      "Epoch: 00 [ 3524/20164 ( 17%)], Train Loss: 0.80216\n",
      "Epoch: 00 [ 3564/20164 ( 18%)], Train Loss: 0.79908\n",
      "Epoch: 00 [ 3604/20164 ( 18%)], Train Loss: 0.79525\n",
      "Epoch: 00 [ 3644/20164 ( 18%)], Train Loss: 0.79099\n",
      "Epoch: 00 [ 3684/20164 ( 18%)], Train Loss: 0.78615\n",
      "Epoch: 00 [ 3724/20164 ( 18%)], Train Loss: 0.78285\n",
      "Epoch: 00 [ 3764/20164 ( 19%)], Train Loss: 0.77827\n",
      "Epoch: 00 [ 3804/20164 ( 19%)], Train Loss: 0.77687\n",
      "Epoch: 00 [ 3844/20164 ( 19%)], Train Loss: 0.77384\n",
      "Epoch: 00 [ 3884/20164 ( 19%)], Train Loss: 0.76980\n",
      "Epoch: 00 [ 3924/20164 ( 19%)], Train Loss: 0.76507\n",
      "Epoch: 00 [ 3964/20164 ( 20%)], Train Loss: 0.76148\n",
      "Epoch: 00 [ 4004/20164 ( 20%)], Train Loss: 0.75755\n",
      "Epoch: 00 [ 4044/20164 ( 20%)], Train Loss: 0.75456\n",
      "Epoch: 00 [ 4084/20164 ( 20%)], Train Loss: 0.75158\n",
      "Epoch: 00 [ 4124/20164 ( 20%)], Train Loss: 0.74645\n",
      "Epoch: 00 [ 4164/20164 ( 21%)], Train Loss: 0.74419\n",
      "Epoch: 00 [ 4204/20164 ( 21%)], Train Loss: 0.74152\n",
      "Epoch: 00 [ 4244/20164 ( 21%)], Train Loss: 0.73816\n",
      "Epoch: 00 [ 4284/20164 ( 21%)], Train Loss: 0.73391\n",
      "Epoch: 00 [ 4324/20164 ( 21%)], Train Loss: 0.73150\n",
      "Epoch: 00 [ 4364/20164 ( 22%)], Train Loss: 0.72795\n",
      "Epoch: 00 [ 4404/20164 ( 22%)], Train Loss: 0.72445\n",
      "Epoch: 00 [ 4444/20164 ( 22%)], Train Loss: 0.72064\n",
      "Epoch: 00 [ 4484/20164 ( 22%)], Train Loss: 0.71679\n",
      "Epoch: 00 [ 4524/20164 ( 22%)], Train Loss: 0.71450\n",
      "Epoch: 00 [ 4564/20164 ( 23%)], Train Loss: 0.71096\n",
      "Epoch: 00 [ 4604/20164 ( 23%)], Train Loss: 0.70960\n",
      "Epoch: 00 [ 4644/20164 ( 23%)], Train Loss: 0.70646\n",
      "Epoch: 00 [ 4684/20164 ( 23%)], Train Loss: 0.70473\n",
      "Epoch: 00 [ 4724/20164 ( 23%)], Train Loss: 0.70266\n",
      "Epoch: 00 [ 4764/20164 ( 24%)], Train Loss: 0.69991\n",
      "Epoch: 00 [ 4804/20164 ( 24%)], Train Loss: 0.69827\n",
      "Epoch: 00 [ 4844/20164 ( 24%)], Train Loss: 0.69817\n",
      "Epoch: 00 [ 4884/20164 ( 24%)], Train Loss: 0.69455\n",
      "Epoch: 00 [ 4924/20164 ( 24%)], Train Loss: 0.69176\n",
      "Epoch: 00 [ 4964/20164 ( 25%)], Train Loss: 0.68852\n",
      "Epoch: 00 [ 5004/20164 ( 25%)], Train Loss: 0.68499\n",
      "Epoch: 00 [ 5044/20164 ( 25%)], Train Loss: 0.68413\n",
      "Epoch: 00 [ 5084/20164 ( 25%)], Train Loss: 0.68088\n",
      "Epoch: 00 [ 5124/20164 ( 25%)], Train Loss: 0.67811\n",
      "Epoch: 00 [ 5164/20164 ( 26%)], Train Loss: 0.67606\n",
      "Epoch: 00 [ 5204/20164 ( 26%)], Train Loss: 0.67405\n",
      "Epoch: 00 [ 5244/20164 ( 26%)], Train Loss: 0.67355\n",
      "Epoch: 00 [ 5284/20164 ( 26%)], Train Loss: 0.66981\n",
      "Epoch: 00 [ 5324/20164 ( 26%)], Train Loss: 0.66823\n",
      "Epoch: 00 [ 5364/20164 ( 27%)], Train Loss: 0.66535\n",
      "Epoch: 00 [ 5404/20164 ( 27%)], Train Loss: 0.66558\n",
      "Epoch: 00 [ 5444/20164 ( 27%)], Train Loss: 0.66348\n",
      "Epoch: 00 [ 5484/20164 ( 27%)], Train Loss: 0.66198\n",
      "Epoch: 00 [ 5524/20164 ( 27%)], Train Loss: 0.66201\n",
      "Epoch: 00 [ 5564/20164 ( 28%)], Train Loss: 0.66092\n",
      "Epoch: 00 [ 5604/20164 ( 28%)], Train Loss: 0.65858\n",
      "Epoch: 00 [ 5644/20164 ( 28%)], Train Loss: 0.65574\n",
      "Epoch: 00 [ 5684/20164 ( 28%)], Train Loss: 0.65319\n",
      "Epoch: 00 [ 5724/20164 ( 28%)], Train Loss: 0.64976\n",
      "Epoch: 00 [ 5764/20164 ( 29%)], Train Loss: 0.64919\n",
      "Epoch: 00 [ 5804/20164 ( 29%)], Train Loss: 0.64681\n",
      "Epoch: 00 [ 5844/20164 ( 29%)], Train Loss: 0.64438\n",
      "Epoch: 00 [ 5884/20164 ( 29%)], Train Loss: 0.64187\n",
      "Epoch: 00 [ 5924/20164 ( 29%)], Train Loss: 0.64019\n",
      "Epoch: 00 [ 5964/20164 ( 30%)], Train Loss: 0.63871\n",
      "Epoch: 00 [ 6004/20164 ( 30%)], Train Loss: 0.63847\n",
      "Epoch: 00 [ 6044/20164 ( 30%)], Train Loss: 0.63712\n",
      "Epoch: 00 [ 6084/20164 ( 30%)], Train Loss: 0.63606\n",
      "Epoch: 00 [ 6124/20164 ( 30%)], Train Loss: 0.63576\n",
      "Epoch: 00 [ 6164/20164 ( 31%)], Train Loss: 0.63453\n",
      "Epoch: 00 [ 6204/20164 ( 31%)], Train Loss: 0.63319\n",
      "Epoch: 00 [ 6244/20164 ( 31%)], Train Loss: 0.63126\n",
      "Epoch: 00 [ 6284/20164 ( 31%)], Train Loss: 0.63028\n",
      "Epoch: 00 [ 6324/20164 ( 31%)], Train Loss: 0.62988\n",
      "Epoch: 00 [ 6364/20164 ( 32%)], Train Loss: 0.62929\n",
      "Epoch: 00 [ 6404/20164 ( 32%)], Train Loss: 0.62706\n",
      "Epoch: 00 [ 6444/20164 ( 32%)], Train Loss: 0.62612\n",
      "Epoch: 00 [ 6484/20164 ( 32%)], Train Loss: 0.62356\n",
      "Epoch: 00 [ 6524/20164 ( 32%)], Train Loss: 0.62181\n",
      "Epoch: 00 [ 6564/20164 ( 33%)], Train Loss: 0.61993\n",
      "Epoch: 00 [ 6604/20164 ( 33%)], Train Loss: 0.62028\n",
      "Epoch: 00 [ 6644/20164 ( 33%)], Train Loss: 0.61939\n",
      "Epoch: 00 [ 6684/20164 ( 33%)], Train Loss: 0.61780\n",
      "Epoch: 00 [ 6724/20164 ( 33%)], Train Loss: 0.61703\n",
      "Epoch: 00 [ 6764/20164 ( 34%)], Train Loss: 0.61520\n",
      "Epoch: 00 [ 6804/20164 ( 34%)], Train Loss: 0.61381\n",
      "Epoch: 00 [ 6844/20164 ( 34%)], Train Loss: 0.61150\n",
      "Epoch: 00 [ 6884/20164 ( 34%)], Train Loss: 0.61101\n",
      "Epoch: 00 [ 6924/20164 ( 34%)], Train Loss: 0.60857\n",
      "Epoch: 00 [ 6964/20164 ( 35%)], Train Loss: 0.60688\n",
      "Epoch: 00 [ 7004/20164 ( 35%)], Train Loss: 0.60580\n",
      "Epoch: 00 [ 7044/20164 ( 35%)], Train Loss: 0.60413\n",
      "Epoch: 00 [ 7084/20164 ( 35%)], Train Loss: 0.60315\n",
      "Epoch: 00 [ 7124/20164 ( 35%)], Train Loss: 0.60181\n",
      "Epoch: 00 [ 7164/20164 ( 36%)], Train Loss: 0.60079\n",
      "Epoch: 00 [ 7204/20164 ( 36%)], Train Loss: 0.59929\n",
      "Epoch: 00 [ 7244/20164 ( 36%)], Train Loss: 0.59749\n",
      "Epoch: 00 [ 7284/20164 ( 36%)], Train Loss: 0.59677\n",
      "Epoch: 00 [ 7324/20164 ( 36%)], Train Loss: 0.59480\n",
      "Epoch: 00 [ 7364/20164 ( 37%)], Train Loss: 0.59338\n",
      "Epoch: 00 [ 7404/20164 ( 37%)], Train Loss: 0.59269\n",
      "Epoch: 00 [ 7444/20164 ( 37%)], Train Loss: 0.59124\n",
      "Epoch: 00 [ 7484/20164 ( 37%)], Train Loss: 0.59050\n",
      "Epoch: 00 [ 7524/20164 ( 37%)], Train Loss: 0.58886\n",
      "Epoch: 00 [ 7564/20164 ( 38%)], Train Loss: 0.58717\n",
      "Epoch: 00 [ 7604/20164 ( 38%)], Train Loss: 0.58637\n",
      "Epoch: 00 [ 7644/20164 ( 38%)], Train Loss: 0.58501\n",
      "Epoch: 00 [ 7684/20164 ( 38%)], Train Loss: 0.58438\n",
      "Epoch: 00 [ 7724/20164 ( 38%)], Train Loss: 0.58450\n",
      "Epoch: 00 [ 7764/20164 ( 39%)], Train Loss: 0.58319\n",
      "Epoch: 00 [ 7804/20164 ( 39%)], Train Loss: 0.58287\n",
      "Epoch: 00 [ 7844/20164 ( 39%)], Train Loss: 0.58260\n",
      "Epoch: 00 [ 7884/20164 ( 39%)], Train Loss: 0.58206\n",
      "Epoch: 00 [ 7924/20164 ( 39%)], Train Loss: 0.58090\n",
      "Epoch: 00 [ 7964/20164 ( 39%)], Train Loss: 0.58054\n",
      "Epoch: 00 [ 8004/20164 ( 40%)], Train Loss: 0.57978\n",
      "Epoch: 00 [ 8044/20164 ( 40%)], Train Loss: 0.57824\n",
      "Epoch: 00 [ 8084/20164 ( 40%)], Train Loss: 0.57767\n",
      "Epoch: 00 [ 8124/20164 ( 40%)], Train Loss: 0.57614\n",
      "Epoch: 00 [ 8164/20164 ( 40%)], Train Loss: 0.57575\n",
      "Epoch: 00 [ 8204/20164 ( 41%)], Train Loss: 0.57407\n",
      "Epoch: 00 [ 8244/20164 ( 41%)], Train Loss: 0.57271\n",
      "Epoch: 00 [ 8284/20164 ( 41%)], Train Loss: 0.57192\n",
      "Epoch: 00 [ 8324/20164 ( 41%)], Train Loss: 0.57028\n",
      "Epoch: 00 [ 8364/20164 ( 41%)], Train Loss: 0.56938\n",
      "Epoch: 00 [ 8404/20164 ( 42%)], Train Loss: 0.56946\n",
      "Epoch: 00 [ 8444/20164 ( 42%)], Train Loss: 0.56788\n",
      "Epoch: 00 [ 8484/20164 ( 42%)], Train Loss: 0.56616\n",
      "Epoch: 00 [ 8524/20164 ( 42%)], Train Loss: 0.56568\n",
      "Epoch: 00 [ 8564/20164 ( 42%)], Train Loss: 0.56517\n",
      "Epoch: 00 [ 8604/20164 ( 43%)], Train Loss: 0.56402\n",
      "Epoch: 00 [ 8644/20164 ( 43%)], Train Loss: 0.56335\n",
      "Epoch: 00 [ 8684/20164 ( 43%)], Train Loss: 0.56289\n",
      "Epoch: 00 [ 8724/20164 ( 43%)], Train Loss: 0.56221\n",
      "Epoch: 00 [ 8764/20164 ( 43%)], Train Loss: 0.56156\n",
      "Epoch: 00 [ 8804/20164 ( 44%)], Train Loss: 0.56047\n",
      "Epoch: 00 [ 8844/20164 ( 44%)], Train Loss: 0.56036\n",
      "Epoch: 00 [ 8884/20164 ( 44%)], Train Loss: 0.55944\n",
      "Epoch: 00 [ 8924/20164 ( 44%)], Train Loss: 0.55845\n",
      "Epoch: 00 [ 8964/20164 ( 44%)], Train Loss: 0.55905\n",
      "Epoch: 00 [ 9004/20164 ( 45%)], Train Loss: 0.55798\n",
      "Epoch: 00 [ 9044/20164 ( 45%)], Train Loss: 0.55724\n",
      "Epoch: 00 [ 9084/20164 ( 45%)], Train Loss: 0.55675\n",
      "Epoch: 00 [ 9124/20164 ( 45%)], Train Loss: 0.55686\n",
      "Epoch: 00 [ 9164/20164 ( 45%)], Train Loss: 0.55566\n",
      "Epoch: 00 [ 9204/20164 ( 46%)], Train Loss: 0.55497\n",
      "Epoch: 00 [ 9244/20164 ( 46%)], Train Loss: 0.55435\n",
      "Epoch: 00 [ 9284/20164 ( 46%)], Train Loss: 0.55416\n",
      "Epoch: 00 [ 9324/20164 ( 46%)], Train Loss: 0.55354\n",
      "Epoch: 00 [ 9364/20164 ( 46%)], Train Loss: 0.55267\n",
      "Epoch: 00 [ 9404/20164 ( 47%)], Train Loss: 0.55137\n",
      "Epoch: 00 [ 9444/20164 ( 47%)], Train Loss: 0.55068\n",
      "Epoch: 00 [ 9484/20164 ( 47%)], Train Loss: 0.54936\n",
      "Epoch: 00 [ 9524/20164 ( 47%)], Train Loss: 0.54781\n",
      "Epoch: 00 [ 9564/20164 ( 47%)], Train Loss: 0.54691\n",
      "Epoch: 00 [ 9604/20164 ( 48%)], Train Loss: 0.54657\n",
      "Epoch: 00 [ 9644/20164 ( 48%)], Train Loss: 0.54561\n",
      "Epoch: 00 [ 9684/20164 ( 48%)], Train Loss: 0.54563\n",
      "Epoch: 00 [ 9724/20164 ( 48%)], Train Loss: 0.54564\n",
      "Epoch: 00 [ 9764/20164 ( 48%)], Train Loss: 0.54507\n",
      "Epoch: 00 [ 9804/20164 ( 49%)], Train Loss: 0.54513\n",
      "Epoch: 00 [ 9844/20164 ( 49%)], Train Loss: 0.54538\n",
      "Epoch: 00 [ 9884/20164 ( 49%)], Train Loss: 0.54481\n",
      "Epoch: 00 [ 9924/20164 ( 49%)], Train Loss: 0.54423\n",
      "Epoch: 00 [ 9964/20164 ( 49%)], Train Loss: 0.54296\n",
      "Epoch: 00 [10004/20164 ( 50%)], Train Loss: 0.54182\n",
      "Epoch: 00 [10044/20164 ( 50%)], Train Loss: 0.54171\n",
      "Epoch: 00 [10084/20164 ( 50%)], Train Loss: 0.54069\n",
      "Epoch: 00 [10124/20164 ( 50%)], Train Loss: 0.54003\n",
      "Epoch: 00 [10164/20164 ( 50%)], Train Loss: 0.53948\n",
      "Epoch: 00 [10204/20164 ( 51%)], Train Loss: 0.53862\n",
      "Epoch: 00 [10244/20164 ( 51%)], Train Loss: 0.53823\n",
      "Epoch: 00 [10284/20164 ( 51%)], Train Loss: 0.53717\n",
      "Epoch: 00 [10324/20164 ( 51%)], Train Loss: 0.53596\n",
      "Epoch: 00 [10364/20164 ( 51%)], Train Loss: 0.53643\n",
      "Epoch: 00 [10404/20164 ( 52%)], Train Loss: 0.53595\n",
      "Epoch: 00 [10444/20164 ( 52%)], Train Loss: 0.53533\n",
      "Epoch: 00 [10484/20164 ( 52%)], Train Loss: 0.53538\n",
      "Epoch: 00 [10524/20164 ( 52%)], Train Loss: 0.53535\n",
      "Epoch: 00 [10564/20164 ( 52%)], Train Loss: 0.53508\n",
      "Epoch: 00 [10604/20164 ( 53%)], Train Loss: 0.53457\n",
      "Epoch: 00 [10644/20164 ( 53%)], Train Loss: 0.53402\n",
      "Epoch: 00 [10684/20164 ( 53%)], Train Loss: 0.53413\n",
      "Epoch: 00 [10724/20164 ( 53%)], Train Loss: 0.53381\n",
      "Epoch: 00 [10764/20164 ( 53%)], Train Loss: 0.53285\n",
      "Epoch: 00 [10804/20164 ( 54%)], Train Loss: 0.53219\n",
      "Epoch: 00 [10844/20164 ( 54%)], Train Loss: 0.53187\n",
      "Epoch: 00 [10884/20164 ( 54%)], Train Loss: 0.53131\n",
      "Epoch: 00 [10924/20164 ( 54%)], Train Loss: 0.53087\n",
      "Epoch: 00 [10964/20164 ( 54%)], Train Loss: 0.53056\n",
      "Epoch: 00 [11004/20164 ( 55%)], Train Loss: 0.52986\n",
      "Epoch: 00 [11044/20164 ( 55%)], Train Loss: 0.52939\n",
      "Epoch: 00 [11084/20164 ( 55%)], Train Loss: 0.52940\n",
      "Epoch: 00 [11124/20164 ( 55%)], Train Loss: 0.52884\n",
      "Epoch: 00 [11164/20164 ( 55%)], Train Loss: 0.52849\n",
      "Epoch: 00 [11204/20164 ( 56%)], Train Loss: 0.52819\n",
      "Epoch: 00 [11244/20164 ( 56%)], Train Loss: 0.52809\n",
      "Epoch: 00 [11284/20164 ( 56%)], Train Loss: 0.52767\n",
      "Epoch: 00 [11324/20164 ( 56%)], Train Loss: 0.52725\n",
      "Epoch: 00 [11364/20164 ( 56%)], Train Loss: 0.52676\n",
      "Epoch: 00 [11404/20164 ( 57%)], Train Loss: 0.52616\n",
      "Epoch: 00 [11444/20164 ( 57%)], Train Loss: 0.52539\n",
      "Epoch: 00 [11484/20164 ( 57%)], Train Loss: 0.52467\n",
      "Epoch: 00 [11524/20164 ( 57%)], Train Loss: 0.52435\n",
      "Epoch: 00 [11564/20164 ( 57%)], Train Loss: 0.52413\n",
      "Epoch: 00 [11604/20164 ( 58%)], Train Loss: 0.52468\n",
      "Epoch: 00 [11644/20164 ( 58%)], Train Loss: 0.52385\n",
      "Epoch: 00 [11684/20164 ( 58%)], Train Loss: 0.52396\n",
      "Epoch: 00 [11724/20164 ( 58%)], Train Loss: 0.52454\n",
      "Epoch: 00 [11764/20164 ( 58%)], Train Loss: 0.52374\n",
      "Epoch: 00 [11804/20164 ( 59%)], Train Loss: 0.52345\n",
      "Epoch: 00 [11844/20164 ( 59%)], Train Loss: 0.52242\n",
      "Epoch: 00 [11884/20164 ( 59%)], Train Loss: 0.52185\n",
      "Epoch: 00 [11924/20164 ( 59%)], Train Loss: 0.52106\n",
      "Epoch: 00 [11964/20164 ( 59%)], Train Loss: 0.51986\n",
      "Epoch: 00 [12004/20164 ( 60%)], Train Loss: 0.51929\n",
      "Epoch: 00 [12044/20164 ( 60%)], Train Loss: 0.51850\n",
      "Epoch: 00 [12084/20164 ( 60%)], Train Loss: 0.51808\n",
      "Epoch: 00 [12124/20164 ( 60%)], Train Loss: 0.51750\n",
      "Epoch: 00 [12164/20164 ( 60%)], Train Loss: 0.51661\n",
      "Epoch: 00 [12204/20164 ( 61%)], Train Loss: 0.51585\n",
      "Epoch: 00 [12244/20164 ( 61%)], Train Loss: 0.51517\n",
      "Epoch: 00 [12284/20164 ( 61%)], Train Loss: 0.51453\n",
      "Epoch: 00 [12324/20164 ( 61%)], Train Loss: 0.51448\n",
      "Epoch: 00 [12364/20164 ( 61%)], Train Loss: 0.51402\n",
      "Epoch: 00 [12404/20164 ( 62%)], Train Loss: 0.51358\n",
      "Epoch: 00 [12444/20164 ( 62%)], Train Loss: 0.51316\n",
      "Epoch: 00 [12484/20164 ( 62%)], Train Loss: 0.51300\n",
      "Epoch: 00 [12524/20164 ( 62%)], Train Loss: 0.51289\n",
      "Epoch: 00 [12564/20164 ( 62%)], Train Loss: 0.51268\n",
      "Epoch: 00 [12604/20164 ( 63%)], Train Loss: 0.51219\n",
      "Epoch: 00 [12644/20164 ( 63%)], Train Loss: 0.51166\n",
      "Epoch: 00 [12684/20164 ( 63%)], Train Loss: 0.51086\n",
      "Epoch: 00 [12724/20164 ( 63%)], Train Loss: 0.51050\n",
      "Epoch: 00 [12764/20164 ( 63%)], Train Loss: 0.50956\n",
      "Epoch: 00 [12804/20164 ( 63%)], Train Loss: 0.50900\n",
      "Epoch: 00 [12844/20164 ( 64%)], Train Loss: 0.50846\n",
      "Epoch: 00 [12884/20164 ( 64%)], Train Loss: 0.50827\n",
      "Epoch: 00 [12924/20164 ( 64%)], Train Loss: 0.50737\n",
      "Epoch: 00 [12964/20164 ( 64%)], Train Loss: 0.50710\n",
      "Epoch: 00 [13004/20164 ( 64%)], Train Loss: 0.50740\n",
      "Epoch: 00 [13044/20164 ( 65%)], Train Loss: 0.50719\n",
      "Epoch: 00 [13084/20164 ( 65%)], Train Loss: 0.50692\n",
      "Epoch: 00 [13124/20164 ( 65%)], Train Loss: 0.50634\n",
      "Epoch: 00 [13164/20164 ( 65%)], Train Loss: 0.50578\n",
      "Epoch: 00 [13204/20164 ( 65%)], Train Loss: 0.50481\n",
      "Epoch: 00 [13244/20164 ( 66%)], Train Loss: 0.50484\n",
      "Epoch: 00 [13284/20164 ( 66%)], Train Loss: 0.50466\n",
      "Epoch: 00 [13324/20164 ( 66%)], Train Loss: 0.50466\n",
      "Epoch: 00 [13364/20164 ( 66%)], Train Loss: 0.50491\n",
      "Epoch: 00 [13404/20164 ( 66%)], Train Loss: 0.50453\n",
      "Epoch: 00 [13444/20164 ( 67%)], Train Loss: 0.50451\n",
      "Epoch: 00 [13484/20164 ( 67%)], Train Loss: 0.50401\n",
      "Epoch: 00 [13524/20164 ( 67%)], Train Loss: 0.50414\n",
      "Epoch: 00 [13564/20164 ( 67%)], Train Loss: 0.50365\n",
      "Epoch: 00 [13604/20164 ( 67%)], Train Loss: 0.50321\n",
      "Epoch: 00 [13644/20164 ( 68%)], Train Loss: 0.50270\n",
      "Epoch: 00 [13684/20164 ( 68%)], Train Loss: 0.50238\n",
      "Epoch: 00 [13724/20164 ( 68%)], Train Loss: 0.50254\n",
      "Epoch: 00 [13764/20164 ( 68%)], Train Loss: 0.50212\n",
      "Epoch: 00 [13804/20164 ( 68%)], Train Loss: 0.50151\n",
      "Epoch: 00 [13844/20164 ( 69%)], Train Loss: 0.50087\n",
      "Epoch: 00 [13884/20164 ( 69%)], Train Loss: 0.50030\n",
      "Epoch: 00 [13924/20164 ( 69%)], Train Loss: 0.50008\n",
      "Epoch: 00 [13964/20164 ( 69%)], Train Loss: 0.49986\n",
      "Epoch: 00 [14004/20164 ( 69%)], Train Loss: 0.49897\n",
      "Epoch: 00 [14044/20164 ( 70%)], Train Loss: 0.49859\n",
      "Epoch: 00 [14084/20164 ( 70%)], Train Loss: 0.49816\n",
      "Epoch: 00 [14124/20164 ( 70%)], Train Loss: 0.49755\n",
      "Epoch: 00 [14164/20164 ( 70%)], Train Loss: 0.49694\n",
      "Epoch: 00 [14204/20164 ( 70%)], Train Loss: 0.49668\n",
      "Epoch: 00 [14244/20164 ( 71%)], Train Loss: 0.49616\n",
      "Epoch: 00 [14284/20164 ( 71%)], Train Loss: 0.49571\n",
      "Epoch: 00 [14324/20164 ( 71%)], Train Loss: 0.49536\n",
      "Epoch: 00 [14364/20164 ( 71%)], Train Loss: 0.49441\n",
      "Epoch: 00 [14404/20164 ( 71%)], Train Loss: 0.49401\n",
      "Epoch: 00 [14444/20164 ( 72%)], Train Loss: 0.49350\n",
      "Epoch: 00 [14484/20164 ( 72%)], Train Loss: 0.49276\n",
      "Epoch: 00 [14524/20164 ( 72%)], Train Loss: 0.49247\n",
      "Epoch: 00 [14564/20164 ( 72%)], Train Loss: 0.49238\n",
      "Epoch: 00 [14604/20164 ( 72%)], Train Loss: 0.49218\n",
      "Epoch: 00 [14644/20164 ( 73%)], Train Loss: 0.49163\n",
      "Epoch: 00 [14684/20164 ( 73%)], Train Loss: 0.49117\n",
      "Epoch: 00 [14724/20164 ( 73%)], Train Loss: 0.49022\n",
      "Epoch: 00 [14764/20164 ( 73%)], Train Loss: 0.48952\n",
      "Epoch: 00 [14804/20164 ( 73%)], Train Loss: 0.48907\n",
      "Epoch: 00 [14844/20164 ( 74%)], Train Loss: 0.48852\n",
      "Epoch: 00 [14884/20164 ( 74%)], Train Loss: 0.48848\n",
      "Epoch: 00 [14924/20164 ( 74%)], Train Loss: 0.48781\n",
      "Epoch: 00 [14964/20164 ( 74%)], Train Loss: 0.48731\n",
      "Epoch: 00 [15004/20164 ( 74%)], Train Loss: 0.48731\n",
      "Epoch: 00 [15044/20164 ( 75%)], Train Loss: 0.48678\n",
      "Epoch: 00 [15084/20164 ( 75%)], Train Loss: 0.48669\n",
      "Epoch: 00 [15124/20164 ( 75%)], Train Loss: 0.48607\n",
      "Epoch: 00 [15164/20164 ( 75%)], Train Loss: 0.48596\n",
      "Epoch: 00 [15204/20164 ( 75%)], Train Loss: 0.48558\n",
      "Epoch: 00 [15244/20164 ( 76%)], Train Loss: 0.48493\n",
      "Epoch: 00 [15284/20164 ( 76%)], Train Loss: 0.48520\n",
      "Epoch: 00 [15324/20164 ( 76%)], Train Loss: 0.48504\n",
      "Epoch: 00 [15364/20164 ( 76%)], Train Loss: 0.48454\n",
      "Epoch: 00 [15404/20164 ( 76%)], Train Loss: 0.48385\n",
      "Epoch: 00 [15444/20164 ( 77%)], Train Loss: 0.48369\n",
      "Epoch: 00 [15484/20164 ( 77%)], Train Loss: 0.48318\n",
      "Epoch: 00 [15524/20164 ( 77%)], Train Loss: 0.48276\n",
      "Epoch: 00 [15564/20164 ( 77%)], Train Loss: 0.48202\n",
      "Epoch: 00 [15604/20164 ( 77%)], Train Loss: 0.48154\n",
      "Epoch: 00 [15644/20164 ( 78%)], Train Loss: 0.48190\n",
      "Epoch: 00 [15684/20164 ( 78%)], Train Loss: 0.48153\n",
      "Epoch: 00 [15724/20164 ( 78%)], Train Loss: 0.48157\n",
      "Epoch: 00 [15764/20164 ( 78%)], Train Loss: 0.48135\n",
      "Epoch: 00 [15804/20164 ( 78%)], Train Loss: 0.48093\n",
      "Epoch: 00 [15844/20164 ( 79%)], Train Loss: 0.48034\n",
      "Epoch: 00 [15884/20164 ( 79%)], Train Loss: 0.47982\n",
      "Epoch: 00 [15924/20164 ( 79%)], Train Loss: 0.47930\n",
      "Epoch: 00 [15964/20164 ( 79%)], Train Loss: 0.47868\n",
      "Epoch: 00 [16004/20164 ( 79%)], Train Loss: 0.47809\n",
      "Epoch: 00 [16044/20164 ( 80%)], Train Loss: 0.47746\n",
      "Epoch: 00 [16084/20164 ( 80%)], Train Loss: 0.47719\n",
      "Epoch: 00 [16124/20164 ( 80%)], Train Loss: 0.47697\n",
      "Epoch: 00 [16164/20164 ( 80%)], Train Loss: 0.47697\n",
      "Epoch: 00 [16204/20164 ( 80%)], Train Loss: 0.47689\n",
      "Epoch: 00 [16244/20164 ( 81%)], Train Loss: 0.47659\n",
      "Epoch: 00 [16284/20164 ( 81%)], Train Loss: 0.47588\n",
      "Epoch: 00 [16324/20164 ( 81%)], Train Loss: 0.47540\n",
      "Epoch: 00 [16364/20164 ( 81%)], Train Loss: 0.47510\n",
      "Epoch: 00 [16404/20164 ( 81%)], Train Loss: 0.47443\n",
      "Epoch: 00 [16444/20164 ( 82%)], Train Loss: 0.47443\n",
      "Epoch: 00 [16484/20164 ( 82%)], Train Loss: 0.47377\n",
      "Epoch: 00 [16524/20164 ( 82%)], Train Loss: 0.47331\n",
      "Epoch: 00 [16564/20164 ( 82%)], Train Loss: 0.47325\n",
      "Epoch: 00 [16604/20164 ( 82%)], Train Loss: 0.47304\n",
      "Epoch: 00 [16644/20164 ( 83%)], Train Loss: 0.47290\n",
      "Epoch: 00 [16684/20164 ( 83%)], Train Loss: 0.47269\n",
      "Epoch: 00 [16724/20164 ( 83%)], Train Loss: 0.47226\n",
      "Epoch: 00 [16764/20164 ( 83%)], Train Loss: 0.47180\n",
      "Epoch: 00 [16804/20164 ( 83%)], Train Loss: 0.47135\n",
      "Epoch: 00 [16844/20164 ( 84%)], Train Loss: 0.47075\n",
      "Epoch: 00 [16884/20164 ( 84%)], Train Loss: 0.47073\n",
      "Epoch: 00 [16924/20164 ( 84%)], Train Loss: 0.47032\n",
      "Epoch: 00 [16964/20164 ( 84%)], Train Loss: 0.46990\n",
      "Epoch: 00 [17004/20164 ( 84%)], Train Loss: 0.46974\n",
      "Epoch: 00 [17044/20164 ( 85%)], Train Loss: 0.46943\n",
      "Epoch: 00 [17084/20164 ( 85%)], Train Loss: 0.46923\n",
      "Epoch: 00 [17124/20164 ( 85%)], Train Loss: 0.46873\n",
      "Epoch: 00 [17164/20164 ( 85%)], Train Loss: 0.46830\n",
      "Epoch: 00 [17204/20164 ( 85%)], Train Loss: 0.46869\n",
      "Epoch: 00 [17244/20164 ( 86%)], Train Loss: 0.46918\n",
      "Epoch: 00 [17284/20164 ( 86%)], Train Loss: 0.46905\n",
      "Epoch: 00 [17324/20164 ( 86%)], Train Loss: 0.46885\n",
      "Epoch: 00 [17364/20164 ( 86%)], Train Loss: 0.46832\n",
      "Epoch: 00 [17404/20164 ( 86%)], Train Loss: 0.46808\n",
      "Epoch: 00 [17444/20164 ( 87%)], Train Loss: 0.46774\n",
      "Epoch: 00 [17484/20164 ( 87%)], Train Loss: 0.46731\n",
      "Epoch: 00 [17524/20164 ( 87%)], Train Loss: 0.46674\n",
      "Epoch: 00 [17564/20164 ( 87%)], Train Loss: 0.46608\n",
      "Epoch: 00 [17604/20164 ( 87%)], Train Loss: 0.46554\n",
      "Epoch: 00 [17644/20164 ( 88%)], Train Loss: 0.46488\n",
      "Epoch: 00 [17684/20164 ( 88%)], Train Loss: 0.46463\n",
      "Epoch: 00 [17724/20164 ( 88%)], Train Loss: 0.46424\n",
      "Epoch: 00 [17764/20164 ( 88%)], Train Loss: 0.46415\n",
      "Epoch: 00 [17804/20164 ( 88%)], Train Loss: 0.46388\n",
      "Epoch: 00 [17844/20164 ( 88%)], Train Loss: 0.46344\n",
      "Epoch: 00 [17884/20164 ( 89%)], Train Loss: 0.46298\n",
      "Epoch: 00 [17924/20164 ( 89%)], Train Loss: 0.46229\n",
      "Epoch: 00 [17964/20164 ( 89%)], Train Loss: 0.46187\n",
      "Epoch: 00 [18004/20164 ( 89%)], Train Loss: 0.46170\n",
      "Epoch: 00 [18044/20164 ( 89%)], Train Loss: 0.46189\n",
      "Epoch: 00 [18084/20164 ( 90%)], Train Loss: 0.46180\n",
      "Epoch: 00 [18124/20164 ( 90%)], Train Loss: 0.46183\n",
      "Epoch: 00 [18164/20164 ( 90%)], Train Loss: 0.46146\n",
      "Epoch: 00 [18204/20164 ( 90%)], Train Loss: 0.46119\n",
      "Epoch: 00 [18244/20164 ( 90%)], Train Loss: 0.46082\n",
      "Epoch: 00 [18284/20164 ( 91%)], Train Loss: 0.46125\n",
      "Epoch: 00 [18324/20164 ( 91%)], Train Loss: 0.46116\n",
      "Epoch: 00 [18364/20164 ( 91%)], Train Loss: 0.46107\n",
      "Epoch: 00 [18404/20164 ( 91%)], Train Loss: 0.46061\n",
      "Epoch: 00 [18444/20164 ( 91%)], Train Loss: 0.46004\n",
      "Epoch: 00 [18484/20164 ( 92%)], Train Loss: 0.45992\n",
      "Epoch: 00 [18524/20164 ( 92%)], Train Loss: 0.45946\n",
      "Epoch: 00 [18564/20164 ( 92%)], Train Loss: 0.45892\n",
      "Epoch: 00 [18604/20164 ( 92%)], Train Loss: 0.45872\n",
      "Epoch: 00 [18644/20164 ( 92%)], Train Loss: 0.45865\n",
      "Epoch: 00 [18684/20164 ( 93%)], Train Loss: 0.45850\n",
      "Epoch: 00 [18724/20164 ( 93%)], Train Loss: 0.45799\n",
      "Epoch: 00 [18764/20164 ( 93%)], Train Loss: 0.45743\n",
      "Epoch: 00 [18804/20164 ( 93%)], Train Loss: 0.45683\n",
      "Epoch: 00 [18844/20164 ( 93%)], Train Loss: 0.45648\n",
      "Epoch: 00 [18884/20164 ( 94%)], Train Loss: 0.45640\n",
      "Epoch: 00 [18924/20164 ( 94%)], Train Loss: 0.45578\n",
      "Epoch: 00 [18964/20164 ( 94%)], Train Loss: 0.45533\n",
      "Epoch: 00 [19004/20164 ( 94%)], Train Loss: 0.45519\n",
      "Epoch: 00 [19044/20164 ( 94%)], Train Loss: 0.45483\n",
      "Epoch: 00 [19084/20164 ( 95%)], Train Loss: 0.45440\n",
      "Epoch: 00 [19124/20164 ( 95%)], Train Loss: 0.45417\n",
      "Epoch: 00 [19164/20164 ( 95%)], Train Loss: 0.45397\n",
      "Epoch: 00 [19204/20164 ( 95%)], Train Loss: 0.45380\n",
      "Epoch: 00 [19244/20164 ( 95%)], Train Loss: 0.45339\n",
      "Epoch: 00 [19284/20164 ( 96%)], Train Loss: 0.45342\n",
      "Epoch: 00 [19324/20164 ( 96%)], Train Loss: 0.45297\n",
      "Epoch: 00 [19364/20164 ( 96%)], Train Loss: 0.45271\n",
      "Epoch: 00 [19404/20164 ( 96%)], Train Loss: 0.45261\n",
      "Epoch: 00 [19444/20164 ( 96%)], Train Loss: 0.45233\n",
      "Epoch: 00 [19484/20164 ( 97%)], Train Loss: 0.45212\n",
      "Epoch: 00 [19524/20164 ( 97%)], Train Loss: 0.45213\n",
      "Epoch: 00 [19564/20164 ( 97%)], Train Loss: 0.45192\n",
      "Epoch: 00 [19604/20164 ( 97%)], Train Loss: 0.45188\n",
      "Epoch: 00 [19644/20164 ( 97%)], Train Loss: 0.45177\n",
      "Epoch: 00 [19684/20164 ( 98%)], Train Loss: 0.45208\n",
      "Epoch: 00 [19724/20164 ( 98%)], Train Loss: 0.45174\n",
      "Epoch: 00 [19764/20164 ( 98%)], Train Loss: 0.45156\n",
      "Epoch: 00 [19804/20164 ( 98%)], Train Loss: 0.45155\n",
      "Epoch: 00 [19844/20164 ( 98%)], Train Loss: 0.45143\n",
      "Epoch: 00 [19884/20164 ( 99%)], Train Loss: 0.45132\n",
      "Epoch: 00 [19924/20164 ( 99%)], Train Loss: 0.45102\n",
      "Epoch: 00 [19964/20164 ( 99%)], Train Loss: 0.45082\n",
      "Epoch: 00 [20004/20164 ( 99%)], Train Loss: 0.45056\n",
      "Epoch: 00 [20044/20164 ( 99%)], Train Loss: 0.45053\n",
      "Epoch: 00 [20084/20164 (100%)], Train Loss: 0.45028\n",
      "Epoch: 00 [20124/20164 (100%)], Train Loss: 0.45057\n",
      "Epoch: 00 [20164/20164 (100%)], Train Loss: 0.45071\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 0.22045\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 0.22045\n",
      "Saving model checkpoint to output/checkpoint-fold-4.\n",
      "\n",
      "Epoch: 01 [    4/20164 (  0%)], Train Loss: 0.51259\n",
      "Epoch: 01 [   44/20164 (  0%)], Train Loss: 0.31509\n",
      "Epoch: 01 [   84/20164 (  0%)], Train Loss: 0.34863\n",
      "Epoch: 01 [  124/20164 (  1%)], Train Loss: 0.36191\n",
      "Epoch: 01 [  164/20164 (  1%)], Train Loss: 0.32905\n",
      "Epoch: 01 [  204/20164 (  1%)], Train Loss: 0.31817\n",
      "Epoch: 01 [  244/20164 (  1%)], Train Loss: 0.32306\n",
      "Epoch: 01 [  284/20164 (  1%)], Train Loss: 0.32392\n",
      "Epoch: 01 [  324/20164 (  2%)], Train Loss: 0.32913\n",
      "Epoch: 01 [  364/20164 (  2%)], Train Loss: 0.34870\n",
      "Epoch: 01 [  404/20164 (  2%)], Train Loss: 0.35316\n",
      "Epoch: 01 [  444/20164 (  2%)], Train Loss: 0.35470\n",
      "Epoch: 01 [  484/20164 (  2%)], Train Loss: 0.34765\n",
      "Epoch: 01 [  524/20164 (  3%)], Train Loss: 0.34375\n",
      "Epoch: 01 [  564/20164 (  3%)], Train Loss: 0.33167\n",
      "Epoch: 01 [  604/20164 (  3%)], Train Loss: 0.32607\n",
      "Epoch: 01 [  644/20164 (  3%)], Train Loss: 0.32718\n",
      "Epoch: 01 [  684/20164 (  3%)], Train Loss: 0.32253\n",
      "Epoch: 01 [  724/20164 (  4%)], Train Loss: 0.31537\n",
      "Epoch: 01 [  764/20164 (  4%)], Train Loss: 0.31241\n",
      "Epoch: 01 [  804/20164 (  4%)], Train Loss: 0.31728\n",
      "Epoch: 01 [  844/20164 (  4%)], Train Loss: 0.31566\n",
      "Epoch: 01 [  884/20164 (  4%)], Train Loss: 0.31397\n",
      "Epoch: 01 [  924/20164 (  5%)], Train Loss: 0.30760\n",
      "Epoch: 01 [  964/20164 (  5%)], Train Loss: 0.29921\n",
      "Epoch: 01 [ 1004/20164 (  5%)], Train Loss: 0.29641\n",
      "Epoch: 01 [ 1044/20164 (  5%)], Train Loss: 0.29510\n",
      "Epoch: 01 [ 1084/20164 (  5%)], Train Loss: 0.29384\n",
      "Epoch: 01 [ 1124/20164 (  6%)], Train Loss: 0.29173\n",
      "Epoch: 01 [ 1164/20164 (  6%)], Train Loss: 0.28763\n",
      "Epoch: 01 [ 1204/20164 (  6%)], Train Loss: 0.29388\n",
      "Epoch: 01 [ 1244/20164 (  6%)], Train Loss: 0.29437\n",
      "Epoch: 01 [ 1284/20164 (  6%)], Train Loss: 0.29374\n",
      "Epoch: 01 [ 1324/20164 (  7%)], Train Loss: 0.29644\n",
      "Epoch: 01 [ 1364/20164 (  7%)], Train Loss: 0.29631\n",
      "Epoch: 01 [ 1404/20164 (  7%)], Train Loss: 0.29346\n",
      "Epoch: 01 [ 1444/20164 (  7%)], Train Loss: 0.29281\n",
      "Epoch: 01 [ 1484/20164 (  7%)], Train Loss: 0.29008\n",
      "Epoch: 01 [ 1524/20164 (  8%)], Train Loss: 0.29099\n",
      "Epoch: 01 [ 1564/20164 (  8%)], Train Loss: 0.28797\n",
      "Epoch: 01 [ 1604/20164 (  8%)], Train Loss: 0.28446\n",
      "Epoch: 01 [ 1644/20164 (  8%)], Train Loss: 0.28128\n",
      "Epoch: 01 [ 1684/20164 (  8%)], Train Loss: 0.27893\n",
      "Epoch: 01 [ 1724/20164 (  9%)], Train Loss: 0.27849\n",
      "Epoch: 01 [ 1764/20164 (  9%)], Train Loss: 0.27867\n",
      "Epoch: 01 [ 1804/20164 (  9%)], Train Loss: 0.27651\n",
      "Epoch: 01 [ 1844/20164 (  9%)], Train Loss: 0.27461\n",
      "Epoch: 01 [ 1884/20164 (  9%)], Train Loss: 0.27166\n",
      "Epoch: 01 [ 1924/20164 ( 10%)], Train Loss: 0.27560\n",
      "Epoch: 01 [ 1964/20164 ( 10%)], Train Loss: 0.27378\n",
      "Epoch: 01 [ 2004/20164 ( 10%)], Train Loss: 0.27630\n",
      "Epoch: 01 [ 2044/20164 ( 10%)], Train Loss: 0.27490\n",
      "Epoch: 01 [ 2084/20164 ( 10%)], Train Loss: 0.27284\n",
      "Epoch: 01 [ 2124/20164 ( 11%)], Train Loss: 0.27165\n",
      "Epoch: 01 [ 2164/20164 ( 11%)], Train Loss: 0.27073\n",
      "Epoch: 01 [ 2204/20164 ( 11%)], Train Loss: 0.27161\n",
      "Epoch: 01 [ 2244/20164 ( 11%)], Train Loss: 0.27251\n",
      "Epoch: 01 [ 2284/20164 ( 11%)], Train Loss: 0.27082\n",
      "Epoch: 01 [ 2324/20164 ( 12%)], Train Loss: 0.27131\n",
      "Epoch: 01 [ 2364/20164 ( 12%)], Train Loss: 0.26817\n",
      "Epoch: 01 [ 2404/20164 ( 12%)], Train Loss: 0.26761\n",
      "Epoch: 01 [ 2444/20164 ( 12%)], Train Loss: 0.26559\n",
      "Epoch: 01 [ 2484/20164 ( 12%)], Train Loss: 0.26631\n",
      "Epoch: 01 [ 2524/20164 ( 13%)], Train Loss: 0.26673\n",
      "Epoch: 01 [ 2564/20164 ( 13%)], Train Loss: 0.26525\n",
      "Epoch: 01 [ 2604/20164 ( 13%)], Train Loss: 0.26365\n",
      "Epoch: 01 [ 2644/20164 ( 13%)], Train Loss: 0.26385\n",
      "Epoch: 01 [ 2684/20164 ( 13%)], Train Loss: 0.26380\n",
      "Epoch: 01 [ 2724/20164 ( 14%)], Train Loss: 0.26271\n",
      "Epoch: 01 [ 2764/20164 ( 14%)], Train Loss: 0.26053\n",
      "Epoch: 01 [ 2804/20164 ( 14%)], Train Loss: 0.26198\n",
      "Epoch: 01 [ 2844/20164 ( 14%)], Train Loss: 0.26156\n",
      "Epoch: 01 [ 2884/20164 ( 14%)], Train Loss: 0.26002\n",
      "Epoch: 01 [ 2924/20164 ( 15%)], Train Loss: 0.26256\n",
      "Epoch: 01 [ 2964/20164 ( 15%)], Train Loss: 0.26239\n",
      "Epoch: 01 [ 3004/20164 ( 15%)], Train Loss: 0.26046\n",
      "Epoch: 01 [ 3044/20164 ( 15%)], Train Loss: 0.26011\n",
      "Epoch: 01 [ 3084/20164 ( 15%)], Train Loss: 0.25813\n",
      "Epoch: 01 [ 3124/20164 ( 15%)], Train Loss: 0.25715\n",
      "Epoch: 01 [ 3164/20164 ( 16%)], Train Loss: 0.25657\n",
      "Epoch: 01 [ 3204/20164 ( 16%)], Train Loss: 0.25556\n",
      "Epoch: 01 [ 3244/20164 ( 16%)], Train Loss: 0.25601\n",
      "Epoch: 01 [ 3284/20164 ( 16%)], Train Loss: 0.25479\n",
      "Epoch: 01 [ 3324/20164 ( 16%)], Train Loss: 0.25338\n",
      "Epoch: 01 [ 3364/20164 ( 17%)], Train Loss: 0.25154\n",
      "Epoch: 01 [ 3404/20164 ( 17%)], Train Loss: 0.25002\n",
      "Epoch: 01 [ 3444/20164 ( 17%)], Train Loss: 0.24859\n",
      "Epoch: 01 [ 3484/20164 ( 17%)], Train Loss: 0.24709\n",
      "Epoch: 01 [ 3524/20164 ( 17%)], Train Loss: 0.24579\n",
      "Epoch: 01 [ 3564/20164 ( 18%)], Train Loss: 0.24522\n",
      "Epoch: 01 [ 3604/20164 ( 18%)], Train Loss: 0.24356\n",
      "Epoch: 01 [ 3644/20164 ( 18%)], Train Loss: 0.24244\n",
      "Epoch: 01 [ 3684/20164 ( 18%)], Train Loss: 0.24075\n",
      "Epoch: 01 [ 3724/20164 ( 18%)], Train Loss: 0.24051\n",
      "Epoch: 01 [ 3764/20164 ( 19%)], Train Loss: 0.23872\n",
      "Epoch: 01 [ 3804/20164 ( 19%)], Train Loss: 0.23758\n",
      "Epoch: 01 [ 3844/20164 ( 19%)], Train Loss: 0.23690\n",
      "Epoch: 01 [ 3884/20164 ( 19%)], Train Loss: 0.23637\n",
      "Epoch: 01 [ 3924/20164 ( 19%)], Train Loss: 0.23488\n",
      "Epoch: 01 [ 3964/20164 ( 20%)], Train Loss: 0.23406\n",
      "Epoch: 01 [ 4004/20164 ( 20%)], Train Loss: 0.23306\n",
      "Epoch: 01 [ 4044/20164 ( 20%)], Train Loss: 0.23290\n",
      "Epoch: 01 [ 4084/20164 ( 20%)], Train Loss: 0.23184\n",
      "Epoch: 01 [ 4124/20164 ( 20%)], Train Loss: 0.23027\n",
      "Epoch: 01 [ 4164/20164 ( 21%)], Train Loss: 0.22964\n",
      "Epoch: 01 [ 4204/20164 ( 21%)], Train Loss: 0.22891\n",
      "Epoch: 01 [ 4244/20164 ( 21%)], Train Loss: 0.22749\n",
      "Epoch: 01 [ 4284/20164 ( 21%)], Train Loss: 0.22581\n",
      "Epoch: 01 [ 4324/20164 ( 21%)], Train Loss: 0.22615\n",
      "Epoch: 01 [ 4364/20164 ( 22%)], Train Loss: 0.22465\n",
      "Epoch: 01 [ 4404/20164 ( 22%)], Train Loss: 0.22330\n",
      "Epoch: 01 [ 4444/20164 ( 22%)], Train Loss: 0.22201\n",
      "Epoch: 01 [ 4484/20164 ( 22%)], Train Loss: 0.22041\n",
      "Epoch: 01 [ 4524/20164 ( 22%)], Train Loss: 0.22006\n",
      "Epoch: 01 [ 4564/20164 ( 23%)], Train Loss: 0.21942\n",
      "Epoch: 01 [ 4604/20164 ( 23%)], Train Loss: 0.21991\n",
      "Epoch: 01 [ 4644/20164 ( 23%)], Train Loss: 0.21957\n",
      "Epoch: 01 [ 4684/20164 ( 23%)], Train Loss: 0.21995\n",
      "Epoch: 01 [ 4724/20164 ( 23%)], Train Loss: 0.21917\n",
      "Epoch: 01 [ 4764/20164 ( 24%)], Train Loss: 0.21833\n",
      "Epoch: 01 [ 4804/20164 ( 24%)], Train Loss: 0.21827\n",
      "Epoch: 01 [ 4844/20164 ( 24%)], Train Loss: 0.21877\n",
      "Epoch: 01 [ 4884/20164 ( 24%)], Train Loss: 0.21733\n",
      "Epoch: 01 [ 4924/20164 ( 24%)], Train Loss: 0.21627\n",
      "Epoch: 01 [ 4964/20164 ( 25%)], Train Loss: 0.21517\n",
      "Epoch: 01 [ 5004/20164 ( 25%)], Train Loss: 0.21411\n",
      "Epoch: 01 [ 5044/20164 ( 25%)], Train Loss: 0.21364\n",
      "Epoch: 01 [ 5084/20164 ( 25%)], Train Loss: 0.21298\n",
      "Epoch: 01 [ 5124/20164 ( 25%)], Train Loss: 0.21210\n",
      "Epoch: 01 [ 5164/20164 ( 26%)], Train Loss: 0.21093\n",
      "Epoch: 01 [ 5204/20164 ( 26%)], Train Loss: 0.21044\n",
      "Epoch: 01 [ 5244/20164 ( 26%)], Train Loss: 0.21061\n",
      "Epoch: 01 [ 5284/20164 ( 26%)], Train Loss: 0.20945\n",
      "Epoch: 01 [ 5324/20164 ( 26%)], Train Loss: 0.20921\n",
      "Epoch: 01 [ 5364/20164 ( 27%)], Train Loss: 0.20790\n",
      "Epoch: 01 [ 5404/20164 ( 27%)], Train Loss: 0.20831\n",
      "Epoch: 01 [ 5444/20164 ( 27%)], Train Loss: 0.20763\n",
      "Epoch: 01 [ 5484/20164 ( 27%)], Train Loss: 0.20731\n",
      "Epoch: 01 [ 5524/20164 ( 27%)], Train Loss: 0.20741\n",
      "Epoch: 01 [ 5564/20164 ( 28%)], Train Loss: 0.20707\n",
      "Epoch: 01 [ 5604/20164 ( 28%)], Train Loss: 0.20639\n",
      "Epoch: 01 [ 5644/20164 ( 28%)], Train Loss: 0.20573\n",
      "Epoch: 01 [ 5684/20164 ( 28%)], Train Loss: 0.20501\n",
      "Epoch: 01 [ 5724/20164 ( 28%)], Train Loss: 0.20418\n",
      "Epoch: 01 [ 5764/20164 ( 29%)], Train Loss: 0.20397\n",
      "Epoch: 01 [ 5804/20164 ( 29%)], Train Loss: 0.20288\n",
      "Epoch: 01 [ 5844/20164 ( 29%)], Train Loss: 0.20242\n",
      "Epoch: 01 [ 5884/20164 ( 29%)], Train Loss: 0.20172\n",
      "Epoch: 01 [ 5924/20164 ( 29%)], Train Loss: 0.20096\n",
      "Epoch: 01 [ 5964/20164 ( 30%)], Train Loss: 0.20053\n",
      "Epoch: 01 [ 6004/20164 ( 30%)], Train Loss: 0.20058\n",
      "Epoch: 01 [ 6044/20164 ( 30%)], Train Loss: 0.20034\n",
      "Epoch: 01 [ 6084/20164 ( 30%)], Train Loss: 0.20055\n",
      "Epoch: 01 [ 6124/20164 ( 30%)], Train Loss: 0.20029\n",
      "Epoch: 01 [ 6164/20164 ( 31%)], Train Loss: 0.20009\n",
      "Epoch: 01 [ 6204/20164 ( 31%)], Train Loss: 0.19995\n",
      "Epoch: 01 [ 6244/20164 ( 31%)], Train Loss: 0.19926\n",
      "Epoch: 01 [ 6284/20164 ( 31%)], Train Loss: 0.19851\n",
      "Epoch: 01 [ 6324/20164 ( 31%)], Train Loss: 0.19910\n",
      "Epoch: 01 [ 6364/20164 ( 32%)], Train Loss: 0.19895\n",
      "Epoch: 01 [ 6404/20164 ( 32%)], Train Loss: 0.19835\n",
      "Epoch: 01 [ 6444/20164 ( 32%)], Train Loss: 0.19808\n",
      "Epoch: 01 [ 6484/20164 ( 32%)], Train Loss: 0.19723\n",
      "Epoch: 01 [ 6524/20164 ( 32%)], Train Loss: 0.19668\n",
      "Epoch: 01 [ 6564/20164 ( 33%)], Train Loss: 0.19619\n",
      "Epoch: 01 [ 6604/20164 ( 33%)], Train Loss: 0.19620\n",
      "Epoch: 01 [ 6644/20164 ( 33%)], Train Loss: 0.19570\n",
      "Epoch: 01 [ 6684/20164 ( 33%)], Train Loss: 0.19554\n",
      "Epoch: 01 [ 6724/20164 ( 33%)], Train Loss: 0.19605\n",
      "Epoch: 01 [ 6764/20164 ( 34%)], Train Loss: 0.19554\n",
      "Epoch: 01 [ 6804/20164 ( 34%)], Train Loss: 0.19502\n",
      "Epoch: 01 [ 6844/20164 ( 34%)], Train Loss: 0.19417\n",
      "Epoch: 01 [ 6884/20164 ( 34%)], Train Loss: 0.19467\n",
      "Epoch: 01 [ 6924/20164 ( 34%)], Train Loss: 0.19392\n",
      "Epoch: 01 [ 6964/20164 ( 35%)], Train Loss: 0.19327\n",
      "Epoch: 01 [ 7004/20164 ( 35%)], Train Loss: 0.19286\n",
      "Epoch: 01 [ 7044/20164 ( 35%)], Train Loss: 0.19220\n",
      "Epoch: 01 [ 7084/20164 ( 35%)], Train Loss: 0.19176\n",
      "Epoch: 01 [ 7124/20164 ( 35%)], Train Loss: 0.19104\n",
      "Epoch: 01 [ 7164/20164 ( 36%)], Train Loss: 0.19068\n",
      "Epoch: 01 [ 7204/20164 ( 36%)], Train Loss: 0.19047\n",
      "Epoch: 01 [ 7244/20164 ( 36%)], Train Loss: 0.19017\n",
      "Epoch: 01 [ 7284/20164 ( 36%)], Train Loss: 0.19029\n",
      "Epoch: 01 [ 7324/20164 ( 36%)], Train Loss: 0.18961\n",
      "Epoch: 01 [ 7364/20164 ( 37%)], Train Loss: 0.18929\n",
      "Epoch: 01 [ 7404/20164 ( 37%)], Train Loss: 0.18918\n",
      "Epoch: 01 [ 7444/20164 ( 37%)], Train Loss: 0.18883\n",
      "Epoch: 01 [ 7484/20164 ( 37%)], Train Loss: 0.18874\n",
      "Epoch: 01 [ 7524/20164 ( 37%)], Train Loss: 0.18835\n",
      "Epoch: 01 [ 7564/20164 ( 38%)], Train Loss: 0.18785\n",
      "Epoch: 01 [ 7604/20164 ( 38%)], Train Loss: 0.18762\n",
      "Epoch: 01 [ 7644/20164 ( 38%)], Train Loss: 0.18727\n",
      "Epoch: 01 [ 7684/20164 ( 38%)], Train Loss: 0.18694\n",
      "Epoch: 01 [ 7724/20164 ( 38%)], Train Loss: 0.18707\n",
      "Epoch: 01 [ 7764/20164 ( 39%)], Train Loss: 0.18666\n",
      "Epoch: 01 [ 7804/20164 ( 39%)], Train Loss: 0.18661\n",
      "Epoch: 01 [ 7844/20164 ( 39%)], Train Loss: 0.18650\n",
      "Epoch: 01 [ 7884/20164 ( 39%)], Train Loss: 0.18647\n",
      "Epoch: 01 [ 7924/20164 ( 39%)], Train Loss: 0.18645\n",
      "Epoch: 01 [ 7964/20164 ( 39%)], Train Loss: 0.18668\n",
      "Epoch: 01 [ 8004/20164 ( 40%)], Train Loss: 0.18650\n",
      "Epoch: 01 [ 8044/20164 ( 40%)], Train Loss: 0.18653\n",
      "Epoch: 01 [ 8084/20164 ( 40%)], Train Loss: 0.18649\n",
      "Epoch: 01 [ 8124/20164 ( 40%)], Train Loss: 0.18621\n",
      "Epoch: 01 [ 8164/20164 ( 40%)], Train Loss: 0.18587\n",
      "Epoch: 01 [ 8204/20164 ( 41%)], Train Loss: 0.18530\n",
      "Epoch: 01 [ 8244/20164 ( 41%)], Train Loss: 0.18477\n",
      "Epoch: 01 [ 8284/20164 ( 41%)], Train Loss: 0.18446\n",
      "Epoch: 01 [ 8324/20164 ( 41%)], Train Loss: 0.18396\n",
      "Epoch: 01 [ 8364/20164 ( 41%)], Train Loss: 0.18341\n",
      "Epoch: 01 [ 8404/20164 ( 42%)], Train Loss: 0.18338\n",
      "Epoch: 01 [ 8444/20164 ( 42%)], Train Loss: 0.18295\n",
      "Epoch: 01 [ 8484/20164 ( 42%)], Train Loss: 0.18227\n",
      "Epoch: 01 [ 8524/20164 ( 42%)], Train Loss: 0.18275\n",
      "Epoch: 01 [ 8564/20164 ( 42%)], Train Loss: 0.18253\n",
      "Epoch: 01 [ 8604/20164 ( 43%)], Train Loss: 0.18203\n",
      "Epoch: 01 [ 8644/20164 ( 43%)], Train Loss: 0.18163\n",
      "Epoch: 01 [ 8684/20164 ( 43%)], Train Loss: 0.18160\n",
      "Epoch: 01 [ 8724/20164 ( 43%)], Train Loss: 0.18144\n",
      "Epoch: 01 [ 8764/20164 ( 43%)], Train Loss: 0.18146\n",
      "Epoch: 01 [ 8804/20164 ( 44%)], Train Loss: 0.18102\n",
      "Epoch: 01 [ 8844/20164 ( 44%)], Train Loss: 0.18100\n",
      "Epoch: 01 [ 8884/20164 ( 44%)], Train Loss: 0.18076\n",
      "Epoch: 01 [ 8924/20164 ( 44%)], Train Loss: 0.18047\n",
      "Epoch: 01 [ 8964/20164 ( 44%)], Train Loss: 0.18018\n",
      "Epoch: 01 [ 9004/20164 ( 45%)], Train Loss: 0.17995\n",
      "Epoch: 01 [ 9044/20164 ( 45%)], Train Loss: 0.17967\n",
      "Epoch: 01 [ 9084/20164 ( 45%)], Train Loss: 0.17959\n",
      "Epoch: 01 [ 9124/20164 ( 45%)], Train Loss: 0.17961\n",
      "Epoch: 01 [ 9164/20164 ( 45%)], Train Loss: 0.17910\n",
      "Epoch: 01 [ 9204/20164 ( 46%)], Train Loss: 0.17883\n",
      "Epoch: 01 [ 9244/20164 ( 46%)], Train Loss: 0.17876\n",
      "Epoch: 01 [ 9284/20164 ( 46%)], Train Loss: 0.17854\n",
      "Epoch: 01 [ 9324/20164 ( 46%)], Train Loss: 0.17834\n",
      "Epoch: 01 [ 9364/20164 ( 46%)], Train Loss: 0.17801\n",
      "Epoch: 01 [ 9404/20164 ( 47%)], Train Loss: 0.17773\n",
      "Epoch: 01 [ 9444/20164 ( 47%)], Train Loss: 0.17753\n",
      "Epoch: 01 [ 9484/20164 ( 47%)], Train Loss: 0.17701\n",
      "Epoch: 01 [ 9524/20164 ( 47%)], Train Loss: 0.17640\n",
      "Epoch: 01 [ 9564/20164 ( 47%)], Train Loss: 0.17603\n",
      "Epoch: 01 [ 9604/20164 ( 48%)], Train Loss: 0.17585\n",
      "Epoch: 01 [ 9644/20164 ( 48%)], Train Loss: 0.17560\n",
      "Epoch: 01 [ 9684/20164 ( 48%)], Train Loss: 0.17601\n",
      "Epoch: 01 [ 9724/20164 ( 48%)], Train Loss: 0.17624\n",
      "Epoch: 01 [ 9764/20164 ( 48%)], Train Loss: 0.17581\n",
      "Epoch: 01 [ 9804/20164 ( 49%)], Train Loss: 0.17602\n",
      "Epoch: 01 [ 9844/20164 ( 49%)], Train Loss: 0.17632\n",
      "Epoch: 01 [ 9884/20164 ( 49%)], Train Loss: 0.17606\n",
      "Epoch: 01 [ 9924/20164 ( 49%)], Train Loss: 0.17595\n",
      "Epoch: 01 [ 9964/20164 ( 49%)], Train Loss: 0.17544\n",
      "Epoch: 01 [10004/20164 ( 50%)], Train Loss: 0.17498\n",
      "Epoch: 01 [10044/20164 ( 50%)], Train Loss: 0.17501\n",
      "Epoch: 01 [10084/20164 ( 50%)], Train Loss: 0.17453\n",
      "Epoch: 01 [10124/20164 ( 50%)], Train Loss: 0.17418\n",
      "Epoch: 01 [10164/20164 ( 50%)], Train Loss: 0.17393\n",
      "Epoch: 01 [10204/20164 ( 51%)], Train Loss: 0.17349\n",
      "Epoch: 01 [10244/20164 ( 51%)], Train Loss: 0.17339\n",
      "Epoch: 01 [10284/20164 ( 51%)], Train Loss: 0.17301\n",
      "Epoch: 01 [10324/20164 ( 51%)], Train Loss: 0.17267\n",
      "Epoch: 01 [10364/20164 ( 51%)], Train Loss: 0.17309\n",
      "Epoch: 01 [10404/20164 ( 52%)], Train Loss: 0.17314\n",
      "Epoch: 01 [10444/20164 ( 52%)], Train Loss: 0.17319\n",
      "Epoch: 01 [10484/20164 ( 52%)], Train Loss: 0.17331\n",
      "Epoch: 01 [10524/20164 ( 52%)], Train Loss: 0.17337\n",
      "Epoch: 01 [10564/20164 ( 52%)], Train Loss: 0.17330\n",
      "Epoch: 01 [10604/20164 ( 53%)], Train Loss: 0.17308\n",
      "Epoch: 01 [10644/20164 ( 53%)], Train Loss: 0.17291\n",
      "Epoch: 01 [10684/20164 ( 53%)], Train Loss: 0.17306\n",
      "Epoch: 01 [10724/20164 ( 53%)], Train Loss: 0.17290\n",
      "Epoch: 01 [10764/20164 ( 53%)], Train Loss: 0.17272\n",
      "Epoch: 01 [10804/20164 ( 54%)], Train Loss: 0.17229\n",
      "Epoch: 01 [10844/20164 ( 54%)], Train Loss: 0.17186\n",
      "Epoch: 01 [10884/20164 ( 54%)], Train Loss: 0.17150\n",
      "Epoch: 01 [10924/20164 ( 54%)], Train Loss: 0.17115\n",
      "Epoch: 01 [10964/20164 ( 54%)], Train Loss: 0.17114\n",
      "Epoch: 01 [11004/20164 ( 55%)], Train Loss: 0.17102\n",
      "Epoch: 01 [11044/20164 ( 55%)], Train Loss: 0.17085\n",
      "Epoch: 01 [11084/20164 ( 55%)], Train Loss: 0.17088\n",
      "Epoch: 01 [11124/20164 ( 55%)], Train Loss: 0.17063\n",
      "Epoch: 01 [11164/20164 ( 55%)], Train Loss: 0.17083\n",
      "Epoch: 01 [11204/20164 ( 56%)], Train Loss: 0.17071\n",
      "Epoch: 01 [11244/20164 ( 56%)], Train Loss: 0.17098\n",
      "Epoch: 01 [11284/20164 ( 56%)], Train Loss: 0.17088\n",
      "Epoch: 01 [11324/20164 ( 56%)], Train Loss: 0.17075\n",
      "Epoch: 01 [11364/20164 ( 56%)], Train Loss: 0.17065\n",
      "Epoch: 01 [11404/20164 ( 57%)], Train Loss: 0.17051\n",
      "Epoch: 01 [11444/20164 ( 57%)], Train Loss: 0.17026\n",
      "Epoch: 01 [11484/20164 ( 57%)], Train Loss: 0.17004\n",
      "Epoch: 01 [11524/20164 ( 57%)], Train Loss: 0.16979\n",
      "Epoch: 01 [11564/20164 ( 57%)], Train Loss: 0.16977\n",
      "Epoch: 01 [11604/20164 ( 58%)], Train Loss: 0.17032\n",
      "Epoch: 01 [11644/20164 ( 58%)], Train Loss: 0.16997\n",
      "Epoch: 01 [11684/20164 ( 58%)], Train Loss: 0.16985\n",
      "Epoch: 01 [11724/20164 ( 58%)], Train Loss: 0.17005\n",
      "Epoch: 01 [11764/20164 ( 58%)], Train Loss: 0.16979\n",
      "Epoch: 01 [11804/20164 ( 59%)], Train Loss: 0.16975\n",
      "Epoch: 01 [11844/20164 ( 59%)], Train Loss: 0.16936\n",
      "Epoch: 01 [11884/20164 ( 59%)], Train Loss: 0.16904\n",
      "Epoch: 01 [11924/20164 ( 59%)], Train Loss: 0.16894\n",
      "Epoch: 01 [11964/20164 ( 59%)], Train Loss: 0.16865\n",
      "Epoch: 01 [12004/20164 ( 60%)], Train Loss: 0.16842\n",
      "Epoch: 01 [12044/20164 ( 60%)], Train Loss: 0.16807\n",
      "Epoch: 01 [12084/20164 ( 60%)], Train Loss: 0.16805\n",
      "Epoch: 01 [12124/20164 ( 60%)], Train Loss: 0.16795\n",
      "Epoch: 01 [12164/20164 ( 60%)], Train Loss: 0.16756\n",
      "Epoch: 01 [12204/20164 ( 61%)], Train Loss: 0.16718\n",
      "Epoch: 01 [12244/20164 ( 61%)], Train Loss: 0.16724\n",
      "Epoch: 01 [12284/20164 ( 61%)], Train Loss: 0.16733\n",
      "Epoch: 01 [12324/20164 ( 61%)], Train Loss: 0.16752\n",
      "Epoch: 01 [12364/20164 ( 61%)], Train Loss: 0.16726\n",
      "Epoch: 01 [12404/20164 ( 62%)], Train Loss: 0.16702\n",
      "Epoch: 01 [12444/20164 ( 62%)], Train Loss: 0.16674\n",
      "Epoch: 01 [12484/20164 ( 62%)], Train Loss: 0.16656\n",
      "Epoch: 01 [12524/20164 ( 62%)], Train Loss: 0.16679\n",
      "Epoch: 01 [12564/20164 ( 62%)], Train Loss: 0.16674\n",
      "Epoch: 01 [12604/20164 ( 63%)], Train Loss: 0.16662\n",
      "Epoch: 01 [12644/20164 ( 63%)], Train Loss: 0.16638\n",
      "Epoch: 01 [12684/20164 ( 63%)], Train Loss: 0.16620\n",
      "Epoch: 01 [12724/20164 ( 63%)], Train Loss: 0.16598\n",
      "Epoch: 01 [12764/20164 ( 63%)], Train Loss: 0.16568\n",
      "Epoch: 01 [12804/20164 ( 63%)], Train Loss: 0.16560\n",
      "Epoch: 01 [12844/20164 ( 64%)], Train Loss: 0.16550\n",
      "Epoch: 01 [12884/20164 ( 64%)], Train Loss: 0.16549\n",
      "Epoch: 01 [12924/20164 ( 64%)], Train Loss: 0.16519\n",
      "Epoch: 01 [12964/20164 ( 64%)], Train Loss: 0.16504\n",
      "Epoch: 01 [13004/20164 ( 64%)], Train Loss: 0.16480\n",
      "Epoch: 01 [13044/20164 ( 65%)], Train Loss: 0.16462\n",
      "Epoch: 01 [13084/20164 ( 65%)], Train Loss: 0.16445\n",
      "Epoch: 01 [13124/20164 ( 65%)], Train Loss: 0.16438\n",
      "Epoch: 01 [13164/20164 ( 65%)], Train Loss: 0.16420\n",
      "Epoch: 01 [13204/20164 ( 65%)], Train Loss: 0.16390\n",
      "Epoch: 01 [13244/20164 ( 66%)], Train Loss: 0.16367\n",
      "Epoch: 01 [13284/20164 ( 66%)], Train Loss: 0.16379\n",
      "Epoch: 01 [13324/20164 ( 66%)], Train Loss: 0.16389\n",
      "Epoch: 01 [13364/20164 ( 66%)], Train Loss: 0.16391\n",
      "Epoch: 01 [13404/20164 ( 66%)], Train Loss: 0.16384\n",
      "Epoch: 01 [13444/20164 ( 67%)], Train Loss: 0.16377\n",
      "Epoch: 01 [13484/20164 ( 67%)], Train Loss: 0.16346\n",
      "Epoch: 01 [13524/20164 ( 67%)], Train Loss: 0.16346\n",
      "Epoch: 01 [13564/20164 ( 67%)], Train Loss: 0.16336\n",
      "Epoch: 01 [13604/20164 ( 67%)], Train Loss: 0.16316\n",
      "Epoch: 01 [13644/20164 ( 68%)], Train Loss: 0.16293\n",
      "Epoch: 01 [13684/20164 ( 68%)], Train Loss: 0.16288\n",
      "Epoch: 01 [13724/20164 ( 68%)], Train Loss: 0.16288\n",
      "Epoch: 01 [13764/20164 ( 68%)], Train Loss: 0.16277\n",
      "Epoch: 01 [13804/20164 ( 68%)], Train Loss: 0.16258\n",
      "Epoch: 01 [13844/20164 ( 69%)], Train Loss: 0.16225\n",
      "Epoch: 01 [13884/20164 ( 69%)], Train Loss: 0.16202\n",
      "Epoch: 01 [13924/20164 ( 69%)], Train Loss: 0.16177\n",
      "Epoch: 01 [13964/20164 ( 69%)], Train Loss: 0.16168\n",
      "Epoch: 01 [14004/20164 ( 69%)], Train Loss: 0.16136\n",
      "Epoch: 01 [14044/20164 ( 70%)], Train Loss: 0.16133\n",
      "Epoch: 01 [14084/20164 ( 70%)], Train Loss: 0.16106\n",
      "Epoch: 01 [14124/20164 ( 70%)], Train Loss: 0.16082\n",
      "Epoch: 01 [14164/20164 ( 70%)], Train Loss: 0.16082\n",
      "Epoch: 01 [14204/20164 ( 70%)], Train Loss: 0.16075\n",
      "Epoch: 01 [14244/20164 ( 71%)], Train Loss: 0.16070\n",
      "Epoch: 01 [14284/20164 ( 71%)], Train Loss: 0.16044\n",
      "Epoch: 01 [14324/20164 ( 71%)], Train Loss: 0.16029\n",
      "Epoch: 01 [14364/20164 ( 71%)], Train Loss: 0.15989\n",
      "Epoch: 01 [14404/20164 ( 71%)], Train Loss: 0.15967\n",
      "Epoch: 01 [14444/20164 ( 72%)], Train Loss: 0.15958\n",
      "Epoch: 01 [14484/20164 ( 72%)], Train Loss: 0.15929\n",
      "Epoch: 01 [14524/20164 ( 72%)], Train Loss: 0.15911\n",
      "Epoch: 01 [14564/20164 ( 72%)], Train Loss: 0.15899\n",
      "Epoch: 01 [14604/20164 ( 72%)], Train Loss: 0.15895\n",
      "Epoch: 01 [14644/20164 ( 73%)], Train Loss: 0.15875\n",
      "Epoch: 01 [14684/20164 ( 73%)], Train Loss: 0.15860\n",
      "Epoch: 01 [14724/20164 ( 73%)], Train Loss: 0.15828\n",
      "Epoch: 01 [14764/20164 ( 73%)], Train Loss: 0.15806\n",
      "Epoch: 01 [14804/20164 ( 73%)], Train Loss: 0.15797\n",
      "Epoch: 01 [14844/20164 ( 74%)], Train Loss: 0.15775\n",
      "Epoch: 01 [14884/20164 ( 74%)], Train Loss: 0.15769\n",
      "Epoch: 01 [14924/20164 ( 74%)], Train Loss: 0.15746\n",
      "Epoch: 01 [14964/20164 ( 74%)], Train Loss: 0.15725\n",
      "Epoch: 01 [15004/20164 ( 74%)], Train Loss: 0.15734\n",
      "Epoch: 01 [15044/20164 ( 75%)], Train Loss: 0.15703\n",
      "Epoch: 01 [15084/20164 ( 75%)], Train Loss: 0.15710\n",
      "Epoch: 01 [15124/20164 ( 75%)], Train Loss: 0.15681\n",
      "Epoch: 01 [15164/20164 ( 75%)], Train Loss: 0.15677\n",
      "Epoch: 01 [15204/20164 ( 75%)], Train Loss: 0.15672\n",
      "Epoch: 01 [15244/20164 ( 76%)], Train Loss: 0.15660\n",
      "Epoch: 01 [15284/20164 ( 76%)], Train Loss: 0.15644\n",
      "Epoch: 01 [15324/20164 ( 76%)], Train Loss: 0.15643\n",
      "Epoch: 01 [15364/20164 ( 76%)], Train Loss: 0.15617\n",
      "Epoch: 01 [15404/20164 ( 76%)], Train Loss: 0.15597\n",
      "Epoch: 01 [15444/20164 ( 77%)], Train Loss: 0.15589\n",
      "Epoch: 01 [15484/20164 ( 77%)], Train Loss: 0.15568\n",
      "Epoch: 01 [15524/20164 ( 77%)], Train Loss: 0.15544\n",
      "Epoch: 01 [15564/20164 ( 77%)], Train Loss: 0.15517\n",
      "Epoch: 01 [15604/20164 ( 77%)], Train Loss: 0.15493\n",
      "Epoch: 01 [15644/20164 ( 78%)], Train Loss: 0.15492\n",
      "Epoch: 01 [15684/20164 ( 78%)], Train Loss: 0.15480\n",
      "Epoch: 01 [15724/20164 ( 78%)], Train Loss: 0.15474\n",
      "Epoch: 01 [15764/20164 ( 78%)], Train Loss: 0.15475\n",
      "Epoch: 01 [15804/20164 ( 78%)], Train Loss: 0.15450\n",
      "Epoch: 01 [15844/20164 ( 79%)], Train Loss: 0.15421\n",
      "Epoch: 01 [15884/20164 ( 79%)], Train Loss: 0.15399\n",
      "Epoch: 01 [15924/20164 ( 79%)], Train Loss: 0.15378\n",
      "Epoch: 01 [15964/20164 ( 79%)], Train Loss: 0.15355\n",
      "Epoch: 01 [16004/20164 ( 79%)], Train Loss: 0.15323\n",
      "Epoch: 01 [16044/20164 ( 80%)], Train Loss: 0.15298\n",
      "Epoch: 01 [16084/20164 ( 80%)], Train Loss: 0.15272\n",
      "Epoch: 01 [16124/20164 ( 80%)], Train Loss: 0.15243\n",
      "Epoch: 01 [16164/20164 ( 80%)], Train Loss: 0.15229\n",
      "Epoch: 01 [16204/20164 ( 80%)], Train Loss: 0.15226\n",
      "Epoch: 01 [16244/20164 ( 81%)], Train Loss: 0.15221\n",
      "Epoch: 01 [16284/20164 ( 81%)], Train Loss: 0.15200\n",
      "Epoch: 01 [16324/20164 ( 81%)], Train Loss: 0.15195\n",
      "Epoch: 01 [16364/20164 ( 81%)], Train Loss: 0.15198\n",
      "Epoch: 01 [16404/20164 ( 81%)], Train Loss: 0.15172\n",
      "Epoch: 01 [16444/20164 ( 82%)], Train Loss: 0.15173\n",
      "Epoch: 01 [16484/20164 ( 82%)], Train Loss: 0.15153\n",
      "Epoch: 01 [16524/20164 ( 82%)], Train Loss: 0.15139\n",
      "Epoch: 01 [16564/20164 ( 82%)], Train Loss: 0.15113\n",
      "Epoch: 01 [16604/20164 ( 82%)], Train Loss: 0.15093\n",
      "Epoch: 01 [16644/20164 ( 83%)], Train Loss: 0.15090\n",
      "Epoch: 01 [16684/20164 ( 83%)], Train Loss: 0.15083\n",
      "Epoch: 01 [16724/20164 ( 83%)], Train Loss: 0.15071\n",
      "Epoch: 01 [16764/20164 ( 83%)], Train Loss: 0.15054\n",
      "Epoch: 01 [16804/20164 ( 83%)], Train Loss: 0.15040\n",
      "Epoch: 01 [16844/20164 ( 84%)], Train Loss: 0.15014\n",
      "Epoch: 01 [16884/20164 ( 84%)], Train Loss: 0.15007\n",
      "Epoch: 01 [16924/20164 ( 84%)], Train Loss: 0.14991\n",
      "Epoch: 01 [16964/20164 ( 84%)], Train Loss: 0.14976\n",
      "Epoch: 01 [17004/20164 ( 84%)], Train Loss: 0.14960\n",
      "Epoch: 01 [17044/20164 ( 85%)], Train Loss: 0.14931\n",
      "Epoch: 01 [17084/20164 ( 85%)], Train Loss: 0.14914\n",
      "Epoch: 01 [17124/20164 ( 85%)], Train Loss: 0.14901\n",
      "Epoch: 01 [17164/20164 ( 85%)], Train Loss: 0.14890\n",
      "Epoch: 01 [17204/20164 ( 85%)], Train Loss: 0.14888\n",
      "Epoch: 01 [17244/20164 ( 86%)], Train Loss: 0.14899\n",
      "Epoch: 01 [17284/20164 ( 86%)], Train Loss: 0.14913\n",
      "Epoch: 01 [17324/20164 ( 86%)], Train Loss: 0.14908\n",
      "Epoch: 01 [17364/20164 ( 86%)], Train Loss: 0.14888\n",
      "Epoch: 01 [17404/20164 ( 86%)], Train Loss: 0.14862\n",
      "Epoch: 01 [17444/20164 ( 87%)], Train Loss: 0.14846\n",
      "Epoch: 01 [17484/20164 ( 87%)], Train Loss: 0.14833\n",
      "Epoch: 01 [17524/20164 ( 87%)], Train Loss: 0.14808\n",
      "Epoch: 01 [17564/20164 ( 87%)], Train Loss: 0.14782\n",
      "Epoch: 01 [17604/20164 ( 87%)], Train Loss: 0.14763\n",
      "Epoch: 01 [17644/20164 ( 88%)], Train Loss: 0.14750\n",
      "Epoch: 01 [17684/20164 ( 88%)], Train Loss: 0.14740\n",
      "Epoch: 01 [17724/20164 ( 88%)], Train Loss: 0.14733\n",
      "Epoch: 01 [17764/20164 ( 88%)], Train Loss: 0.14755\n",
      "Epoch: 01 [17804/20164 ( 88%)], Train Loss: 0.14752\n",
      "Epoch: 01 [17844/20164 ( 88%)], Train Loss: 0.14728\n",
      "Epoch: 01 [17884/20164 ( 89%)], Train Loss: 0.14714\n",
      "Epoch: 01 [17924/20164 ( 89%)], Train Loss: 0.14695\n",
      "Epoch: 01 [17964/20164 ( 89%)], Train Loss: 0.14679\n",
      "Epoch: 01 [18004/20164 ( 89%)], Train Loss: 0.14667\n",
      "Epoch: 01 [18044/20164 ( 89%)], Train Loss: 0.14672\n",
      "Epoch: 01 [18084/20164 ( 90%)], Train Loss: 0.14658\n",
      "Epoch: 01 [18124/20164 ( 90%)], Train Loss: 0.14674\n",
      "Epoch: 01 [18164/20164 ( 90%)], Train Loss: 0.14653\n",
      "Epoch: 01 [18204/20164 ( 90%)], Train Loss: 0.14635\n",
      "Epoch: 01 [18244/20164 ( 90%)], Train Loss: 0.14614\n",
      "Epoch: 01 [18284/20164 ( 91%)], Train Loss: 0.14643\n",
      "Epoch: 01 [18324/20164 ( 91%)], Train Loss: 0.14644\n",
      "Epoch: 01 [18364/20164 ( 91%)], Train Loss: 0.14661\n",
      "Epoch: 01 [18404/20164 ( 91%)], Train Loss: 0.14647\n",
      "Epoch: 01 [18444/20164 ( 91%)], Train Loss: 0.14621\n",
      "Epoch: 01 [18484/20164 ( 92%)], Train Loss: 0.14611\n",
      "Epoch: 01 [18524/20164 ( 92%)], Train Loss: 0.14588\n",
      "Epoch: 01 [18564/20164 ( 92%)], Train Loss: 0.14567\n",
      "Epoch: 01 [18604/20164 ( 92%)], Train Loss: 0.14574\n",
      "Epoch: 01 [18644/20164 ( 92%)], Train Loss: 0.14581\n",
      "Epoch: 01 [18684/20164 ( 93%)], Train Loss: 0.14576\n",
      "Epoch: 01 [18724/20164 ( 93%)], Train Loss: 0.14557\n",
      "Epoch: 01 [18764/20164 ( 93%)], Train Loss: 0.14542\n",
      "Epoch: 01 [18804/20164 ( 93%)], Train Loss: 0.14519\n",
      "Epoch: 01 [18844/20164 ( 93%)], Train Loss: 0.14514\n",
      "Epoch: 01 [18884/20164 ( 94%)], Train Loss: 0.14506\n",
      "Epoch: 01 [18924/20164 ( 94%)], Train Loss: 0.14482\n",
      "Epoch: 01 [18964/20164 ( 94%)], Train Loss: 0.14463\n",
      "Epoch: 01 [19004/20164 ( 94%)], Train Loss: 0.14456\n",
      "Epoch: 01 [19044/20164 ( 94%)], Train Loss: 0.14451\n",
      "Epoch: 01 [19084/20164 ( 95%)], Train Loss: 0.14443\n",
      "Epoch: 01 [19124/20164 ( 95%)], Train Loss: 0.14435\n",
      "Epoch: 01 [19164/20164 ( 95%)], Train Loss: 0.14429\n",
      "Epoch: 01 [19204/20164 ( 95%)], Train Loss: 0.14427\n",
      "Epoch: 01 [19244/20164 ( 95%)], Train Loss: 0.14411\n",
      "Epoch: 01 [19284/20164 ( 96%)], Train Loss: 0.14423\n",
      "Epoch: 01 [19324/20164 ( 96%)], Train Loss: 0.14402\n",
      "Epoch: 01 [19364/20164 ( 96%)], Train Loss: 0.14404\n",
      "Epoch: 01 [19404/20164 ( 96%)], Train Loss: 0.14424\n",
      "Epoch: 01 [19444/20164 ( 96%)], Train Loss: 0.14411\n",
      "Epoch: 01 [19484/20164 ( 97%)], Train Loss: 0.14404\n",
      "Epoch: 01 [19524/20164 ( 97%)], Train Loss: 0.14393\n",
      "Epoch: 01 [19564/20164 ( 97%)], Train Loss: 0.14394\n",
      "Epoch: 01 [19604/20164 ( 97%)], Train Loss: 0.14397\n",
      "Epoch: 01 [19644/20164 ( 97%)], Train Loss: 0.14396\n",
      "Epoch: 01 [19684/20164 ( 98%)], Train Loss: 0.14405\n",
      "Epoch: 01 [19724/20164 ( 98%)], Train Loss: 0.14385\n",
      "Epoch: 01 [19764/20164 ( 98%)], Train Loss: 0.14374\n",
      "Epoch: 01 [19804/20164 ( 98%)], Train Loss: 0.14385\n",
      "Epoch: 01 [19844/20164 ( 98%)], Train Loss: 0.14389\n",
      "Epoch: 01 [19884/20164 ( 99%)], Train Loss: 0.14383\n",
      "Epoch: 01 [19924/20164 ( 99%)], Train Loss: 0.14374\n",
      "Epoch: 01 [19964/20164 ( 99%)], Train Loss: 0.14369\n",
      "Epoch: 01 [20004/20164 ( 99%)], Train Loss: 0.14365\n",
      "Epoch: 01 [20044/20164 ( 99%)], Train Loss: 0.14358\n",
      "Epoch: 01 [20084/20164 (100%)], Train Loss: 0.14349\n",
      "Epoch: 01 [20124/20164 (100%)], Train Loss: 0.14359\n",
      "Epoch: 01 [20164/20164 (100%)], Train Loss: 0.14382\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 0.27770\n",
      "\n",
      "Total Training Time: 5703.200906038284secs, Average Training Time per Epoch: 2851.600453019142secs.\n",
      "Total Validation Time: 245.28325033187866secs, Average Validation Time per Epoch: 122.64162516593933secs.\n"
     ]
    }
   ],
   "source": [
    "for fold in range(4, 5):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221517c",
   "metadata": {
    "papermill": {
     "duration": 1.565943,
     "end_time": "2021-10-03T11:39:49.147924",
     "exception": false,
     "start_time": "2021-10-03T11:39:47.581981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Thanks and please do Upvote!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30230.436474,
   "end_time": "2021-10-03T11:39:53.753975",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-03T03:16:03.317501",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "081730b0e55b46f9b7de024d0649994a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c173978fde644ab8455e4e558674895": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1841dd2970f64df486480b34b42769bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "184fd044951e40789c776d46232d4d73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9321dd5c46cd4a06a3a8b408c0028213",
       "placeholder": "​",
       "style": "IPY_MODEL_85af4d5b37544fefb1a628a11c256d95",
       "value": "Downloading: 100%"
      }
     },
     "1b8112ce5b71487eb8ae14befffc0725": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2846222af9cd4dfaa310ae36548a6c64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_551d1fdec05c414b8c5781d03245b370",
       "placeholder": "​",
       "style": "IPY_MODEL_5ffe9dd6cea2433db711022856a38b27",
       "value": "Downloading: 100%"
      }
     },
     "2c09e73e374246c89cc13b2e810e6515": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_772aa1e9c01944abb63874d42cf9376a",
       "max": 2239666418.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5f77cb4c23b54ff19dc70cec6f9099be",
       "value": 2239666418.0
      }
     },
     "32a2e84a0ec54234aad2f51288eb83cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32cbc309403c4f1c8a0fcf8c54af82b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35e9a14a382c465db04b49f6acf516e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "38fbd1f0955343019a07982e2e01a958": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a1aff577a0646959ab11a0c28b47b06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f304adc76315433cac962c1cbc23b55b",
       "placeholder": "​",
       "style": "IPY_MODEL_0c173978fde644ab8455e4e558674895",
       "value": " 2.24G/2.24G [01:11&lt;00:00, 33.0MB/s]"
      }
     },
     "3d27d3c231bc45e6819dd562d7306f2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ae804f8f6e8545379c962034321bf744",
       "placeholder": "​",
       "style": "IPY_MODEL_88fdd793ad2541988ca5c17aaf6ae699",
       "value": "Downloading: 100%"
      }
     },
     "4014b3447f0f43bf8c838fc2aa86967c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4912f3c823674254babaca8e7b92d26e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_184fd044951e40789c776d46232d4d73",
        "IPY_MODEL_9fb4210102a24355a39bb368bc7221ac",
        "IPY_MODEL_ea092a8c67c74a1f85e3595505289c86"
       ],
       "layout": "IPY_MODEL_38fbd1f0955343019a07982e2e01a958"
      }
     },
     "4e2ed95bbdfb4c80bb88df444bac4933": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f726c529c7f43e6af7c8f0ad912f1c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "551d1fdec05c414b8c5781d03245b370": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "55f94b70de2a4504bf35270e1c54416b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3d27d3c231bc45e6819dd562d7306f2b",
        "IPY_MODEL_a621f2fc90234af8b0f4a3d0bca4e736",
        "IPY_MODEL_c6775129fa40439884337aef5bad944e"
       ],
       "layout": "IPY_MODEL_4f726c529c7f43e6af7c8f0ad912f1c8"
      }
     },
     "5f77cb4c23b54ff19dc70cec6f9099be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5ffe9dd6cea2433db711022856a38b27": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "60b45e22d9354b478b036b6f1a73cb14": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7ba59d1f6f7f4919bcd7b5da6b380773",
       "placeholder": "​",
       "style": "IPY_MODEL_89c4f88e0e8b4d1db74e065c9c9894f8",
       "value": "Downloading: 100%"
      }
     },
     "696419eaa7b34a6c8a48f0627df97249": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "69e66acdd2b3497094d9417471e43a08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6fdcd36786fd4d3abf318d1dffee8693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_32a2e84a0ec54234aad2f51288eb83cd",
       "placeholder": "​",
       "style": "IPY_MODEL_35e9a14a382c465db04b49f6acf516e6",
       "value": " 179/179 [00:00&lt;00:00, 6.41kB/s]"
      }
     },
     "772aa1e9c01944abb63874d42cf9376a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ba59d1f6f7f4919bcd7b5da6b380773": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7f4741e0b4fb49f28403a89b4e1679d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8046d6a5812b485d8e86c07a8f329caf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_facdc9dab02b40bdaf68a8eb7c76c476",
        "IPY_MODEL_2c09e73e374246c89cc13b2e810e6515",
        "IPY_MODEL_3a1aff577a0646959ab11a0c28b47b06"
       ],
       "layout": "IPY_MODEL_b2a14d254faa47b89d265d461d354eb8"
      }
     },
     "85af4d5b37544fefb1a628a11c256d95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "88fdd793ad2541988ca5c17aaf6ae699": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "89c4f88e0e8b4d1db74e065c9c9894f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8eabe97d850d448db71bf4172246dbbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_60b45e22d9354b478b036b6f1a73cb14",
        "IPY_MODEL_9e66945a9d054a76a0b51fd88df4991a",
        "IPY_MODEL_6fdcd36786fd4d3abf318d1dffee8693"
       ],
       "layout": "IPY_MODEL_ed54f27ebb144662bbd8a4c1b46de311"
      }
     },
     "928f5c6824314b12a63f6b10cef93c0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9321dd5c46cd4a06a3a8b408c0028213": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9b9c6a83a91240a397cee9f80047d8e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9e66945a9d054a76a0b51fd88df4991a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d34c95aa328447ed9c67366e5ef06c73",
       "max": 179.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9b9c6a83a91240a397cee9f80047d8e7",
       "value": 179.0
      }
     },
     "9fb4210102a24355a39bb368bc7221ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_696419eaa7b34a6c8a48f0627df97249",
       "max": 150.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ae6be8acaa994e1491695318f7df5679",
       "value": 150.0
      }
     },
     "a621f2fc90234af8b0f4a3d0bca4e736": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_928f5c6824314b12a63f6b10cef93c0b",
       "max": 606.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e32c198f072b4e46b194df959423732f",
       "value": 606.0
      }
     },
     "a746d967e63f495f9b2b755ccd90c583": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ae6be8acaa994e1491695318f7df5679": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ae804f8f6e8545379c962034321bf744": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b2a14d254faa47b89d265d461d354eb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9b2cd2821c04556a08035adb562553c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4e2ed95bbdfb4c80bb88df444bac4933",
       "max": 5069051.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a746d967e63f495f9b2b755ccd90c583",
       "value": 5069051.0
      }
     },
     "bc296f50dfa54a73a9cbece292ced5b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d138b16362a24e45a9d957adbd299942",
       "placeholder": "​",
       "style": "IPY_MODEL_1b8112ce5b71487eb8ae14befffc0725",
       "value": " 5.07M/5.07M [00:00&lt;00:00, 19.1MB/s]"
      }
     },
     "bfa803b8f26c4fc8805f84ec35309e71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2846222af9cd4dfaa310ae36548a6c64",
        "IPY_MODEL_b9b2cd2821c04556a08035adb562553c",
        "IPY_MODEL_bc296f50dfa54a73a9cbece292ced5b7"
       ],
       "layout": "IPY_MODEL_081730b0e55b46f9b7de024d0649994a"
      }
     },
     "c6775129fa40439884337aef5bad944e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d47bd81a24ab45b0a598b63fa1313f1e",
       "placeholder": "​",
       "style": "IPY_MODEL_69e66acdd2b3497094d9417471e43a08",
       "value": " 606/606 [00:00&lt;00:00, 20.2kB/s]"
      }
     },
     "d138b16362a24e45a9d957adbd299942": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d34c95aa328447ed9c67366e5ef06c73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d47bd81a24ab45b0a598b63fa1313f1e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e32c198f072b4e46b194df959423732f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ea092a8c67c74a1f85e3595505289c86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7f4741e0b4fb49f28403a89b4e1679d8",
       "placeholder": "​",
       "style": "IPY_MODEL_4014b3447f0f43bf8c838fc2aa86967c",
       "value": " 150/150 [00:00&lt;00:00, 4.57kB/s]"
      }
     },
     "ed54f27ebb144662bbd8a4c1b46de311": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f304adc76315433cac962c1cbc23b55b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "facdc9dab02b40bdaf68a8eb7c76c476": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_32cbc309403c4f1c8a0fcf8c54af82b5",
       "placeholder": "​",
       "style": "IPY_MODEL_1841dd2970f64df486480b34b42769bc",
       "value": "Downloading: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
